---
title: 16. Greedy Algorithms
# description: Short summary of the post
date: 2024-12-16 16:04
categories: [Computer Science, Algorithm]
tags: [greedy-algorithm, activity-selection-problem, greedy-choice-property, optimal-substructure, huffman-code, matroid, minimum-spanning-tree, task-scheduling-problem]     # TAG names should always be lowercase
math: true
pin: false
---

A greedy algorithm always makes the choice that looks best at the moment.
It makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution.

## 16.1 An activity-selection problem

Suppose we have a set $$ S = \{ a_1, \ a_2, \dots, \ a_n \} $$ of $$ n $$ proposed activities that wish to use a resource.
Each activity $$ a_i $$ has a start time $$ s_i $$ and a finish time $$ f_i $$, where $$ 0 \le s_i < f_i < \infty $$.
If selected, activity $$ a_i $$ takes place during the half-open time interval $$ [s_i, \ f_i) $$.
Activities $$ a_i $$ and $$ a_j $$ are compatible if the intervals $$ [s_i, \ f_i) $$ and $$ [s_j, \ f_j) $$ do not overlap.
In the activity-selection problem, we wish to select a maximum-size subset of mutually compatible activities.
Assume that the activities are sorted in monotonically increasing order of finish time:

$$
\begin{align*}
    f_1 \le f_2 \le \dots \le f_n
\end{align*}
\label{eq:1}
\tag{16.1}
$$

### The optimal substructure of the activity-selection problem

Denote by $$ S_{ij} $$ the set of activities that start after activity $$ a_i $$ finishes and that finish before activity $$ a_j $$ starts.
Suppose $$ A_{ij} $$ is a maximum set of mutually compatible activities in $$ S_{ij} $$, which includes some activity $$ a_k $$.
We assume that $$ a_k $$ is included in an optimal solution.
Let $$ A_{ik} = A_{ij} \cap S_{ik} $$ and $$ A_{kj} = A_{ij} \cap S_{kj} $$, then it turns out that $$ A_{ij} = A_{ik} \cup \{ a_k \} \cup A_{kj} $$.
Therefore, $$ A_{ij} $$ consists of $$ |A_{ij}| = |A_{ik}| + |A_{kj}| + 1 $$ activities.

The cut-and-paste argument shows that the optimal solution $$ A_{ij} $$ includes optimal solutions to the two subproblems for $$ S_{ik} $$ and $$ S_{kj} $$.
Suppose there is a set $$ A'_{kj} $$ of mutually compatible activities in $$ S_{kj} $$ where $$ |A'_{kj}| > |A_{kj}| $$, then $$ |A_{ik}| + |A'_{kj}| + 1 > |A_{ik}| + |A_{kj}| + 1 = |A_{ij}| $$ holds, which contradicts the assumption that $$ A_{ij} $$ is an optimal solution.
A symmetric argument applies to the activities in $$ S_{ik} $$.

This way of characterizing optimal substructure suggests that we might solve the activity-selection problem by dynamic programming.
Denote the size of an optimal solution for the set $$ S_{ij} $$ by $$ c[i, \ j] $$, then we would have the recurrence

$$
\begin{align*}
    c[i, \ j] =
    \begin{cases}
        \displaystyle 0                                                     & \text{if } S_{ij} = \emptyset \\
        \displaystyle \max_{a_k \in S_{ij}} \{ c[i, \ k] + c[k, \ j] + 1 \} & \text{if } S_{ij} \neq \emptyset
    \end{cases}
\end{align*}
\label{eq:2}
\tag{16.2}
$$

We could then develop a recursive algorithm and memoize it, or we could work bottom-up and fill in table entries as we go along.
However, we would be overlooking another important characteristic of the activity-selection problem.

### Making the greedy choice

Intuition suggest that we should choose an activity that leaves the resource available for as many other activities as possible.
Now, of the activities we end up choosing, one of them must be the first one to finish.
Therefore, we choose the activity in $$ S $$ with the earliest finish time, since that would leave the resource available for as many of the activities that follow it as possible.
Because the activities are sorted in monotonically increasing order by finish time, the greedy choice is activity $$ a_1 $$.

If we make the greedy choice, we have only one remaining subproblem to solve: finding activities that start after $$ a_1 $$ finishes.
We have that $$ s_1 < f_1 $$, and $$ f_1 $$ is the earliest finish time of any activity, and therefore no activity can have a finish time less than or equal to $$ s_1 $$.
Thus, all activities that are compatible with activity $$ a_1 $$ must start after $$ a_1 $$ finishes.  
Furthermore, we have already established that the activity-selection problem exhibits optimal substructure.
Let $$ S_k = \{ a_i \in S : s_i \ge f_k \} $$ be the set of activities that start after activity $$ a_k $$ finishes.
If we make the greedy choice of activity $$ a_1 $$, then $$ S_1 $$ remains as the only subproblem to solve.
(We sometimes refer to the sets $$ S_k $$ as subproblems rather than as just sets of activities.)
Optimal substructure tells us that if $$ a_1 $$ is in the optimal solution, then an optimal solution to the original problem consists of activity $$ a_1 $$ and all the activities in an optimal solution to the subproblem $$ S_1 $$.

### Theorem 16.1

Consider any nonempty subproblem $$ S_k $$, and let $$ a_m $$ be an activity in $$ S_k $$ with the earliest finish time.
Then $$ a_m $$ is included in some maximum-size subset of mutually compatible activities of $$ S_k $$.

**Proof**  
Let $$ A_k $$ be a maximum-size subset of mutually compatible activities in $$ S_k $$, and let $$ a_j $$ be the activity in $$ A_k $$ with the earliest finish time.
If $$ a_j = a_m $$, we are done.
If $$ a_j \neq a_m $$, let the set $$ A'_k = A_k - \{ a_j \} \cup \{ a_m \} $$ be $$ A_k $$ but substituting $$ a_m $$ for $$ a_j $$.
The activities in $$ A'_k $$ are disjoint, which follows because the activities in $$ A_k $$ are disjoint, $$ a_j $$ is the first activity in $$ A_k $$ to finish, and $$ f_m \le f_j $$.
Since $$ |A'_k| = |A_k| $$, we conclude that $$ |A'_k| $$ is a maximum-size subset of mutually compatible activities of $$ S_k $$, and it includes $$ a_m $$. $$ \blacksquare $$

We can repeatedly choose the activity that finishes first, keep only the activities compatible with this activity, and repeat until no activities remain.
An algorithm to solve the activity-selection problem does not need to work bottom-up, like a table-based dynamic-programming algorithm.
Instead, it can work top-down, choosing an activity to put into the optimal solution and then solving the subproblem of choosing activities from those that are compatible with those already chosen.
Greedy algorithms typically have this top-down design: make a choice and then solve a subproblem, rather than the bottom-up technique of solving subproblems before making a choice.

### A recursive greedy algorithm

>RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ k, \ n $$)  
>01&nbsp; $$ m = k + 1 $$  
>02&nbsp; while $$ m \le n $$ and $$ s[m] < f[k] $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ m = m + 1 $$  
>04&nbsp; if $$ m \le n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ \{ a_m \} \ \cup $$ RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ m, \ n $$)  
>06&nbsp; else return $$ \emptyset $$

In a given recursive call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ k, \ n $$), the while loop of lines 2–3 looks for the first activity in $$ S_k $$ to finish.
The loop examines $$ a_{k + 1}, \ a_{k + 2}, \dots , \ a_{n} $$, until it finds the first activity $$ a_m $$ that is compatible with $$ a_k $$; such an activity has $$ s_m \ge f_k $$.
If the loop terminates because it finds such an activity, line 5 returns the union of $$ \{ a_m \} $$ and the maximum-size subset of $$ S_m $$ returned by the recursive call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ m, \ n $$).
Alternatively, the loop may terminate because $$ m > n $$, in which case we have examined all activities in $$ S_k $$ without finding one that is compatible with $$ a_k $$.
In this case, $$ S_k = \emptyset $$, and so the procedure returns $$ \emptyset $$ in line 6.

Assuming that the activities have already been sorted by finish times, the running time of the call RECURSIVE-ACTIVITY-SELECTOR($$ s, \ f, \ 0, \ n $$) is $$ \Theta(n) $$, which we can see as follows.
Over all recursive calls, each activity is examined exactly once in the while loop test of line 2.
In particular, activity $$ a_i $$ is examined in the last call made in which $$ k < i $$.

### An iterative greedy algorithm

>GREEDY-ACTIVITY-SELECTOR($$ s, \ f $$)  
>01&nbsp; $$ n = s.length $$  
>02&nbsp; $$ A = \{ a_1 \} $$  
>03&nbsp; $$ k = 1 $$  
>04&nbsp; for $$ m = 2 $$ to $$ n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ s[m] \ge f[k] $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ A = A \cup \{ a_m \} $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ k = m $$  
>08&nbsp; return $$ A $$

The variable $$ k $$ indexes the most recent addition to $$ A $$, corresponding to the activity $$ a_k $$ in the recursive version.
Since we consider the activities in order of monotonically increasing finish time, $$ f_k $$ is always the maximum finish time of any activity in $$ A $$.
That is,

$$
\begin{align*}
    f_k = \max \{ f_i : a_i \in A \}
\end{align*}
\label{eq:3}
\tag{16.3}
$$

Lines 2–3 select activity $$ a_1 $$, initialize $$ A $$ to contain just this activity, and initialize $$ k $$ to index this activity.
The for loop of lines 4–7 finds the earliest activity in $$ S_k $$ to finish.
The loop considers each activity $$ a_m $$ in turn and adds $$ a_m $$ to $$ A $$ if it is compatible with all previously selected activities.
To see whether activity $$ a_m $$ is compatible with every activity in $$ A $$, by equation \eqref{eq:3}, it suffices to check that $$ s_m \ge f_k $$.
If activity $$ a_m $$ is compatible, then lines 6–7 add activity $$ a_m $$ to $$ A $$ and set $$ k $$ to $$ m $$.  
Like the recursive version, GREEDY-ACTIVITY-SELECTOR schedules a set of $$ n $$ activities in $$ \Theta(n) $$ time, assuming that the activities were already sorted initially by their finish times.

## 16.2 Elements of the greedy strategy

A greedy algorithm obtains an optimal solution to a solution to a problem by making a sequence of choices.
At each decision point, the algorithm makes the choice that seems best at the moment.
This heuristic strategy does not always produce an optimal solution, but sometimes, it does.
We design greedy algorithms according to the following sequence of steps:

1. Cast the optimization problem as one in which we make a choice and are left with one subproblem to solve.
2. Prove that there is always an optimal solution to the original problem that makes the greedy choice, so that the greedy choice is always safe.
3. Demonstrate optimal substructure by showing that, having made the greedy choice, what remains is a subproblem with the property that if we combine an optimal solution to the subproblem with the greedy choice we have made, we arrive at an optimal solution to the original problem.

There are two key ingredients that the problem can be solved by using a greedy algorithm: the greedy-choice property and optimal substructure.

### Greedy-choice property

The greedy-choice property is that we can assemble a globally optimal solution by making locally optimal greedy choices.
In other words, when we are considering which choice to make, we make the choice that looks best in the current problem, without considering results from subproblems.  
Here is where greedy algorithms differ from dynamic programming.
In dynamic programming, the choice we make depends on the solutions to subproblems.
Consequently, we typically solve dynamic-programming problems in a bottom-up manner, progressing from smaller subproblems to larger ones.
In a greedy algorithm, we make whatever choice seems best at the moment and then solve the subproblem that remains.
The choice made by a greedy algorithm may depend on choices so far, but it cannot depend on any future choices or on the solutions to subproblems.
A dynamic programming algorithm proceeds bottom up, whereas a greedy strategy usually progresses in a top-down fashion, making one greedy choice after another, reducing each given problem instance to a smaller one.

### Optimal substructure

A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solution to subproblems.
This property is a key ingredient of assessing the applicability of dynamic programming as well as greedy algorithms.
As an example of optimal substructure in Section 16.1, if an optimal solution to subproblem $$ S_{ij} $$ includes an activity $$ a_k $$, then it must also contain optimal solutions to the subproblems $$ S_{ik} $$ and $$ S_{kj} $$.
Based on this observation of optimal substructure, we were able to devise the recurrence \eqref{eq:2} that described the value of an optimal solution.  
We usually use a more direct approach regarding optimal substructure when applying it to greedy algorithms.
We can assume that we arrived at a subproblem by having made the greedy choice in the original problem.
All we really need to do is argue that an optimal solution to the subproblem, combined with the greedy choice already made, yields an optimal solution to the original problem.
This scheme implicitly uses induction on the subproblems to prove that making the greedy choice at every step producese an optimal solution.

## 16.3 Huffman codes

Suppose we have a 100,000-character data file that we wish to store compactly.
We observe that the characters in the file occur with the frequencies given as below. That is, only 6 different characters appear, and the character **a** occurs 45,000 times.

| |**a**|**b**|**c**|**d**|**e**|**f**|
|---|:---:|:---:|:---:|:---:|:---:|:---:|
|Frequency (in thousands)|45|13|12|6|9|5|
|Fixed-length codeword|000|001|010|011|100|101|
|Variable-length codeword|0|101|100|111|1101|1100|

Here, we consider the problem of designing a binary character code (or code for short) in which each character is represented by a unique binary string, which is called a codeword.
If we use a fixed-length codeword, we need 3 bits to represent 6 characters.
This method requires 300,000 bits to code the entire file.  
A variable-length code can do considerably better that a fixed-length code, by giving frequent characters short codewords and infrequent characters long codewords.
The table above shows such a code; here the 1-bit string 0 represents **a**, and the 4-bit string 1100 represents **f**.
This code requires

$$
\begin{align*}
    (45 \cdot 1 + 13 \cdot 3 + 12 \cdot 3 + 16 \cdot 3 + 9 \cdot 4 + 5 \cdot 4) \cdot 1000 = 224,000 \text{ bits}
\end{align*}
$$

to represent the file, a savings of approximately 25%.
In fact, this is an optimal character code for this file.

### Prefix codes

We consider only codes in which no codewords is also a prefix of some other codeword.
Such codes are called prefix codes.
A prefix code can always achieve the optimal data compression among any character code, and so we suffer no loss of generality by restricting our attention to prefix codes.

Prefix codes are desirable because they simplify decoding.
Since no codeword is a prefix of any other, the codeword that begins an encoded file is unambiguous.
For example, the string 001011101 parses uniquely as $$ 0 \cdot 0 \cdot 101 \cdot 1101 $$, which decodes to **aabe**.

A binary tree whose leaves are the given characters provides a convenient representation for the prefix code.
We interpret the binary codeword for a character as the simple path from the root to that character, where 0 means "go to the left child" and 1 means "go to the right child."
Figure 16.1 shows the trees for the two codes of our example.
Note that these are not binary search trees, since the leaves need not appear in sorted order and internal nodes do not contain character keys.

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.1.png){: width="700" height="400" }
_**Figure 16.1** **(a)** The tree corresponding to the fixed-length code. **(b)** The tree corresponding to the optimal prefix code._

An optimal code for a file is always represented by a full binary tree, in which every nonleaf node has two children.
The fixed-length code is not optimal since its tree is not a full binary tree: it contains codewords beginning $$ 10 \dots $$ but none beginning $$ 11 \dots $$.
Since we can now restrict our attention to full binary trees, we can say that if $$ C $$ is the alphabet from which the characters are drawn and all character frequencies are positive, then the tree for an optimal prefix code has exactly $$ |C| $$ leaves, one for each letter of the alphabet, and exactly $$ |C| - 1 $$ internal nodes.

Given a tree $$ T $$ corresponding to a prefix code, we can easily compute the number of bits required to encode a file.
For each character $$ c $$ in the alphabet $$ C $$, let the attribute $$ c.freq $$ denote the frequency of $$ c $$ in the file and let $$ d_T(c) $$ denote the depth of $$ c $$'s leaf in the tree.
Note that $$ d_T(c) $$ is also the length of the codeword for character $$ c $$.
The number of bits required to encode a file is thus

$$
\begin{align*}
    B(T) = \sum_{c \in C}{c.freq \cdot d_T(c)}
\end{align*}
\label{eq:4}
\tag{16.4}
$$

which we define as the cost of the tree $$ T $$.

### Constructing a Huffman code

In the pseudocode that follows, the algorithm builds the tree $$ T $$ corresponding to the optimal code in a bottom-up manner.
It begins with $$ |C| $$ leaves and performs a sequence of $$ |C| - 1 $$ merging operations to create the final tree.
The algorithm uses a min-priority queue $$ Q $$, keyed on the $$ freq $$ attribute, to identify the two least-frequent objects to merge together.
When we merge two objects, the result is a new object whose frequency is the sum of the frequencies of the two objects that were merged.

>HUFFMAN($$ C $$)  
>01&nbsp; $$ n = |C| $$  
>02&nbsp; $$ Q = C $$  
>03&nbsp; for $$ i = 1 $$ to $$ n - 1 $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;allocate a new node $$ z $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ z.left = x = $$ EXTRACT-MIN($$ Q $$)  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ z.right = y = $$ EXTRACT-MIN($$ Q $$)  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ z.freq = x.freq + y.freq $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;INSERT($$ Q, \ z $$)  
>09&nbsp; return EXTRACT-MIN($$ Q $$)

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.2.png){: width="700" height="400" }
_**Figure 16.2** The construction of a Huffman code._

Line 2 initializes the min-priority queue $$ Q $$ with the characters in $$ C $$.
The for loop in lines 3–8 repeatedly extracts the two nodes $$ x $$ and $$ y $$ of lowest frequency from the queue, replacing them in the queue with a new node $$ z $$, representing their merger.
The frequency of $$ z $$ is computed as the sum of the frequencies of $$ x $$ and $$ y $$ in line 7.
The node $$ z $$ has $$ x $$ and $$ y $$ as its children.
(The order of $$ x $$ and $$ y $$ is arbitrary; switching the left and right child of any node yields a different code of the same cost.)
After $$ n - 1 $$ mergers, line 9 returns the one node left in the queue, which is the root of the code tree.

To analyze the running time of Huffman's algorithm, we assume that $$ Q $$ is implemented as a binary min-heap.
For a set $$ C $$ of $$ n $$ characters, we can initialize $$ Q $$ in $$ O(n) $$ time using the BUILD-MIN-HEAP procedure discussed in Section 6.3.
The for loop in lines 3–8 executes exactly $$ n - 1 $$ times, and since each heap operation requires $$ O(\lg n) $$, the loop contributes $$ O(n \lg n) $$ to the running time.
Thus, the total running time of HUFFMAN on a set of $$ n $$ characters is $$ O(n \lg n) $$.

### Correctness of Huffman's algorithm

### Lemma 16.2 (Greedy-choice property of Huffman's algorithm)

Let $$ C $$ be an alphabet in which each character $$ c \in C $$ has frequency $$ c.freq $$.
Let $$ x $$ and $$ y $$ be two characters in $$ C $$ having the lowest frequencies.
Then there exists an optimal prefix code for $$ C $$ in which the codewords for $$ x $$ and $$ y $$ have the same length and differ only in the last bit.

**Proof**  
Let $$ a $$ and $$ b $$ be two characters that are sibling leaves of maximum depth in $$ T $$.
WLOG we assume that $$ a.freq \le b.freq $$ and $$ x.freq \le y.freq $$.
Since $$ x.freq $$ and $$ y.freq $$ are the two lowest leaf frequencies, in order, and $$ a.freq $$ and $$ b.freq $$ are two arbitrary frequencies, in order, we have $$ x.freq \le a.freq $$ and $$ y.freq \le b.freq $$.
If $$ x.freq = b.freq $$, then we would have $$ a.freq = b.freq = x.freq = y.freq $$, and the lemma would be trivially true.
Thus, we will assume that $$ x.freq \neq b.freq $$, which means that $$ x \neq b $$.

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.3.png){: width="700" height="200" }
_**Figure 16.3**_

As Figure 16.3 shows, we exchange the positions in $$ T $$ of $$ a $$ and $$ x $$ to produce a tree $$ T' $$, and then we exchange the positions in $$ T' $$ of $$ b $$ and $$ y $$ to produce a tree $$ T'' $$ in which $$ x $$ and $$ y $$ are sibling leaves of maximum depth.
(Note that if $$ x = b $$ but $$ y \neq a $$, then tree $$ T'' $$ does not have $$ x $$ and $$ y $$ as sibling leaves of maximum depth.)
By equation \eqref{eq:4}, the difference in cost between $$ T $$ and $$ T' $$ is

$$
\begin{align*}
    B(T) - B(T') &= \sum_{c \in C}{c.freq \cdot d_T(c)} - \sum_{c \in C}{c.freq \cdot d_{T'}(c)} \\
                 &= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_{T'}(x) - a.freq \cdot d_{T'}(a) \\
                 &= x.freq \cdot d_T(x) + a.freq \cdot d_T(a) - x.freq \cdot d_T(a) - a.freq \cdot d_T(x) \\
                 &= (a.freq - x.freq)(d_T(a) - d_T(x)) \\
                 &\ge 0
\end{align*}
$$

Observe that exchanging $$ x $$ and $$ a $$ does not increase the cost.
Similarly, exchanging $$ y $$ and $$ b $$ does not increase the cost, and so $$ B(T') - B(T'') $$ is nonnegative.
Thus, $$ B(T'') \le B(T) $$, and since $$ T $$ is optimal, we have $$ B(T) \le B(T'') $$, which implies $$ B(T'') = B(T) $$.
Therefore, $$ T'' $$ is an optimal tree in which $$ x $$ and $$ y $$ appear as sibling leaves of maximum depth, from which the lemma follows. $$ \blacksquare $$

Lemma 16.2 implies that the process of building up an optimal tree by mergers can, WLOG begin with the greedy choice of mergine together those two characters of lowest frequency.

### Lemma 16.3 (Optimal-substructure property of Huffman's algorithm)

Let $$ C $$ be a given alphabet with frequency $$ c.freq $$ defined for each character $$ c \in C $$.
Let $$ x $$ and $$ y $$ be two characters in $$ C $$ with minimum frequency.
Let $$ C' $$ be the alphabet $$ C $$ with the characters $$ x $$ and $$ y $$ removed and a new character $$ z $$ added, so that $$ C' = C - \{ x, \ y \} \cup \{ z \} $$.
Define $$ freq $$ for $$ C' $$ as for $$ C $$, except that $$ z.freq = x.freq + y.freq $$.
Let $$ T' $$ be any tree representing an optimal prefix code for the alphabet $$ C' $$.
Then the tree $$ T $$, obtained from $$ T' $$ by replacing the leaf node for $$ z $$ with an internal node having $$ x $$ and $$ y $$ as children, represents an optimal prefix code for the alphabet $$ C $$.

**Proof**  
For each character $$ c \in C - \{ x, \ y \} $$, we have that $$ d_T(c) = d_{T'}(c) $$, and hence $$ c.freq \cdot d_T(c) = c.freq \cdot d_{T'}(c) $$.
Since $$ d_T(x) = d_T(y) = d_{T'}(z) + 1 $$, we have

$$
\begin{align*}
    x.freq \cdot d_T(x) + y.freq \cdot d_T(y) &= (x.freq + y.freq)(d_{T'}(z) + 1) \\
                                              &= z.freq \cdot d_{T'}(z) + (x.freq + y.freq)
\end{align*}
$$

from which we conclude that

$$
\begin{align*}
    B(T) = B(T') + x.freq + y.freq
\end{align*}
$$

or, equivalently,

$$
\begin{align*}
    B(T') = B(T) - x.freq - y.freq
\end{align*}
$$

We now prove the lemma by contradiction.
Suppose that $$ T $$ does not represent an optimal prefix code for $$ C $$.
Then there exists an optimal tree $$ T'' $$ such that $$ B(T'') < B(T) $$.
WLOG (by Lemma 16.2), $$ T'' $$ has $$ x $$ and $$ y $$ as siblings.
Let $$ T''' $$ be the tree $$ T'' $$ witht he common parent of $$ x $$ and $$ y $$ replaced by a leaf $$ z $$ with frequency $$ z.freq = x.freq + y.freq $$.
Then

$$
\begin{align*}
    B(T''') &= B(T'') - x.freq - y.freq \\
            &< B(T) - x.freq - y.freq \\
            &= B(T')
\end{align*}
$$

yielding a contradiction to the assumption that $$ T' $$ represents an optimal prefix code for $$ C' $$.
Thus, $$ T $$ must represent an optimal prefix code for the alphabet $$ C $$. $$ \blacksquare $$

### Theorem 16.4

Procedure HUFFMAN produces an optimal prefix code.

**Proof**  
Immediate from Lemmas 16.2 and 16.3. $$ \blacksquare $$

## 16.4 Matroids and greedy methods

### Matroids

A matroid is an ordered pair $$ M = (S, \ \mathcal{I}) $$ satisfying the following conditions.

1. $$ S $$ is a finite set.
2. $$ \mathcal{I} $$ is a nonempty family of subsets of $$ S $$, called the independent subsets of $$ S $$, such that if $$ B \in \mathcal{I} $$ and $$ A \subseteq B $$, then $$ A \in \mathcal{I} $$.
We say that $$ \mathcal{I} $$ is hereditary if it satisfies this property.
Note that the empty set $$ \emptyset $$ is necessarily a member of $$ \mathcal{I} $$.
3. If $$ A \in \mathcal{I} $$, and $$ |A| < |B| $$, then there exists some element $$ x \in B - A $$ such that $$ A \cup \{ x \} \in \mathcal{I} $$.
We say that $$ M $$ satisfies the exchange property.

Consider the graphic matroid $$ M_G = (S_G, \ \mathcal{I}_G) $$ defined in terms of a given undirected graph $$ G = (V, \ E) $$ as follows:

- The set $$ S_G $$ is defined to be $$ E $$, the set of edges of $$ G $$.
- If $$ A $$ is a subset of $$ E $$, then $$ A \in \mathcal{I}_G $$ if and only if $$ A $$ is acyclic. That is, a set of edges $$ A $$ is independent if and only if the subgraph $$ G_A = (V, \ A) $$ forms a forest.

The graphic matroid $$ M_G $$ is closely related to the minimum-spanning-tree problem.

### Theorem 16.5

If $$ G = (V, \ E) $$ is an undirected graph, then $$ M_G = (S_G, \ \mathcal{I}_G) $$ is a matroid.

**Proof**  
Clearly, $$ S_G = E $$ is a finite set.
Furthermore, $$ \mathcal{I}_G $$ is hereditary, since a subset of a forest is a forest.
Thus, it remains to show that $$ M_G $$ satisfies the exchange property.
Suppose that $$ G_A = (V, \ A) $$ and $$ G_B = (V, \ B) $$ are forests of $$ G $$ and that $$ |B| > |A| $$.  
We claim that a forest $$ F = (V_F, \ E_F) $$ contains exactly $$ |V_F| - |E_F| $$ trees.
Suppose that $$ F $$ consists of $$ t $$ trees, where the $$ i $$th tree contains $$ v_i $$ vertices and $$ e_i $$ edges.
Then, we have

$$
\begin{align*}
    |E_F| &= \sum_{i = 1}^{t} e_i \\
          &= \sum_{i = 1}^{t} (v_i - 1) \quad \text{(by Theorem B.2)} \\
          &= \sum_{i = 1}^{t} v_i - t \\
          &= |V_F| - t          
\end{align*}
$$

which implies that $$ t = |V_F| - |E_F| $$.
Thus, forest $$ G_A $$ contains
$$ |V| - |A| $$
trees, and forest $$ G_B $$ contains $$ |V| - |B| $$
trees.  
Since forest $$ G_B $$ has fewer trees than $$ G_A $$ does, $$ G_B $$ must contain some tree $$ T $$ whose vertices are in two different trees in $$ G_A $$.
Moreover, since $$ T $$ is connected, it must contain an edge $$ (u, \ v) $$ such that vertices $$ u $$ and $$ v $$ are in different trees in forest $$ G_A $$.
Since the edge $$ (u, \ v) $$ connects vertices in two different trees in $$ G_A $$, we can add the edge $$ (u, \ v) $$ to forest $$ G_A $$ without creating a cycle.
Therefore, $$ M_G $$ satisfies the exchange property. $$ \blacksquare $$

Given a matroid $$ M = (S, \ \mathcal{I}) $$, we call an element $$ x \notin A $$ an extension of $$ A \in \mathcal{I} $$ if we can add $$ x $$ to $$ A $$ while preserving independence, that is, $$ x $$ is an extension of $$ A $$ if $$ A \cup \{ x \} \in \mathcal{I} $$.
If $$ A $$ is an independent subset in a matroid $$ M $$, $$ A $$ is maximal if it has no extensions.
That is, $$ A $$ is maximal if it is not contained in any larger independent subset of $$ M $$.

### Theorem 16.6

All maximal independent subsets in a matroid have the same size.

**Proof**  
Suppose to the contrary that $$ A $$ is a maximal independent subset of $$ M $$ and there exists another larger maximal independent subset $$ B $$ of $$ M $$.
Then, the exchange property implies that for some $$ x \in B - A $$, we can extend $$ A $$ to a larger independent set $$ A \cup \{ x \} $$, contradicting the assumption that $$ A $$ is maximal. $$ \blacksquare $$

Consider a graphic matroid $$ M_G $$ for a connected, undirected graph $$ G $$.
Every maximal independent subset of $$ M_G $$ must be a free tree with exactly
$$ |V| - 1 $$
edges that connects all the vertices of $$ G $$.
Such a tree is called a spanning tree of $$ G $$.

### Greedy algorithms on a weighted matroid

We say that a matroid $$ M = (S, \ \mathcal{I}) $$ is weighted if it is associated with a weight function $$ w $$ that assigns a strictly positive weight $$ w(x) $$ to each element $$ x \in S $$.
The weight function $$ w $$ extends to subsets of $$ S $$ by summation:

$$
\begin{align*}
    w(A) = \sum_{x \in A} w(x)
\end{align*}
$$

for any $$ A \subseteq S $$.
For example, if we let $$ w(e) $$ denote the weight of an edge $$ e $$ in a graphic matroid $$ M_G $$, then $$ w(A) $$ is the total weight of the edges in edge set $$ A $$.  
Now, we are given a weighted matroid $$ M = (S, \ \mathcal{I}) $$, and we wish to find an independent set $$ A \in \mathcal{I} $$ such that $$ w(A) $$ is maximized.
We call such a subset that is independent and has maximal possible weight an optimal subset of the matroid.
Because the weight $$ w(x) $$ of any element $$ x \in S $$ is positive, an optimal subset is always a maximal independent subset.  
For example, in the minimum-spanning-tree problem, we are given a connected undirected graph $$ G = (V, \ E) $$ and a length function $$ w $$ such that $$ w(e) $$ is the positive length of edge $$ e $$.
(The term "length" refers to the original edge weights for the graph, reserving the term "weight" to refer to the weights in the associated matroid.)
We wish to find a subset of the edges that connects all of the vertices together and has minimum total length.
To view this as a problem of finding an optimal subset of a matroid, consider the weighted matroid $$ M_G $$ with weight function $$ w' $$, where $$ w'(e) = w_0 - w(e) $$ and $$ w_0 $$ is larger than the maximum length of any edge.
In this weight matroid, all weights are positive and an optimal subset is a spanning tree of minimum total length in the original graph.
Each maximal independent subset $$ A $$ corresponds to a spanning tree with
$$ |V| - 1 $$
edges, and since

$$
\begin{align*}
    w'(A) &= \sum_{e \in A} w'(e) \\
          &= \sum_{e \in A} (w_0 - w(e)) \\
          &= (|V| - 1) w_0 - \sum_{e \in A} w(e) \\
          &= (|V| - 1) w_0 - w(A)
\end{align*}
$$

for any maximal independent subset $$ A $$, an independent subset that maximizes the quantity $$ w'(A) $$ must minimize $$ w(A) $$.
Thus, any algorithm that can find an optimal subset $$ A $$ in an arbitrary matroid can solve the minimum-spanning-tree problem.

>GREEDY($$ M, \ w $$)  
>01&nbsp; $$ A = \emptyset $$  
>02&nbsp; sort $$ M.S $$ into monotonically decreasing order by weight $$ w $$  
>03&nbsp; for each $$ x \in M.S $$, taken in monotonically decreasing order by weight $$ w(x) $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ A \cup \{ x \} \in M.\mathcal{I} $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ A = A \cup \{ x \} $$  
>06&nbsp; return $$ A $$

The algorithm above is greedy because it considers in turn each element $$ x \in S $$, in order of monotonically decreasing weight, and immediately adds it to the set $$ A $$ being accumulated if $$ A \cup \{ x \} $$ is independent.
Line 4 checks whether adding each element $$ x $$ to $$ A $$ would maintain $$ A $$ as an independent set.
If $$ A $$ would remain independent, then line 5 adds $$ x $$ to $$ A $$.
Otherwise, $$ x $$ is discarded.
Since the empty set is independent, and since each iteration of the for loop maintains $$ A $$'s independence, the subset $$ A $$ is always independent, by induction.
Therefore, GREEDY always returns an independent subset $$ A $$.
In fact, $$ A $$ is a subset of maximum possible weight, so that $$ A $$ is an optimal subset.  
To analyze the running time of GREEDY, let $$ n $$ denote $$ |S| $$.
The sorting phase of GREEDY takes time $$ O(n \lg n) $$.
Line 4 executes exactly $$ n $$ times, once for each element $$ S $$.
Each execution of line 4 requires a check on whether or not the set $$ A \cup \{ x \} $$ is independent.
If each such check takes time $$ O(f(n)) $$, the entire algorithm runs in time $$ O(n \lg n + n f(n)) $$.

### Lemma 16.7 (Greedy-choice property of matroids)

Suppose that $$ M = (S, \ \mathcal{I}) $$ is a weighted matroid with weight function $$ w $$ and that $$ S $$ is sorted into monotonically decreasing order by weight.
Let $$ x $$ be the first element of $$ S $$ such that $$ \{ x \} $$ is independent, if any such $$ x $$ exists, then there exists an optimal subset $$ A $$ of $$ S $$ that contains $$ x $$.

**Proof**  
If no such $$ x $$ exists, then the only independent subset is the empty set and the lemma is vacuously true.
Otherwise, let $$ B $$ be any nonempty optimal subset.
Assume that $$ x \notin B $$.
We claim that no element of $$ B $$ has weight greater than $$ w(x) $$.
To see why, observe that $$ y \in B $$ implies that $$ \{ y \} $$ is independent, since $$ B \in \mathcal{I} $$ and $$ \mathcal{I} $$ is hereditary.
Our choice of $$ x $$ therefore ensures that $$ w(x) \ge w(y) $$ for any $$ y \in B $$.  
Construct the set $$ A $$ as follows.
Begin with $$ A = \{ x \} $$.
By the choice of $$ x $$, set $$ A $$ is independent.
Using the exchange property, repeatedly find a new element of $$ B $$ that we can add to $$ A $$ until $$ |A| = |B| $$, while preserving the independence of $$ A $$.
At that point, $$ A $$ and $$ B $$ are the same except that $$ A $$ has $$ x $$ and $$ B $$ has some other element $$ y $$.
That is, $$ A = B - \{ y \} \cup \{ x \} $$ for some $$ y \in B $$, and so

$$
\begin{align*}
    w(A) = w(B) - w(y) + w(x) \ge w(B)
\end{align*}
$$

Because set $$ B $$ is optimal, set $$ A $$, which contains $$ x $$, must also be optimal. $$ \blacksquare $$

### Lemma 16.8

Let $$ M = (S, \ \mathcal{I}) $$ be any matroid.
If $$ x $$ is an element of $$ S $$ that is an extension of some independent subset $$ A $$ of $$ S $$, then $$ x $$ is also an extension of $$ \emptyset $$.

**Proof**  
Since $$ x $$ is an extension of $$ A $$, we have that $$ A \cup \{ x \} $$ is independent.
Since $$ \mathcal{I} $$ is hereditary, $$ \{ x \} $$ must be independent.
Thus, $$ x $$ is an extension of $$ \emptyset $$. $$ \blacksquare $$

### Corollary 16.9

Let $$ M = (S, \ \mathcal{I}) $$ be any matroid.
If $$ x $$ is an element of $$ S $$ such that $$ x $$ is not an extension of $$ \emptyset $$, then $$ x $$ is not an extension of any independent subset $$ A $$ of $$ S $$.

**Proof**  
The contrapositive of Lemma 16.8. $$ \blacksquare $$

Corollary 16.9 says that any element that cannot be used immediately can never be used.
Therefore, GREEDY cannot make an error by passing over any initial elements in $$ S $$ that are not an extension of $$ \emptyset $$, since they can never be used.

### Lemma 16.10 (Optimal-substructure property of matroids)

Let $$ x $$ be the first element of $$ S $$ chosen by GREEDY for the weighted matroid $$ M = (S, \ \mathcal{I}) $$.
The remaining problem of finding a maximum-weight independent subset containing $$ x $$ reduces to finding a maximum-weight independent subset of the weighted matroid $$ M' = (S', \ \mathcal{I}') $$, where

$$
\begin{align*}
    S' &= \{ y \in S : \{ x, \ y \} \in \mathcal{I} \} \\
    \mathcal{I}' &= \{ B \subseteq S - \{ x \} : B \cup \{ x \} \in \mathcal{I} \}
\end{align*}
$$

and the weight function for $$ M' $$ is the weight function for $$ M $$, restricted to $$ S' $$.
We call $$ M' $$ the contraction of $$ M $$ by the element $$ x $$.

**Proof**  
If $$ A $$ is any maximum-weight independent subset of $$ M $$ containing $$ x $$, then $$ A' = A - \{ x \} $$ is an independent subset of $$ M' $$.
Conversely, any independent subset $$ A' $$ of $$ M' $$ yields an independent subset $$ A = A' \cup \{ x \} $$ of $$ M $$.
Since we have in both cases that $$ w(A) = w(A') + w(x) $$, a maximum-weight solution in $$ M $$ containing $$ x $$ yields a maximum-weight solution in $$ M' $$, and vice versa. $$ \blacksquare $$

### Theorem 16.11 (Correctness of the greedy algorithm on matroids)

If $$ M = (S, \ \mathcal{I}) $$ is a weighted matroid with weight function $$ w $$, then GREEDY($$ M, \ w $$) returns an optimal subset.

**Proof**  
By Corollary 16.9, any elements that GREEDY passes over initially because they are not extensions of $$ \emptyset $$ are not useful.
Once GREEDY selects the first element $$ x $$, Lemma 16.7 implies that the algorithm does not err by adding $$ x $$ to $$ A $$, since there exists an optimal subset containing $$ x $$.
Finally, Lemma 16.10 implies that the remaining problem is one of finding an optimal subset in the matroid $$ M' $$ that is the contraction of $$ M $$ by $$ x $$.
After the procedure GREEDY sets $$ A $$ to $$ \{ x \} $$, we can interpret all of its remaining steps as acting in the matroid $$ M' = (S', \ \mathcal{I}') $$, because $$ B $$ is independent in $$ M' $$ if and only if $$ B \cup \{ x \} $$ is independent in $$ M $$, for all sets $$ B \in \mathcal{I}' $$.
Thus, the subsequent operation of GREEDY will find a maximum-weight independent subset for $$ M' $$, and the overall operation of GREEDY will find a maximum-weight independent subset for $$ M $$. $$ \blacksquare $$

## 16.5 A task-scheduling problem as a matroid

A unit-time task is a job that requires exactly one unit of time to compute.
Given a finite set $$ S $$ of unit-time tasks, a schedule for $$ S $$ is a permutation of $$ S $$ specifying the order in which to perform these tasks.
The first task in the schedule begins at time 0 and finishes at time 1, the second task begins at time 1 and finishes at time 2, and so on.  
The problem of scheduling unit-time tasks with deadlines and penalties for a single processor has the following inputs:

- a set $$ S = \{ a_1, \ a_2, \dots, \ a_n \} $$ of $$ n $$ unit-time tasks;
- a set of $$ n $$ integer deadlines $$ d_1, \ d_2, \dots, \ d_n $$, such that each $$ d_i $$ satisfies $$ 1 \le d_i \le n $$ and task $$ a_i $$ is supposed to finish by time $$ d_i $$; and
- a set of $$ n $$ nonnegative penalties $$ w_1, \ w_2, \dots, \ w_n $$, such that we incur a penalty of $$ w_i $$ if task $$ a_i $$ is not finished by time $$ d_i $$.

We wish to find a schedule for $$ S $$ that minimizes the total penalty incurred for missed deadlines.

Consider a given schedule.
A task is late in this schedule if it finishes after its deadline.
Otherwise, the task is early in the schedule.
We can always transform an arbitrary schedule into early-first form, in which the early tasks precede the late tasks.
To see why, note that if some early task $$ a_i $$ follows some late task $$ a_j $$, then we can switch the positions of $$ a_i $$ and $$ a_j $$, and $$ a_i $$ will still be early and $$ a_j $$ will still be late.  
Furthermore, we claim that we can always transform an arbitrary schedule into canonical form, in which the early tasks precede the late tasks and we schedule the early tasks in order of monotonically increasing deadlines.
To do so, we put the schedule into early-first form.
Then, as long as there exist two early tasks $$ a_i $$ and $$ a_j $$ finishing at respective times $$ k $$ and $$ k + 1 $$ in the schedule such that $$ d_j < d_i $$, we swap the positions of $$ a_i $$ and $$ a_j $$.
Since $$ a_j $$ is early before the swap, $$ k + 1 \le d_j $$.
Therefore, $$ k + 1 < d_i $$, and so $$ a_i $$ is still early after the swap.
Because task $$ a_j $$ is moved earlier in the schedule, it remains early after swap.  
The search for an optimal schedule thus reduces to finding a set $$ A $$ of tasks that we assign to be early in the optimal schedule.

We say that a set $$ A $$ of tasks is independent if there exists a schedule for these tasks such that no tasks are late.
Clearly, the set of early tasks for a schedule forms an independent set of tasks.
Let $$ \mathcal{I} $$ denote the set of all independent sets of tasks.  
Consider the problem of determining whether a given set $$ A $$ of tasks is independent.
For $$ t = 0, \ 1, \ 2, \dots, \ n $$, let $$ N_t(A) $$ denote the number of tasks in $$ A $$ whose deadline is $$ t $$ or earlier.
Note that $$ N_0(A) = 0 $$ for any set $$ A $$.

### Lemma 16.12

For any set of tasks $$ A $$, the following statements are equivalent.

1. The set $$ A $$ is independent.
2. For $$ t = 0, \ 1, \ 2, \dots, \ n $$, we have $$ N_t(A) \le t $$.
3. If the tasks in $$ A $$ are scheduled in order of monotonically increasing deadlines, then no task is late.

**Proof**  
(1) $$ \Rightarrow $$ (2): We prove the contrapositive: if $$ N_t(A) > t $$ for some $$ t $$, then there is no way to make a schedule with no late tasks for set $$ A $$, because more than $$ t $$ tasks must finish before time $$ t $$.  
(2) $$ \Rightarrow $$ (3): There is no way to "get stuck" when scheduling the tasks in order of monotonically increasing deadlines, since (2) implies that the $$ i $$th largest deadline is at least $$ i $$.  
(3) $$ \Rightarrow $$ (1): Trivial. $$ \blacksquare $$

Using property 2 of Lemma 16.12, we can easily compute whether or not a given set of tasks is independent.

The problem of minimizing the sum of the penalties of the late tasks is the same as the problem of maximizing the sum of the penalties of the early tasks.
The following theorem ensures that we can use the greedy algorithm to find an independent set $$ A $$ of tasks with the maximum total penalty.

### Theorem 16.13

If $$ S $$ is a set of unit-time tasks with deadlines, and $$ \mathcal{I} $$ is the set of all independent sets of tasks, then the corresponding system $$ (S, \ \mathcal{I}) $$ is a matroid.

**Proof**  
Every subset of an independent set of tasks is certainly independent.
To prove the exchange property, suppose that $$ B $$ and $$ A $$ are independent sets of tasks and that $$ |B| > |A| $$.
Let $$ k $$ be the largest $$ t $$ such that $$ N_t(B) \le N_t(A) $$.
(Such a value of $$ t $$ exists, since $$ N_0(A) = N_0(B) = 0 $$.)
Since $$ N_n(B) = |B| $$ and $$ N_n(A) = |A| $$, but $$ |B| > |A| $$, we must have that $$ k < n $$ and that $$ N_j(B) > N_j(A) $$ for all $$ j $$ in the range $$ k + 1 \le j \le n $$.
Therefore, $$ B $$ contains more tasks with deadline $$ k + 1 $$ than $$ A $$ does.
Let $$ a_i $$ be a task in $$ B - A $$ with deadline $$ k + 1 $$.  
Let $$ A' = A \cup \{ a_i \} $$.
Then, $$ A' $$ must be independent by property 2 of Lemma 16.12.
For $$ 0 \le t \le k $$, we have $$ N_t(A') = N_t(A) \le t $$, since $$ A $$ is independent.
For $$ k < t \le n $$, we have $$ N_t(A') \le N_t(B) \le t $$, since $$ B $$ is independent.
Therefore, $$ A' $$ is independent. $$ \blacksquare $$

By Theorem 16.11, we can use a greedy algorithm to find a maximum-weight independent set of tasks $$ A $$.
We can then create an optimal schedule having the tasks in $$ A $$ as its early tasks.
The running time is $$ O(n^2) $$ using GREEDY, since each of the $$ O(n) $$ independence checks made by that algorithm taeks time $$ O(n) $$.

![Desktop View](/assets/img/16-Greedy-Algorithms/Figure 16.4.png){: width="700" height="200" }
_**Figure 16.4** An instance of the problem of scheduling unit-time tasks with deadlines and penalties for a single processor._

Figure 16.4 demonstrates an example of the problem of scheduling unit-time tasks with deadlines and penalties for a single processor.
In this example, the greedy algorithm selects, in order, tasks $$ a_1, \ a_2, \ a_3, $$ and $$ a_4 $$, then rejects $$ a_5 $$ (because $$ N_4(\{ a_1, \ a_2, \ a_3, \ a_4, \ a_5 \}) = 5 $$) and $$ a_6 $$ (because $$ N_4(\{ a_1, \ a_2, \ a_3, \ a_4, \ a_6 \}) = 5 $$), and accepts $$ a_7 $$. The final optimal schedule is

$$
\begin{align*}
    \langle a_2, \ a_4, \ a_1, \ a_3, \ a_7, \ a_5, \ a_6 \rangle
\end{align*}
$$

which has a total penalty incurred of $$ w_5 + w_6 = 50 $$.

---

Sources:
- [Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press.](https://a.co/d/62TQVO9)