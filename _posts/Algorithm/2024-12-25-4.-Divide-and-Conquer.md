---
title: 4. Divide-and-Conquer
# description: Short summary of the post
date: 2024-12-05 16:23
categories: [Computer Science, Algorithm]
tags: [divide-and-conquer, recurrence, maximum-subarray-problem, matrix-multiplication, strassen's-algorithm, substitution-method]     # TAG names should always be lowercase
math: true
pin: false
---

In the divide-and-conquer paradigm, we typically solve a problem recursively, applying three steps at each level of the recursion:

1. Divide the problem into a number of subproblems that are smaller instances of the same problem.
2. Conquer the subproblems by solving them recursively, unless the subproblem sizes are small enough.
3. Combine the solutions to the subproblems into the solution for the original problem.

### Recurrences

A recurrence is an equation or inequality that describes a function in terms of its value on smaller inputs.
This chapter offers three methods for solving recurrences:

- Substitution method: we guess a bound and then use mathematical induction to prove our guess correct.
- Recursion-tree method: it converts the recurrence into a tree whose nodes represent the costs incurred at various levels of the recursion.
- Master method: it provides bounds for recurrences of the form

$$
\begin{align*}
    T(n) = a T(n/b) + f(n)
\end{align*}
\label{eq:1}
\tag{4.1}
$$

<p class="indented-paragraph">
where $$ a \ge 1 $$, $$ b > 1 $$, and $$ f(n) $$ is a given function.
A recurrence of the form in equation \eqref{eq:1} characterizes a divide-and-conquer algorithm that creates $$ a $$ subproblems, each of which is $$ 1/b $$ the size of the original problem, and in which the divide and combine steps together take $$ f(n) $$ time.
</p>

## 4.1 The maximum-subarray problem

Suppose that you have been offered the opportunity to invest in a company.
The stock price of the company is volatile.
You are allowed to buy one unit of stock only one time and then sell it at a later date.
Also, you are allowed to learn what the price of the stock will be in the future.
Your goal is to maximize your profit.

### A brute-force solution

We can easily devise a brute-force solution to this problem: just try every possible pair of buy and sell dates in which the buy date precedes the sell date.
A period of $$ n $$ days has $$ \binom{n}{2} $$ such pairs of dates.
Since $$ \binom{n}{2} $$ is $$ \Theta(n^2) $$, and it requires a constant time to evaluate each pair of dates, this approach takes $$ \Omega(n^2) $$ time.

### Maximum subarray

Instead of looking at the daily prices, consider the daily change in price, where the change on day $$ i $$ is the difference between the prices after day $$ i - 1 $$ and after day $$ i $$.
If we treat the sequence of the daily change in price as an array $$ A $$, we now want to find the nonempty, contiguous subarray of $$ A $$ whose values have the largest sum.
We call this contiguous subarray the maximum subarray.

### A solution using divide-and-conquer

Suppose we want to find a maximum subarray of the subarray $$ A[low..high] $$.
Divide-and-conquer suggests that we divide the subarray into two subarrays of as equal size as possible.
That is, we find the midpoint, say $$ mid $$, of the subarray, and consider the subarrays $$ A[low..mid] $$ and $$ A[mid + 1..high] $$.
Then, any contiguous subarray $$ A[i..j] $$ of $$ A[low..high] $$ must lie in exactly one of the following places:

- entirely in the subarray $$ A[low..mid] $$, so that $$ low \le i \le j \le mid $$.
- entirely in the subarray $$ A[mid + 1..high] $$, so that $$ mid < i \le j \le high $$, or
- crossing the midpoint, so that $$ low \le i \le mid < j \le high $$.

Therefore, a maximum subarray of $$ A[low..high] $$ must lie in exactly one of these places.
In fact, a maximum subarray of $$ A[low..high] $$ must have the greatest sum over all subarrays entirely in $$ A[low..mid] $$, entirely in $$ A[mid + 1..high] $$, or crossing the midpoint.
We can find maximum subarrays of $$ A[low..mid] $$ and $$ A[mid + 1..high] $$ recursively, because these two subproblems are smaller instances of the problem of finding a maximum subarray.
Thus, all that is left to do is find a maximum subarray that crosses the midpoint, and take a subarray with the largest sum of the three.

We can find a maximum subarray crossing the midpoint in time linear in the size of the subarray $$ A[low..high] $$.
This problem is not a smaller instance of our original problem, because it has the added restriction that the subarray it chooses must cross the midpoint.
Any subarray crossing the midpoint is itself made of two subarrays $$ A[i..mid] $$ and $$ A[mid + 1..j] $$, where $$ low \le i \le mid $$ and $$ mid < j \le high $$.
Therefore, we just need to find maximum subarrays of the form $$ A[i..mid] $$ and $$ A[mid + 1..j] $$ and then combine them.

>FIND-MAX-CROSSING-SUBARRAY($$ A, \ low, \ mid, \ high $$)  
>01&nbsp; $$ left\text{-}sum = -\infty $$  
>02&nbsp; $$ sum = 0 $$  
>03&nbsp; for $$ i = mid $$ downto $$ low $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ sum = sum + A[i] $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ sum > left\text{-}sum $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ left\text{-}sum = sum $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ max\text{-}left = i $$  
>08&nbsp; $$ right\text{-}sum = -\infty $$  
>09&nbsp; $$ sum = 0 $$  
>10&nbsp; for $$ j = mid + 1 $$ to $$ high $$  
>11&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ sum = sum + A[j] $$  
>12&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ sum > right\text{-}sum $$  
>13&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ right\text{-}sum = sum $$  
>14&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ max\text{-}right = j $$  
>15&nbsp; return ($$ max\text{-}left, \ max\text{-}right, \ left\text{-}sum + right\text{-}sum $$)

This procedure works as follows.
Lines 1–7 find a maximum subarray of the left half, $$ A[low..mid] $$.
Since this subarray must contain $$ A[mid] $$, the for loop of lines 3–7 starts the index $$ i $$ at $$ mid $$ and works down to $$ low $$, so that every subarray it considers is of the form $$ A[i..mid] $$.
Whenever we find, in line 5, a subarray $$ A[i..mid] $$ with a sum of values greater than $$ left\text{-}sum $$, we update $$ left\text{-}sum $$ to this subarray's sum in line 6, and update the variable $$ max\text{-}left $$ to record this index $$ i $$ in line 7.
Lines 8–14 work analogously for the right half, $$ A[mid + 1..high] $$.
Here, the for loop of lines 10–14 considers every subarray in the form $$ A[mid + 1..j] $$.
Finally, line 15 returns the indices $$ max\text{-}left $$ and $$ max\text{-}right $$ that demarcate a maximum subarray crossing the midpoint, along with the sum $$ left\text{-}sum + right\text{-}sum $$ of the values in the subarray $$ A[max\text{-}left..max\text{-}right] $$.

If the subarray $$ A[low..high] $$ contains $$ n $$ entries (so that $$ n = high - low + 1 $$), the call FIND-MAX-CROSSING-SUBARRAY($$ A, \ low, \ mid, \ high $$) takes $$ \Theta(n) $$ time.
Since each iteration of each of the two for loops takes $$ \Theta(1) $$ time, we just need to count up how many iterations there are altogether.
The for loop of lines 3–7 makes $$ mid - low + 1 $$ iterations, and the for loop of lines 10–14 makes $$ high - mid $$ iterations, and so the total number of iterations is

$$
\begin{align*}
    (mid - low + 1) + (high - mid) &= high - low + 1 \\
                                   &= n
\end{align*}
$$

>FIND-MAXIMUM-SUBARRAY($$ A, \ low, \ high $$)  
>01&nbsp; if $$ high == low $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return ($$low, \ high, \ A[low] $$)    
>03&nbsp; else $$ mid = \lfloor (low + high) / 2 \rfloor $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ (left\text{-}low, \ left\text{-}high, \ left\text{-}sum) = $$ FIND-MAXIMUM-SUBARRAY($$ A, \ low, \ mid $$)  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ (right\text{-}low, \ right\text{-}high, \ right\text{-}sum) = $$ FIND-MAXIMUM-SUBARRAY($$ A, \ mid + 1, \ high $$)  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ (cross\text{-}low, \ cross\text{-}high, \ cross\text{-}sum) = $$ FIND-MAX-CROSSING-SUBARRAY($$ A, \ low, \ mid, \ high $$)  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ left\text{-}sum \ge right\text{-}sum $$ and $$ left\text{-}sum \ge cross\text{-}sum $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ($$ left\text{-}low, \ left\text{-}high, \ left\text{-}sum $$)  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;elseif $$ right\text{-}sum \ge left\text{-}sum $$ and $$ right\text{-}sum \ge cross\text{-}sum $$  
>10&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return ($$ right\text{-}low, \ right\text{-}high, \ right\text{-}sum $$)  
>11&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;else return ($$ cross\text{-}low, \ cross\text{-}high, \ cross\text{-}sum $$)

The initial call FIND-MAXIMUM-SUBARRAY($$ A, \ 1, \ A.length $$) will find a maximum subarray of $$ A[1..n] $$.
Similar to FIND-MAX-CROSSING-SUBARRAY, the recursive procedure FIND-MAXIMUM-SUBARRAY returns a tuple containing the indices that demarcate a maximum subarray, along with the sum of the values in a maximum subarray.
Line 1–2 tests for the base case, where the subarray has just one element, and returns a tuple with the starting and ending indices of it, along with its value.
Line 3–11 handle the recursive case.
Line 3 does the divide part, computing the index $$ mid $$ of the midpoint.
Because we know that the subarray $$ A[low..high] $$ contains at least two elements, each of the left and right subarrays ($$ A[low..mid] $$ and $$ A[mid + 1..high] $$) must have at least one element.
Lines 4 and 5 conquer by recursively finding maximum subarrays within the left and right subarrays, respectively.
Lines 6–11 form the combine part.
Line 6 finds a maximum subarray that crosses the midpoint.
Recall that finding the maximum crossing subarray is not a smaller instance of the original problem; thus we consider it to be in the combine part.
Line 7 tests whether the left subarray contains a subarray with the maximum sum, and line 8 returns that maximum subarray.
Otherwise, line 9 tests whether the right subarray contains a subarray with the maximum sum, and line 10 returns that maximum subarray.
If neither the left nor right subarray contain a subarray achieving the maximum sum, then a maximum subarray must cross the midpoint, and line 11 returns it.

### Analyzing the divide-and-conquer algorithm

Next we set up a recurrence that describes the running time of the recursive FIND-MAXIMUM-SUBARRAY procedure.
We denote by $$ T(n) $$ the running time of the procedure on a subarray of $$ n $$ elements.
The base case, when $$ n = 1 $$, is easy: line 2 takes constant time, and so

$$
\begin{align*}
    T(1) = \Theta(1)
\end{align*}
\label{eq:2}
\tag{4.2}
$$

The recursive case occurs when $$ n > 1 $$.
Each of the subproblems solved in lines 4 and 5 is on a subarray of $$ n / 2 $$ elements, and so we spend $$ T(n / 2) $$ time solving each of them.
Because we have to solve two subproblems—for the left subarray and for the right subarray—the contribution to the running time from lines 4 and 5 comes to $$ 2 T(n / 2) $$.
The call to FIND-MAX-CROSSING-SUBARRAY in line 6 takes $$ \Theta(n) $$ time.
For the recursive case, therefore, we have

$$
\begin{align*}
    T(n) &= 2 T(n / 2) + \Theta(n)
\end{align*}
\label{eq:3}
\tag{4.3}
$$

Combining equations \eqref{eq:2} and \eqref{eq:3} gives us a recurrence for the running time $$ T(n) $$ of FIND-MAXIMUM-SUBARRAY:

$$
\begin{align*}
    T(n) =
    \begin{cases}
        \Theta(1) & \text{if } n = 1 \\
        2 T(n / 2) + \Theta(n) & \text{if } n > 1
    \end{cases}
\end{align*}
\label{eq:4}
\tag{4.4}
$$

By applying the master method or recursion-tree method, we can observe that this recurrence has the solution $$ T(n) = \Theta(n \lg n) $$.

## 4.2 Strassen's algorithm for matrix multiplication

If $$ A = (a_{ij}) $$ and $$ B = (b_{ij}) $$ are square $$ n \times n $$ matrices, then in the product $$ C = A \cdot B $$, we define the entry $$ c_{ij} $$, for $$ i = 1, \ 2, \dots, \ n $$, by

$$
\begin{align*}
    c_{ij} = \sum_{k = 1}^{n} a_{ik} \cdot b_{kj}
\end{align*}
\label{eq:5}
\tag{4.5}
$$

We must compute $$ n^2 $$ matrix entries, and each is the sum of $$ n $$ values.

>SQUARE-MATRIX-MULTIPLY($$ A, \ B $$)  
>01&nbsp; $$ n = A.rows $$  
>02&nbsp; let $$ C $$ be a new $$ n \times n $$ matrix  
>03&nbsp; for $$ i = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ j = 1 $$ to $$ n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = 0 $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ k = 1 $$ to $$ n $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = c_{ij} + a_{ik} \cdot b_{kj} $$  
>08&nbsp; return $$ C $$


The SQUARE-MATRIX-MULTIPLY procedure works as follows.
The for loop of lines 3–7 computes the entries of each row $$ i $$, and within a given row $$ j $$, the for loop of lines 4–7 computes each of the entries $$ c_{ij} $$, for each column $$ j $$.
Line 5 initializes $$ c_{ij} $$ to $$ 0 $$ as we start computing the sum given in equation \eqref{eq:5}, and each iteration of the for loop of lines 6–7 adds in one more term of equation \eqref{eq:5}.
Because each of the triply-nested for loops runs exactly $$ n $$ iterations, and each execution of line 7 takes constant time, the SQUARE-MATRIX-MULTIPLY procedure takes $$ \Theta(n^3) $$ time.

### A simple divide-and-conquer algorithm

Now, before diving into Strassen's method, let's examine how to apply a divide-and-conquer algorithm to compute matrix multiplication.
To keep things simple, we assume that $$ n $$ is an exact power of 2 in each of the $$ n \times n $$ matrices.
Suppose that we partition each of $$ A $$, $$ B $$, and $$ C $$ into four $$ n/2 \times n/2 $$ matrices

$$
\begin{align*}
    A =
    \begin{pmatrix}
        A_{11} & A_{12} \\
        A_{21} & A_{22}
    \end{pmatrix}
    && B =
    \begin{pmatrix}
        B_{11} & B_{12} \\
        B_{21} & B_{22}
    \end{pmatrix}
    && C =
    \begin{pmatrix}
        C_{11} & C_{12} \\
        C_{21} & C_{22}
    \end{pmatrix}
\end{align*}
\label{eq:6}
\tag{4.6}
$$

so that we rewrite the equation $$ C = A \cdot B $$ as

$$
\begin{align*}
    \begin{pmatrix}
        C_{11} & C_{12} \\
        C_{21} & C_{22}
    \end{pmatrix}
    =
    \begin{pmatrix}
        A_{11} & A_{12} \\
        A_{21} & A_{22}
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        B_{11} & B_{12} \\
        B_{21} & B_{22}
    \end{pmatrix}
\end{align*}
\label{eq:7}
\tag{4.7}
$$

Equation \eqref{eq:7} corresponds to the four equations

$$
\begin{align*}
    C_{11} = A_{11} \cdot B_{11} + A_{12} \cdot B_{21}
\end{align*}
\label{eq:8}
\tag{4.8}
$$

$$
\begin{align*}
    C_{12} = A_{11} \cdot B_{12} + A_{12} \cdot B_{22}
\end{align*}
\label{eq:9}
\tag{4.9}
$$

$$
\begin{align*}
    C_{21} = A_{21} \cdot B_{11} + A_{22} \cdot B_{21}
\end{align*}
\label{eq:10}
\tag{4.10}
$$

$$
\begin{align*}
    C_{22} = A_{21} \cdot B_{12} + A_{22} \cdot B_{22}
\end{align*}
\label{eq:11}
\tag{4.11}
$$

Each of these four equations specifies two multiplications of $$ n/2 \times n/2 $$ matrices and the addition of their $$ n/2 \times n/2 $$ products.
We can use these equations to create a straightforward, recursive, divide-and-conquer algorithm:

>SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A, \ B $$)  
>01&nbsp; $$ n = A.rows $$  
>02&nbsp; let $$ C $$ be a new $$ n \times n $$ matrix  
>03&nbsp; if $$ n == 1 $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ c_{11} = a_{11} \cdot b_{11} $$  
>05&nbsp; else partition $$ A $$, $$ B $$, and $$ C $$ as in equations \eqref{eq:6}  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ C_{11} = $$ SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{11}, \ B_{11} $$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{12}, \ B_{21} $$)  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ C_{12} = $$ SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{11}, \ B_{12} $$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{12}, \ B_{22} $$)  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ C_{21} = $$ SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{21}, \ B_{11} $$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{22}, \ B_{21} $$)  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ C_{22} = $$ SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{21}, \ B_{12} $$) + SQUARE-MATRIX-MULTIPLY-RECURSIVE($$ A_{22}, \ B_{22} $$)   
>10&nbsp; return $$ C $$

Now, we derive a recurrence to characterize the running time of SQUARE-MATRIX-MULTIPLY-RECURSIVE.
Let $$ T(n) $$ be the time to multiply two $$ n \times n $$ matrices using the procedure.
In the base case, when $$ n = 1 $$,

$$
\begin{align*}
    T(1) = \Theta(1)
\end{align*}
\label{eq:12}
\tag{4.12}
$$

The recursive case occurs when $$ n > 1 $$.
The partitioning of the matrices in line 5 can be executed in $$ \Theta(1) $$ time, using index calculations and referring to the original matrix.
In lines 6–9, we recursively call SQUARE-MATRIX-MULTIPLY-RECURSIVE a total of eight times.
Because each recursive call multiplies two $$ n/2 \times n/2 $$ matrices, thereby contributing $$ T(n / 2) $$ to the overall running time, the time taken by all eight recursive calls is $$ 8 T(n / 2) $$.
We also must account for the four matrix additions in lines 6–9.
Each of these matrices contains $$ n^2 / 4 $$ entries, and so each of the four matrix additions takes $$ \Theta(n^2) $$ time.
Therefore, the total time spent adding matrices in lines 6–9 is $$ \Theta(n^2) $$, and the total time for the recursive case is

$$
\begin{align*}
    T(n) &= \Theta(1) + 8 T(n / 2) + \Theta(n^2) \\
         &= 8 T(n / 2) + \Theta(n^2)
\end{align*}
\label{eq:13}
\tag{4.13}
$$

Combining equations \eqref{eq:12} and \eqref{eq:13} gives us the recurrence for the running time of SQUARE-MATRIX-MULTIPLY-RECURSIVE:

$$
\begin{align*}
    T(n) =
    \begin{cases}
        \Theta(1) & \text{if } n = 1 \\
        8 T(n / 2) + \Theta(n^2) & \text{if } n > 1
    \end{cases}
\end{align*}
\label{eq:14}
\tag{4.14}
$$

The recurrence \eqref{eq:14} has the solution $$ T(n) = \Theta(n^3) $$.
Thus, this simple divide-and-conquer approach is no faster than the straightforward SQUARE-MATRIX-MULTIPLY procedure.

### Strassen's method

The key to Strassen's method is, instead of performing eight recursive multiplications of $$ n/2 \times n/2 $$ matrices, it performs only seven.
It has four steps:

1. Divide the input matrices $$ A $$ and $$ B $$ and output matrix $$ C $$ into $$ n/2 \times n/2 $$ submatrices, as in equation \eqref{eq:6}.
This step takes $$ \Theta(1) $$ time by index calculation.
2. Create 10 matrices $$ S_1, \ S_2, \dots, \ S_{10} $$, each of which is $$ n/2 \times n/2 $$ and is the sum or difference of two matrices created in step 1.
We can create all 10 matrices in $$ \Theta(n^2) $$ time.
3. Using the submatrices created in step 1 and the 10 matrices created in step 2, recursively compute seven matrix products $$ P_1, \ P_2, \dots, \ P_7 $$.
Each matrix $$ P_i $$ is $$ n/2 \times n/2 $$.
4. Compute the desired submatrices $$ C_{11}, \ C_{12}, \ C_{21}, \ C_{22} $$ of the result matrix $$ C $$ by adding and subtracting various combinations of the $$ P_i $$ matrices.
We can compute all four submatrices in $$ \Theta(n^2) $$ time.

To set up a recurrence for the running time of Strassen's method, let us assume that once the matrix size $$ n $$ gets down to $$ 1 $$, we perform a simple scalar multiplication, just as in line 4 of SQUARE-MATRIX-MULTIPLY-RECURSIVE.
When $$ n > 1 $$, steps 1, 2, and 4 take a total of $$ \Theta(n^2) $$ time, and step 3 requires us to perform seven multiplications of $$ n/2 \times n/2 $$ matrices.
Hence, we obtain the following recurrence for the running time $$ T(n) $$ of Strassen's algorithm.

$$
\begin{align*}
    T(n) =
    \begin{cases}
        \Theta(1) & \text{if } n = 1 \\
        7 T(n / 2) + \Theta(n^2) & \text{if } n > 1
    \end{cases}
\end{align*}
\label{eq:15}
\tag{4.15}
$$

By the master method, recurrence \eqref{eq:15} has the solution $$ T(n) = \Theta(n^{\lg 7}) $$.

We now proceed to describe the details.
In step 2, we create the following 10 matrices:

$$
\begin{align*}
    S_1 &= B_{12} - B_{22} \\
    S_2 &= A_{11} + A_{12} \\
    S_3 &= A_{21} + A_{22} \\
    S_4 &= B_{21} - B_{11} \\
    S_5 &= A_{11} + A_{22} \\
    S_6 &= B_{11} + B_{22} \\
    S_7 &= A_{12} - A_{22} \\
    S_8 &= B_{21} + B_{22} \\
    S_9 &= A_{11} - A_{21} \\
    S_{10} &= B_{11} + B_{12}
\end{align*}
$$

Since we must add or subtract $$ n/2 \times n/2 $$ matrices 10 times, this step take $$ \Theta(n^2) $$ time.  
In step 3, we recursively multiply $$ n/2 \times n/2 $$ matrices seven times to compute the following $$ n/2 \times n/2 $$ matrices, each of which is the sum or difference of products of $$ A $$ and $$ B $$ submatrices:

$$
\begin{align*}
    P_1 &= A_{11} \cdot S_1 = A_{11} \cdot B_{12} - A_{11} \cdot B_{22} \\
    P_2 &= S_2 \cdot B_{22} = A_{11} \cdot B_{22} + A_{12} \cdot B_{22} \\
    P_3 &= S_3 \cdot B_{11} = A_{21} \cdot B_{11} + A_{22} \cdot B_{11} \\
    P_4 &= A_{22} \cdot S_4 = A_{22} \cdot B_{21} - A_{22} \cdot B_{11} \\
    P_5 &= S_5 \cdot S_6 = A_{11} \cdot B_{11} + A_{11} \cdot B_{22} + A_{22} \cdot B_{11} + A_{22} \cdot B_{22} \\
    P_6 &= S_7 \cdot S_8 = A_{12} \cdot B_{21} + A_{12} \cdot B_{22} - A_{22} \cdot B_{21} - A_{22} \cdot B_{22} \\
    P_7 &= S_9 \cdot S_{10} = A_{11} \cdot B_{11} + A_{11} \cdot B_{12} - A_{21} \cdot B_{11} - A_{21} \cdot B_{12}
\end{align*}
$$

Step 4 adds and subtracts the $$ P_i $$ matrices created in step 3 to construct the four $$ n/2 \times n/2 $$ submatrices of the product $$ C $$.

$$
\begin{align*}
    C_{11} &= P_5 + P_4 - P_2 + P_6 \\
           &= (A_{11} \cdot B_{11} + A_{11} \cdot B_{22} + A_{22} \cdot B_{11} + A_{22} \cdot B_{22}) \\
           &+ (A_{22} \cdot B_{21} - A_{22} \cdot B_{11}) \\
           &- (A_{11} \cdot B_{22} + A_{12} \cdot B_{22}) \\
           &+ (A_{12} \cdot B_{21} + A_{12} \cdot B_{22} - A_{22} \cdot B_{21} - A_{22} \cdot B_{22}) \\
           &= A_{11} \cdot B_{11} + A_{12} \cdot B_{21} \\ \\
    C_{12} &= P_1 + P_2 \\
           &= (A_{11} \cdot B_{12} - A_{11} \cdot B_{22}) \\
           &+ (A_{11} \cdot B_{22} + A_{12} \cdot B_{22}) \\
           &= A_{11} \cdot B_{12} + A_{12} \cdot B_{22} \\ \\
    C_{21} &= P_3 + P_4 \\
           &= (A_{21} \cdot B_{11} + A_{22} \cdot B_{11}) \\
           &+ (A_{22} \cdot B_{21} - A_{22} \cdot B_{11}) \\
           &= A_{21} \cdot B_{11} + A_{22} \cdot B_{21} \\ \\
    C_{22} &= P_5 + P_1 - P_3 - P_7 \\
           &= (A_{11} \cdot B_{11} + A_{11} \cdot B_{22} + A_{22} \cdot B_{11} + A_{22} \cdot B_{22}) \\
           &+ (A_{11} \cdot B_{12} - A_{11} \cdot B_{22}) \\
           &- (A_{21} \cdot B_{11} + A_{22} \cdot B_{11}) \\
           &- (A_{11} \cdot B_{11} + A_{11} \cdot B_{12} - A_{21} \cdot B_{11} - A_{21} \cdot B_{12}) \\
           &= A_{22} \cdot B_{22} + A_{21} \cdot B_{12}
\end{align*}
$$

These results correspond to equations \eqref{eq:8}, \eqref{eq:9}, \eqref{eq:10}, and \eqref{eq:11}.
Altogether, we add or subtract $$ n/2 \times n/2 $$ matrices eight times in step 4, and so this step takes $$ \Theta(n^2) $$ time.

Since recurrence \eqref{eq:15} has the solution $$ T(n) = \Theta(n^{\lg 7}) $$, Strassen's method is asymptotically faster than the straightforward SQUARE-MATRIX-MULTIPLY procedure.

## 4.3 The substitution method for solving recurrences

The substitution method for solving recurrences comprises two steps:
1. Guess the form of the solution.
2. Use mathematical induction to find the constants and show that the solution works.

We can use the substitution method to establish either upper or lower bounds on a recurrence.
As an example, let us determine an upper bound on the recurrence

$$
\begin{align*}
    T(n) = 2 T(\lfloor n / 2 \rfloor) + n
\end{align*}
\label{eq:16}
\tag{4.16}
$$

We guess that the solution is $$ T(n) = O(n \lg n) $$.
The substitution method requires us to prove that $$ T(n) \le cn \lg n $$ for an appropriate choice of the constant $$ c > 0 $$.
We start by assuming that this bound holds for all positive $$ m < n $$, in particular for $$ m = \lfloor n / 2 \rfloor $$, yielding $$ T(\lfloor n / 2 \rfloor) \le c \lfloor n / 2 \rfloor \lg(\lfloor n / 2 \rfloor) $$.
Substituting into the recurrence yields

$$
\begin{align*}
    T(n) &\le 2(c \lfloor n / 2 \rfloor \lg(\lfloor n / 2 \rfloor)) + n \\
         &\le cn \lg(n / 2) + n \\
         &= cn \lg n - cn \lg 2 + n \\
         &= cn \lg n - cn + n \\
         &\le cn \lg n
\end{align*}
$$

where the last step holds as long as $$ c \ge 1 $$.

Mathematical induction now requires us to show that our solution holds for the boundary conditions.
For the recurrence \eqref{eq:16}, we must show that we can choose the constant $$ c $$ large enough so that the bound $$ T(n) \le cn \lg n $$ works for the boundary condition as well.  
Let us assume, for the sake of argument, that $$ T(1) = 1 $$ is the sole boundary condition of the recurrence.
Then for $$ n = 1 $$, the bound $$ T(n) \le cn \lg n $$ yields $$ T(1) \le c 1 \lg 1 = 0 $$, which is at odds with $$ T(1) = 1 $$.  
To overcome this obstacle, we take advantage of asymptotic notation requiring us only to prove $$ T(n) \le cn \lg n $$ for $$ n \ge n_0 $$, where $$ n_0 $$ is a constant that we get to choose.
Observe that for $$ n > 3 $$, the recurrence does not depend directly on $$ T(1) $$.
Thus, we can replace $$ T(1) $$ by $$ T(2) $$ and $$ T(3) $$ as the base cases in the inductive proof, letting $$ n_0 = 2 $$.
Note that we make a distinction between the base case of the recurrence ($$ n = 1 $$) and the base cases of the inductive proof ($$ n = 2 $$ and $$ n = 3 $$).
With $$ T(1) = 1 $$, we derive from the recurrence that $$ T(2) = 4 $$ and $$ T(3) = 5 $$.
Now we can complete the inductive proof that $$ T(n) \le cn \lg n $$ for some constant $$ c \ge 1 $$ by choosing $$ c $$ large enough so that $$ T(2) \le c 2 \lg 2 $$ and $$ T(3) \le c 3 \lg 3 $$.
As it turns out, any choice of $$ c \ge 2 $$ suffices for the base cases of $$ n = 2 $$ and $$ n = 3 $$ to hold.


### Subtleties

Sometimes you might correctly guess an asymptotic bound on the solution of a recurrence, but somehow the math fails to work out in the induction.
The problem frequently turns out to be that the inductive assumption is not strong enough to prove the detailed bound.
Consider the recurrence

$$
\begin{align*}
    T(n) = T(\lfloor n / 2 \rfloor) + T(\lceil n / 2 \rceil) + 1
\end{align*}
$$

We guess that the solution is $$ T(n) = O(n) $$, and we try to show that $$ T(n) \le cn $$ for an appropriate choice of the constant $$ c $$.
Substituting our guess in the recurrence, we obtain

$$
\begin{align*}
    T(n) &\le c \lfloor n / 2 \rfloor + c \lceil n / 2 \rceil + 1 \\
         &= cn + 1
\end{align*}
$$

which does not imply $$ T(n) \le cn $$ for any choice of $$ c $$.
In order to show that it is correct, we must make a stronger inductive hypothesis.
Our new guess is $$ T(n) \le cn - d $$, where $$ d \ge 0 $$ is a constant.
We now have

$$
\begin{align*}
    T(n) &\le (c \lfloor n / 2 \rfloor - d) + (c \lceil n / 2 \rceil - d) + 1 \\
         &= cn - 2d + 1 \\
         &\le cn - d 
\end{align*}
$$

as long as $$ d \ge 1 $$.
As before, we must choose the constant $$ c $$ large enough to handle the boundary conditions.

### Avoiding pitfalls

It is easy to err in the use of asymptotic notation.
For example, in the recurrence \eqref{eq:16} we can make a mistake while proving $$ T(n) = O(n) $$ by guessing $$ T(n) \le cn $$ and then arguing

$$
\begin{align*}
    T(n) &\le 2(c \lfloor n / 2 \rfloor) + n \\
         &\le cn + n \\
         &= O(n) \quad \Longleftarrow \text{ wrong.}
\end{align*}
$$

since $$ c $$ is a constant.
The error is that we have not proved the exact form of the inductive hypothesis, that is, that $$ T(n) \le cn $$.
We therefore have to prove that $$ T(n) \le cn $$ when we want to show that $$ T(n) = O(n) $$.

### Changing variables

Sometimes, an algebraic manipulation can make an unknown recurrence similar to one you already know.
Consider the recurrence

$$
\begin{align*}
    T(n) = 2 T \left( \lfloor \sqrt{n} \rfloor \right) + \lg n
\end{align*}
$$

We can simplify this recurrence with a change of variables.
For convenience, we do not worry about rounding off values.
Renaming $$ m = \lg n $$ yields

$$
\begin{align*}
    T(2^m) = 2 T(2^{m / 2}) + m
\end{align*}
$$

We can now rename $$ S(m) = T(2^m) $$ to produce the new recurrence

$$
\begin{align*}
    S(m) = 2S(m / 2) + m
\end{align*}
$$

which is very much like recurrence \eqref{eq:16}.
Indeed, this new recurrence has the same solution: $$ S(m) = O(m \lg m) $$.
Changing back from $$ S(m) $$ to $$ T(n) $$, we obtain

$$
\begin{align*}
    T(n) = T(2^m) = S(m) = O(m \lg m) = O(\lg n \lg \lg n)
\end{align*}
$$

## 4.4 The recursion-tree method for solving recurrences

Drawing out a recursion tree serves as a straightforward way to devise a good guess.
In a recursion tree, each node represents the cost of a single subproblem somewhere in the set of recursive function invocations.
We sum the costs within each level of the tree to obtain a set of per-level costs, and then we sum all the per-level costs to determine the total cost of all levels of the recursion.

A recursion tree is best used to generate a good guess, which you can then verify by the substitution method.
For example, let us see how a recursion tree would provide a good guess for the recurrence $$ T(n) = 3 T(\lfloor n / 4 \rfloor) + \Theta(n^2) $$.
We start by focusing on finding an upper bound for the solution.
Because floors and ceilings usually do not matter when solving recurrences, we create a recursion tree for the recurrence $$ T(n) = 3 T(n / 4) + cn^2 $$, where $$ c > 0 $$.

![Desktop View](/assets/img/4-Divide-and-Conquer/Figure 4.1.png){: width="700" height="700" }
_**Figure 4.1**_

Figure 4.1 shows how we derive the recursion tree for $$ T(n) = 3 T(n / 4) + cn^2 $$.
For convenience, we assume that $$ n $$ is an exact power of 4 so that all subproblem sizes are integers.
Part (a) of the figure shows $$ T(n) $$, which we expand in part (b) into an equivalent tree representing the recurrence.
The $$ cn^2 $$ term at the root represents the cost at the top level of recursion, and the three subtrees of the root represent the costs incurred by the subproblems of size $$ n / 4 $$.
Part (c) shows this process carried one step further by expanding each node with cost $$ T(n / 4) $$ from part (b).
The cost for each of the three children of the root is $$ c(n / 4)^2 $$.
We continue expanding each node in the tree by breaking it into its constituent parts as determined by the recurrence.

Because subproblem sizes decrease by a factor of 4 each time we go down one level, we eventually must reach a boundary condition.
The subproblem size for a node at depth $$ i $$ is $$ n / 4^i $$.
Thus, the subproblem size hits $$ n = 1 $$ when $$ n / 4^i = 1 $$ or, equivalently, when $$ i = \log_{4} n $$.
Therefore, the tree has $$ \log_{4} n + 1 $$ levels.  
Next we determine the cost at each level of the tree.
Each level has three times more nodes than the level above, and so the number of the nodes at depth $$ i $$ is $$ 3^i $$.
Because subproblem sizes reduce by a factor of 4 for each level we go down from the root, each node at depth $$ i $$, for $$ i = 0, \ 1, \ 2, \dots, \ \log_{4} n - 1 $$, has a cost of $$ c(n / 4^i)^2 $$.
Multiplying, we see that the total cost over all nodes at depth $$ i $$ is $$ 3^i c(n / 4^i)^2 = (3/16)^i cn^2 $$.
The bottom level, at depth $$ \log_{4} n $$, has $$ 3^{\log_{4} n} = n^{\log_{4} 3} $$ nodes, each contributing cost $$ T(1) $$, for a total cost of $$ n^{\log_{4} 3} T(1) $$, which is $$ \Theta(n^{\log_{4} 3}) $$, since we assume that $$ T(1) $$ is a constant.  
Now we add up the costs over all levels to determine the cost for the entire tree:

$$
\begin{align*}
    T(n) &= \sum_{i = 0}^{\log_{4} n - 1} \left( \frac{3}{16} \right)^i cn^2 + \Theta \left( n^{\log_{4} 3} \right) \\
         &< \sum_{i = 0}^{\infty} \left( \frac{3}{16} \right)^i cn^2 + \Theta \left( n^{\log_{4} 3} \right) \\
         &= \frac{1}{1 - (3 / 16)} cn^2 + \Theta \left( n^{\log_{4} 3} \right) \\
         &= \frac{16}{13} cn^2 + \Theta \left( n^{\log_{4} 3} \right) \\
         &= O \left( n^2 \right)
\end{align*}
$$

Thus, we have derived a guess of $$ T(n) = O \left( n^2 \right) $$ for our original recurrence $$ T(n) = 3 T( \lfloor n / 4 \rfloor ) + \Theta \left( n^2 \right) $$. Now we can use the substitution method to verify that our guess was correct, that is, $$ T(n) = O \left( n^2 \right) $$ is an upper bound for the recurrence.
We want to show that $$ T(n) \le dn^2 $$ for some constant $$ d > 0 $$.
Using the same constant $$ c > 0 $$ as before, we have

$$
\begin{align*}
    T(n) &\le 3 T( \lfloor n / 4 \rfloor ) + cn^2 \\
         &\le 3 d \lfloor n / 4 \rfloor^2 + cn^2 \\
         &\le 3 d (n / 4)^2 + cn^2 \\
         &= \frac{3}{16} dn^2 + cn^2 \\
         &\le dn^2
\end{align*}
$$

where the last step holds as long as $$ d \ge (16/13) c $$.  
As $$ O \left( n^2 \right) $$ is an upper bound for the recurrence, it also must be a tight bound.
The first recursive call contributes a cost of $$ \Theta \left( n^2 \right) $$, and so $$ \Omega \left( n^2 \right) $$ must be a lower bound for the recurrence.