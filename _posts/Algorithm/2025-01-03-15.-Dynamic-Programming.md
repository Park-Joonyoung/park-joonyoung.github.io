---
title: 15. Dynamic Programmming
# description: Short summary of the post
date: 2024-12-15 19:06
categories: [Computer Science, Algorithm]
tags: [dynamic-programming, rod-cutting-problem, top-down-approach, bottom-up-approach, subproblem-graph]     # TAG names should always be lowercase
math: true
pin: false
---

Dynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems.
Divide-and-conquer algorithms partition the problem into disjoint subproblems, solve the subproblems recursively, and then combine their solutions to solve the original problem.
In contrast, dynamic programming applies when the subproblems overlap—that is, when subproblems share subsubproblems.
A dynamic-programming algorithm solves each subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblems.
When developing a dynamic-programming algorithm, we follow a sequence of four steps:

1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.

## 15.1 Rod cutting

In a rod-cutting problem, we buy a long steel rod, cut it into shorter rods, and sell them.
Each cut is free.
We want to know the best way to cut up the rods.
We assume that we know, for $$ i = 1, \ 2, \dots $$, the price $$ p_i $$ in dollars that we can charge for a rod of length $$ i $$ inches.
Rod lengths are always an integral number of inches.

The rod-cutting problem is the following.
Given a rod of length $$ n $$ inches and a table of prices $$ p_i $$ for $$ i = 1, \ 2, \dots, \ n $$, determine the maximum revenue $$ r_n $$ obtainable by cutting up the rod and selling the pieces.
We can cut up a rod of length $$ n $$ in $$ 2^{n - 1} $$ different ways, since we have an independent option of cutting, or not cutting, at distance $$ i $$ inches from the left end, for $$ i = 1, \ 2, \dots, \ n - 1 $$.
If an optimal solution cuts the rod into $$ k $$ pieces, for some $$ 1 \le k \le n $$, then an optimal decomposition

$$
\begin{align*}
    n = i_1 + i_2 + \cdots + i_k
\end{align*}
$$

of the rod into pieces of lengths $$ i_1, \ i_2, \dots, \ i_k $$ provides maximum corresponding revenue

$$
\begin{align*}
    r_n = p_{i_1} + p_{i_2} + \cdots + p_{i_k}
\end{align*}
$$

Generally, we can frame the values $$ r_n $$ for $$ n \ge 1 $$ in terms of optimal revenues from shorter rods:

$$
\begin{align*}
    r_n = \max{(p_n, \ r_1 + r_{n - 1}, \ r_2 + r_{n - 2}, \dots, \ r_{n - 1} + r_1)}
\end{align*}
\label{eq:1}
\tag{15.1}
$$

The first argument, $$ p_n $$, corresponds to making no cuts at all and selling the rod of length $$ n $$ as is.
The other $$ n - 1 $$ arguments to max correspond to the maximum revenue obtained by making an initial cut of the rod into two pieces of size $$ i $$ and $$ n - i $$, for each $$ i = 1, \ 2, \dots, \ n - 1 $$, and then optimally cutting up those pieces further, obtaining revenues $$ r_i $$ and $$ r_{n - i} $$ from those two pieces.
Since we don't know ahead of time which value of $$ i $$ optimizes revenue, we have to consider all possible values for $$ i $$ and pick the one that maximizes revenue.

Note that to solve the original problem of size $$ n $$, we solve smaller problems of the same type, but of smaller sizes.
Once we make the first cut, we may consider the two pieces as independent instances of the rod-cutting problem.
The overall optimal solution incorporates optimal solutions to the two related subproblems, maximizing revenue from each of those two pieces.
We say that the rod-cutting problem exhibits optimal substructure: optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently.

In a related way to arrange a recursive structure for the rod-cutting problem, we view a decomposition as consisting of a first piece of length $$ i $$ cut off the left-hand end, and then a right-hand remainder of length $$ n - i $$.
Only the remainder, and not the first piece, may be further divided.
With this approach, we can couch the solution with no cuts at all as saying that the first piece has size $$ i = n $$ and revenue $$ p_n $$ and that the remainder has size 0 with corresponding $$ r_0 = 0 $$.
We thus obtain the following simpler version of equation \eqref{eq:1}:

$$
\begin{align*}
    r_n = \max_{1 \le i \le n} (p_i + r_{n - i})
\end{align*}
\label{eq:2}
\tag{15.2}
$$

In this formulation, an optimal solution embodies the solution to only one related subproblem—the remainder—rather than two.

### Recursive top-down implementation

The following procedure implements the computation implicit in equation \eqref{eq:2} in a straightforward, top-down, recursive manner.

>CUT-ROD($$ p, \ n $$)  
>01&nbsp; if $$ n == 0 $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ 0 $$  
>03&nbsp; $$ q = -\infty $$  
>04&nbsp; for $$ i = 1 $$ to $$ n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ CUT-ROD($$ p, \ n - i $$))  
>06&nbsp; return $$ q $$

If $$ n = 0 $$, no revenue is possible, and so CUT-ROD returns $$ 0 $$ in line 2.
Line 3 initializes the maximum revenue $$ q $$ to $$ -\infty $$, so that the for loop in lines 4–5 correctly computes $$ q = \max_{i \le i \le n} (p_i \ + $$ CUT-ROD($$ p, \ n - i $$)); line 6 then returns this value.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.1.png){: width="700" }
_**Figure 15.1** The recursion tree showing recursive calls resulting from a call CUT-ROD($$ p, \ n $$) for $$ n = 4 $$._

If you were to code up CUT-ROD and run it on your computer, you would find that once the input size becomes moderately large, your program would take a long time to run.
The problem is that CUT-ROD calls itself recursively over and over again with the same parameter values; it solves the same subproblems repeatedly.
Figure 15.1 illustrates what happens for $$ n = 4 $$: CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ n - i $$) for $$ i = 1, \ 2, \dots, \ n $$.
Equivalently, CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ j $$) for each $$ j = 0, \ 1, \dots, \ n - 1 $$.
When this process unfolds recursively, the amount of work done, as a function of $$ n $$, grows explosively.

To analyze the running time of CUT-ROD, let $$ T(n) $$ denote the total number of calls made to CUT-ROD when called with its second parameter equal to $$ n $$.
This expression equals the number of nodes in a subtree whose root is labeled $$ n $$ in the recursion tree.
The count includes the initial call at its root.
Thus, $$ T(0) = 1 $$ and

$$
\begin{align*}
    T(n) = 1 + \sum_{j = 0}^{n - 1} T(j)
\end{align*}
\label{eq:3}
\tag{15.3}
$$

The initial $$ 1 $$ is for the call at the root, and the term $$ T(j) $$ counts the number of calls due to the call CUT-ROD($$ p, \ n - i $$), where $$ j = n - i $$.
The solution of this recurrence is

$$
\begin{align*}
    T(n) = 2^n
\end{align*}
\label{eq:4}
\tag{15.4}
$$

and so the running time of CUT-ROD is exponential in $$ n $$.

### Using dynamic programming for optimal rod cutting

The dynamic-programming method works as follows.
Having observed that a naive recursive solution is inefficient because it solves the same problems repeatedly, we arrange for each subproblem to be solved only once, saving its solution.
If we need to refer to this subproblem's solution again later, we just look it up, rather than recompute it.
Dynamic programming thus uses additional memory to save computation time; it serves an example of a time-memory trade-off.

There are usually two equivalent ways to implement a dynamic-programming approach.
The first approach is top-down with memoization.
In this approach, we write the procedure recursively in a natural manner, but modified to save the result of each subproblem.
The procedure now first checks to see whether it has previously solved this subproblem.
If so, it returns the saved value, saving further computation at this level; if not, the procedure computes the value in the usual manner.
The second approach is the bottom-up method.
In this approach, we sort the subproblems by size and solve them in size order, smallest first.
When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions.
We solve each subproblem only once, and when we first see it, we have already solved all of its prerequisite subproblems.  
These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems.
The bottom-up approach often has much better constant factors, since it has less overhead for procedure calls.

>MEMOIZED-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; for $$ i = 0 $$ to $$ n $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[i] = -\infty $$  
>04&nbsp; return MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)

>MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)  
>01&nbsp; if $$ r[n] \ge 0 $$  
>01&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ r[n] $$  
>03&nbsp; if $$ n == 0 $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = 0 $$  
>05&nbsp; else $$ q = -\infty $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ MEMOIZED-CUT-ROD-AUX($$ p, \ n - i, \ r $$))  
>08&nbsp; $$ r[n] = q $$  
>09&nbsp; return $$ q $$

The main procedure MEMOIZED-CUT-ROD initializes a new auxiliary array $$ r[0..n] $$ with the value $$ -\infty $$, then calls MEMOIZED-CUT-ROD-AUX.
The procedure MEMOIZED-CUT-ROD-AUX is just the memoized version of the previous procedure, CUT-ROD.
It first checks in line 1 to see whether the desired value is already known and, if it is, then line 2 returns it.
Otherwise, lines 3–7 compute the desired value $$ q $$ in the usual manner, line 8 saves it in $$ r[n] $$, and line 9 returns it.

>BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; $$ r[0] = 0 $$  
>03&nbsp; for $$ j = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = -\infty $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] + r[j - i]) $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[j] = q $$  
>08&nbsp; return $$ r[n] $$

For the bottom-up dynamic-programming approach, BOTTOM-UP-CUT-ROD uses the natural ordering of the subproblems: a subproblem of size $$ i $$ is smaller than a subproblem of size $$ j $$ if $$ i < j $$.
Thus, the procedure solves subproblems of sizes $$ j = 0, \ 1, \dots, \ n $$, in that order.
Line 1 of the procedure creates a new array $$ r[0..n] $$ in which to save the results of the subproblems, and line 2 initializes $$ r[0] $$ to $$ 0 $$, since a rod of length 0 earns no revenue.
Lines 3–6 solve each subproblem of size $$ j $$, for $$ j = 1, \ 2, \dots, \ n $$, in order of increasing size.
Line 7 saves in $$ r[j] $$ the solution to the subproblem of size $$ j $$.
Finally, line 8 returns $$ r[n] $$, which equals the optimal value $$ r_n $$.

The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure BOTTOM-UP-CUT-ROD is $$ \Theta(n^2) $$, due to its doubly-nested loop structure.
The running time of its top-down counterpart, MEMOIZED-CUT-ROD, is also $$ \Theta(n^2) $$.
Because a recursive call to solve a previously solved subproblem returns immediately, MEMOIZED-CUT-ROD solves each subproblem just once.
It solves subproblems for sizes $$ 0, \ 1, \dots, \ n $$.
To solve a subproblem of size $$ n $$, the for loop of lines 6–7 iterates $$ n $$ times.
Thus, the total number of iterations of this for loop forms an arithmetic series, giving a total of $$ \Theta(n^2) $$ iterations. (We actually are using a form of aggregate analysis here.)

### Subproblem graphs

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.2.png){: width="300" }
_**Figure 15.2** The subproblem graph for the rod-cutting problem with $$ n = 4 $$. A directed edge ($$ x, \ y $$) indicates that we need a solution to subproblem $$ y $$ when solving subproblem $$ x $$._

When we think about a dynamic-programming problem, we should understand the set of subproblems involved and how subproblems depend on one another.
The subproblem graph for the problem embodies exactly this information.
The subproblem graph has a directed edge from the vertex for subproblem $$ x $$ to the vertex for subproblem $$ y $$ if determining an optimal soltuion for subproblem $$ x $$ involves directly considering an optimal solution for subproblem $$ y $$.
We can think of the subproblem graph as a reduced or collapsed version of the recursion tree for the top-down recursive method, in which we coalesce all nodes for the same subproblem into a single vertex and direct all edges from parent to child.
The bottom-up method for dynamic programming considers the vertices of the subproblem graph in such an order that we solve the subproblems $$ y $$ adjacent to a given subproblem $$ x $$ before we solve subproblem $$ x $$.
In other words, no subproblem is considered until all of the subproblems it depends upon have been solved.

The size of the subproblem graph $$ G = (V, \ E) $$ can help us determine the running time of the dynamic programming algorithm.
Since we solve each subproblem just once, the running time is the sum of the times needed to solve each subproblem.
Typically, the time to compute the solution to a subproblem is proportional to the degree (number of outgoing edges) of the corresponding vertex in the subproblem graph, and the number of subproblems is equal to the number of vertices in the subproblem graph.
In this common case, the running time of dynamic programming is linear the number of vertices and edges.

### Reconstructing a solution

>EXTENDED-BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ and $$ s[0..n] $$ be new arrays  
>02&nbsp; $$ r[0] = 0 $$  
>03&nbsp; for $$ j = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = -\infty $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ q < p[i] + r[j - i] $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = p[i] + r[j - i] $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ s[j] = i $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[j] = q $$  
>10&nbsp; return $$ r $$ and $$ s $$

This procedure is similar to BOTTOM-UP-CUT-ROD, except that it creates the array $$ s $$ in line 1, and it updates $$ s[j] $$ in line 8 to hold the optimal size $$ i $$ of the first piece to cut off when solving a subproblem of size $$ j $$.

>PRINT-CUT-ROD-SOLUTION($$ p, \ n $$)  
>01&nbsp; $$ (r, \ s) = $$ EXTENDED-BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>02&nbsp; while $$ n > 0 $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print $$ s[n] $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ n = n - s[n] $$

This procedure calls EXTENDED-BOTTOM-UP-CUT-ROD to compute the array $$ s[1..n] $$ of optimal first-piece sizes and then prints out the complete list of piece sizes in an optimal decomposition of a rod of length $$ n $$.

## 15.2 Matrix-chain multiplication

We are given a sequence $$ \langle A_1, \ A_2, \dots, \ A_n \rangle $$ of $$ n $$ matrices to be multiplied, and we wish to compute the product

$$
\begin{align*}
    A_1 A_2 \cdots A_n
\end{align*}
\label{eq:5}
\tag{15.5}
$$

We can evaluate the expression \eqref{eq:5} using the standard algorithm for multiplying pairs of matrices as a subroutine once we have parenthesized it to resolve all ambiguities in how the matrices are multiplied together.
Matrix multiplication is associative, and so all parenthesization yield the same product.
How we parenthesize a chain of matrices can have a dramatic impact on the cost of evaluating the product.  
Consider first the cost of multiplying two matrices.

>MATRIX-MULTIPLY($$ A, \ B $$)  
>01&nbsp; if $$ A.columns \neq B.rows $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;error "incompatible dimensions"  
>03&nbsp; else let $$ C $$ be a new $$ A.rows \times B.columns $$ matrix  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ A.rows $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ j = 1 $$ to $$ B.columns $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = 0 $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ k = 1 $$ to $$ A.columns $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = c_{ij} + a_{ik} \cdot b_{kj} $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ C $$

If $$ A $$ is a $$ p \times q $$ matrix and $$ B $$ is a $$ q \times r $$ matrix, the resulting matrix $$ C $$ is a $$ p \times r $$ matrix.
The time to compute $$ C $$ is dominated by the number of scalar multiplications in line 8, which is $$ pqr $$.

We state the matrix-chain multiplication problem as follows: given a chain $$ \langle A_1, \ A_2, \dots, \ A_n \rangle $$ of $$ n $$ matrices, where for $$ i = 1, \ 2, \dots, \ n $$, matrix $$ A_i $$ has dimension $$ p_{i - 1} \times p_i $$, fully parenthesize the product $$ A_1 A_2 \cdots A_n $$ in a way that minimizes the number of scalar multiplications.

### Counting the number of parenthesizations

Before solving the matrix-chain multiplication problem by dynamic programming, let's find out why exhaustively checking all possible parenthesizations does not yield an efficient algorithm.
Denote the number of alternative parenthesizations of a sequence of $$ n $$ matrices by $$ P(n) $$.
When $$ n = 1 $$, we have just one matrix and therefore only one way to fully parenthesize the matrix product.
When $$ n \ge 2 $$, a fully parenthesized matrix product is the product of two fully parenthesized matrix subproducts, and the split between the two subproducts may occur between the $$ k $$th and $$ (k + 1) $$st matrices for any $$ k = 1, \ 2, \dots, \ n - 1 $$.
Thus, we obtain the recurrence

$$
\begin{align*}
    P(n) =
    \begin{cases}
        1 & \text{if } n = 1 \\
        \displaystyle \sum_{k = 1}^{n - 1} P(k) P(n - k) & \text{if } n \ge 2
    \end{cases}
\end{align*}
\label{eq:6}
\tag{15.6}
$$

The solution to this recurrence is the sequence of Catalan numbers, which grows as $$ \Omega(4^n / n^{3/2}) $$.
The number of solutions is thus exponential in $$ n $$, and the brute-force method of exhaustive search makes for a poor strategy when determining how to optimally parenthesize a matrix chain.

### Applying dynamic programming

### Step 1: The structure of an optimal parenthesization

For our first step in the dynamic-programming paradigm, we find the optimal substructure and then use it to construct an optimal solution to the problem from optimal solutions to subproblems.
In the matrix-chain multiplication problem, we can perform this step as follows.
Observe that if the problem is nontrivial, i.e., $$ i < j $$, then to parenthesize the product $$ A_i A_{i + 1} \cdots A_j $$, we must split the product between $$ A_k $$ and $$ A_{k + 1} $$ for some integer $$ k $$ in the range $$ i \le k < j $$.
That is, for some value of $$ k $$, we first compute the matrices $$ A_{i..k} $$ and $$ A_{k + 1..j} $$ and then multiply them together to produce the final product $$ A_{i..j} $$.
The cost of parenthesizing this way is the cost of computing the matrix $$ A_{i..k} $$, plus the cost of computing $$ A_{k + 1..j} $$, plust the cost of multiplying them together.

The optimal substructure of this problem is as follows.
Suppose that to optimally parenthesize $$ A_i A_{i + 1} \cdots A_j $$, we split the product between $$ A_k $$ and $$ A_{k + 1} $$.
Then the way we parenthesize the prefix subchain $$ A_i A_{i + 1} \cdots A_k $$ within this optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$ must be an optimal parenthesization of $$ A_i A_{i + 1} \cdots A_k $$.
Because if there were a less costly way to parenthesize $$ A_i A_{i + 1} \cdots A_k $$, then we could substitute that parenthesization in the optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$: a contradiction.
A similar observation holds for how we parenthesize the subchain $$ A_{k + 1} A_{k + 2} \cdots A_j $$ in the optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$.

Now we use our optimal substructure to show that we can construct an optimal solution to the problem from optimal solutions to subproblems.
We can build an optimal solution to an instance of the matrix-chain multiplication problem by splitting the problem into two subproblems (optimally parenthesizing $$ A_i A_{i + 1} \cdots A_k $$ and $$ A_{k + 1} A_{k + 2} \cdots A_j $$), finding optimal solutions to subproblem instances, and then combining these optimal subproblem solutions.

### Step 2: A recursive solution

Next, we define the cost of an optimal solution recursively in terms of the optimal solutions to subproblems.
Let $$ m[i, \ j] $$ be the minimum number of scalar multiplications needed to compute the matrix $$ A_{i..j} $$; for the full problem, the lowest-cost way to compute $$ A_{1..n} $$ would thus be $$ m[1, \ n] $$.  
We can define $$ m[i, \ j] $$ recursively as follows.
If $$ i = j $$, the problem is trivial; the chain consists of just one matrix $$ A_{i..i} = A_i $$, so that no scalar multiplications are necessary to compute the product.
Thus, $$ m[i, \ i] = 0 $$ for $$ i = 1, \ 2, \dots, \ n $$.
To compute $$ m[i, \ j] $$ when $$ i < j $$, we take advantage of the structure of an optimal solution from step 1.
Let us assume that to optimally parenthesize, we split the product $$ A_i A_{i + 1} \cdots A_j $$ between $$ A_k $$ and $$ A_{k + 1} $$, where $$ i \le k < j $$.
Then, $$ m[i, \ j] $$ equals the minimum cost for computing the subproducts $$ A_{i..k} $$ and $$ A_{k + 1..j} $$, plus the cost of multiplying these two matrices together.
Recalling that each matrix $$ A_i $$ is $$ p_{i - 1} \times p_{i} $$, we see that computing the matrix product $$ A_{i..k} A_{k + 1..j} $$ takes $$ p_{i - 1} p_k p_j $$ scalar multiplications.
Thus, we obtain

$$
\begin{align*}
    m[i, \ j] = m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j
\end{align*}
$$

There are $$ j - 1 $$ possible values of $$ k $$, namely $$ k = i, \ i + 1, \dots, \ j - 1 $$.
Thus, our recursive definition for the minimum cost of parenthesizing the product $$ A_i A_{i + 1} \cdots A_j $$ becomes

$$
\begin{align*}
    m[i, \ j] =
    \begin{cases}
        0 & \text{if } i = j \\
        \displaystyle \min_{i \le k < j} \{ m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j \} & \text{if } i < j
    \end{cases}
\end{align*}
\label{eq:7}
\tag{15.7}
$$

### Step 3: Computing the optimal costs

Observe that we have relatively few distinct subproblems: one subproblem for each choice of $$ i $$ and $$ j $$ satisfying $$ i \le j \le n $$, or $$ \binom{n}{2} + n = \Theta(n^2) $$ in all.
A recursive algorithm may encounter each subproblem many times in different branches of its recursion tree.  
Instead of computing the solution to recurrence \eqref{eq:7} recursively, we compute the optimal cost by using a tabular, bottom-up approach.
In order to implement the bottom-up approach, we must determine which entries of the table we refer to when computing $$ m[i, \ j] $$.
Equation \eqref{eq:7} shows that the cost $$ m[i, \ j] $$ of computing a matrix-chain product of $$ j - i + 1 $$ matrices depends only on the costs of computing matrix-chain products of fewer than $$ j - i + 1 $$ matrices.
That is, for $$ k = i, \ i + 1, \dots, \ j - 1 $$, the matrix $$ A_{i..k} $$ is a product of $$ k - i + 1 < j - i + 1 $$ matrices and the matrix $$ A_{k + 1..j} $$ is a product of $$ j - k < j - i + 1 $$ matrices.
Thus, the algorithm should fill in the table $$ m $$ in a manner that corresponds to solving the parenthesization problem on matrix chains of increasing length.

>MATRIX-CHAIN-ORDER($$ p $$)  
>01&nbsp; $$ n = p.length - 1 $$  
>02&nbsp; let $$ m[1..n, \ 1..n] $$ and $$ s[1..n - 1, \ 2..n] $$ be new tables  
>03&nbsp; for $$ i = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ i] = 0 $$  
>05&nbsp; for $$ l = 2 $$ to $$ n $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n - l + 1 $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ j = i + l - 1 $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = \infty $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ k = i $$ to $$ j - 1 $$  
>10&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j $$  
>11&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ q < m[i, \ j] $$  
>12&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = q $$  
>13&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ s[i, \ j] = k $$  
>14&nbsp; return $$ m $$ and $$ s $$

The algorithm first computes $$ m[i, \ i] = 0 $$ for $$ i = 1, \ 2, \dots, \ n $$ in lines 3–4.
It then uses recurrence \eqref{eq:7} to compute $$ m[i, \ i + 1] $$ for $$ i = 1, \ 2, \dots, \ n - 1 $$ (the minimum costs for chains of length $$ l = 2 $$) during the first execution of the for loop in lines 5–13.
The second time through the loop, it computes $$ m[i, \ i + 2] $$ for $$ i = 1, \ 2, \dots, \ n - 2 $$ (the minimum costs for chains of length $$ l = 3 $$), and so forth.
At each step, the $$ m[i, \ j] $$ cost computed in lines 10–13 depends only on table entries $$ m[i, \ k] $$ and $$ m[k + 1, \ j] $$ already computed.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.3.png){: width="700" }
_**Figure 15.3** The $$ m $$ and $$ s $$ tables computed by MATRIX-CHAIN-ORDER for $$ n = 6 $$ and the following matrix dimensions._

Figure 15.3 illustrates this procedure on a chain of $$ n = 6 $$ matrices.
Since we have defined $$ m[i, \ j] $$ only for $$ i \le j $$, only the portion of the table $$ m $$ strictly above the main diagonal is used.
Using this layout, we can find the minimum cost $$ m[i, \ j] $$ for multiplying a subchain $$ A_i A_{i + 1} \cdots A_j $$ of matrices at the intersection of lines running northeast from $$ A_i $$ and northwest from $$ A_j $$.
MATRIX-CHAIN-ORDER computes each entry $$ m[i, \ j] $$ using the products $$ p_{i - 1} p_k p_j $$ for $$ k = i, \ i + 1, \dots, \ j - 1 $$, and all entries southwest and southeast from $$ m[i, \ j] $$.

A simple inspection of the nested loop structure of MATRIX-CHAIN-ORDER yields a running time of $$ O(n^3) $$ for the algorithm.
The loops are nested three deep, and each loop index takes on at most $$ n - 1 $$ values.
The algorithm requires $$ \Theta(n^2) $$ space to store the $$ m $$ and $$ s $$ tables.
Thus, MATRIX-CHAIN-ORDER $$ is much more efficient than the exponential-time method of enumerating all possible parenthesizations and checking each one.

### Step 4: Constructing an optimal solution

The table $$ s[i..n - 1, \ 2..n] $$ gives us the information we need to show how to multiply the matrices.
Each entry $$ s[i, \ j] $$ records a value of $$ k $$ such that an optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$ splits the product between $$ A_k $$ and $$ A_{k + 1} $$.
Thus, we know that the final matrix multiplication in computing $$ A_{1..n} $$ optimally is $$ A_{1..s[1, \ n]} A_{s[1, \ n] + 1..n} $$.
