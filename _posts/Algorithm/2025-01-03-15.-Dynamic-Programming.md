---
title: 15. Dynamic Programmming
# description: Short summary of the post
date: 2024-12-15 19:06
categories: [Computer Science, Algorithm]
tags: [dynamic-programming, rod-cutting-problem, subproblem-graph, matrix-chain-multiplication]     # TAG names should always be lowercase
math: true
pin: false
---

Dynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems.
Divide-and-conquer algorithms partition the problem into disjoint subproblems, solve the subproblems recursively, and then combine their solutions to solve the original problem.
In contrast, dynamic programming applies when the subproblems overlap—that is, when subproblems share subsubproblems.
A dynamic-programming algorithm solves each subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblems.
When developing a dynamic-programming algorithm, we follow a sequence of four steps:

1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.

## 15.1 Rod cutting

In a rod-cutting problem, we buy a long steel rod, cut it into shorter rods, and sell them.
Each cut is free.
We want to know the best way to cut up the rods.
We assume that we know, for $$ i = 1, \ 2, \dots $$, the price $$ p_i $$ in dollars that we can charge for a rod of length $$ i $$ inches.
Rod lengths are always an integral number of inches.

The rod-cutting problem is the following.
Given a rod of length $$ n $$ inches and a table of prices $$ p_i $$ for $$ i = 1, \ 2, \dots, \ n $$, determine the maximum revenue $$ r_n $$ obtainable by cutting up the rod and selling the pieces.
We can cut up a rod of length $$ n $$ in $$ 2^{n - 1} $$ different ways, since we have an independent option of cutting, or not cutting, at distance $$ i $$ inches from the left end, for $$ i = 1, \ 2, \dots, \ n - 1 $$.
If an optimal solution cuts the rod into $$ k $$ pieces, for some $$ 1 \le k \le n $$, then an optimal decomposition

$$
\begin{align*}
    n = i_1 + i_2 + \cdots + i_k
\end{align*}
$$

of the rod into pieces of lengths $$ i_1, \ i_2, \dots, \ i_k $$ provides maximum corresponding revenue

$$
\begin{align*}
    r_n = p_{i_1} + p_{i_2} + \cdots + p_{i_k}
\end{align*}
$$

Generally, we can frame the values $$ r_n $$ for $$ n \ge 1 $$ in terms of optimal revenues from shorter rods:

$$
\begin{align*}
    r_n = \max{(p_n, \ r_1 + r_{n - 1}, \ r_2 + r_{n - 2}, \dots, \ r_{n - 1} + r_1)}
\end{align*}
\label{eq:1}
\tag{15.1}
$$

The first argument, $$ p_n $$, corresponds to making no cuts at all and selling the rod of length $$ n $$ as is.
The other $$ n - 1 $$ arguments to max correspond to the maximum revenue obtained by making an initial cut of the rod into two pieces of size $$ i $$ and $$ n - i $$, for each $$ i = 1, \ 2, \dots, \ n - 1 $$, and then optimally cutting up those pieces further, obtaining revenues $$ r_i $$ and $$ r_{n - i} $$ from those two pieces.
Since we don't know ahead of time which value of $$ i $$ optimizes revenue, we have to consider all possible values for $$ i $$ and pick the one that maximizes revenue.

Note that to solve the original problem of size $$ n $$, we solve smaller problems of the same type, but of smaller sizes.
Once we make the first cut, we may consider the two pieces as independent instances of the rod-cutting problem.
The overall optimal solution incorporates optimal solutions to the two related subproblems, maximizing revenue from each of those two pieces.
We say that the rod-cutting problem exhibits optimal substructure: optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently.

In a related way to arrange a recursive structure for the rod-cutting problem, we view a decomposition as consisting of a first piece of length $$ i $$ cut off the left-hand end, and then a right-hand remainder of length $$ n - i $$.
Only the remainder, and not the first piece, may be further divided.
With this approach, we can couch the solution with no cuts at all as saying that the first piece has size $$ i = n $$ and revenue $$ p_n $$ and that the remainder has size 0 with corresponding $$ r_0 = 0 $$.
We thus obtain the following simpler version of equation \eqref{eq:1}:

$$
\begin{align*}
    r_n = \max_{1 \le i \le n} (p_i + r_{n - i})
\end{align*}
\label{eq:2}
\tag{15.2}
$$

In this formulation, an optimal solution embodies the solution to only one related subproblem—the remainder—rather than two.

### Recursive top-down implementation

The following procedure implements the computation implicit in equation \eqref{eq:2} in a straightforward, top-down, recursive manner.

>CUT-ROD($$ p, \ n $$)  
>01&nbsp; if $$ n == 0 $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ 0 $$  
>03&nbsp; $$ q = -\infty $$  
>04&nbsp; for $$ i = 1 $$ to $$ n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ CUT-ROD($$ p, \ n - i $$))  
>06&nbsp; return $$ q $$

If $$ n = 0 $$, no revenue is possible, and so CUT-ROD returns $$ 0 $$ in line 2.
Line 3 initializes the maximum revenue $$ q $$ to $$ -\infty $$, so that the for loop in lines 4–5 correctly computes $$ q = \max_{i \le i \le n} (p_i \ + $$ CUT-ROD($$ p, \ n - i $$)); line 6 then returns this value.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.1.png){: width="700" }
_**Figure 15.1** The recursion tree showing recursive calls resulting from a call CUT-ROD($$ p, \ n $$) for $$ n = 4 $$._

If you were to code up CUT-ROD and run it on your computer, you would find that once the input size becomes moderately large, your program would take a long time to run.
The problem is that CUT-ROD calls itself recursively over and over again with the same parameter values; it solves the same subproblems repeatedly.
Figure 15.1 illustrates what happens for $$ n = 4 $$: CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ n - i $$) for $$ i = 1, \ 2, \dots, \ n $$.
Equivalently, CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ j $$) for each $$ j = 0, \ 1, \dots, \ n - 1 $$.
When this process unfolds recursively, the amount of work done, as a function of $$ n $$, grows explosively.

To analyze the running time of CUT-ROD, let $$ T(n) $$ denote the total number of calls made to CUT-ROD when called with its second parameter equal to $$ n $$.
This expression equals the number of nodes in a subtree whose root is labeled $$ n $$ in the recursion tree.
The count includes the initial call at its root.
Thus, $$ T(0) = 1 $$ and

$$
\begin{align*}
    T(n) = 1 + \sum_{j = 0}^{n - 1} T(j)
\end{align*}
\label{eq:3}
\tag{15.3}
$$

The initial $$ 1 $$ is for the call at the root, and the term $$ T(j) $$ counts the number of calls due to the call CUT-ROD($$ p, \ n - i $$), where $$ j = n - i $$.
The solution of this recurrence is

$$
\begin{align*}
    T(n) = 2^n
\end{align*}
\label{eq:4}
\tag{15.4}
$$

and so the running time of CUT-ROD is exponential in $$ n $$.

### Using dynamic programming for optimal rod cutting

The dynamic-programming method works as follows.
Having observed that a naive recursive solution is inefficient because it solves the same problems repeatedly, we arrange for each subproblem to be solved only once, saving its solution.
If we need to refer to this subproblem's solution again later, we just look it up, rather than recompute it.
Dynamic programming thus uses additional memory to save computation time; it serves an example of a time-memory trade-off.

There are usually two equivalent ways to implement a dynamic-programming approach.
The first approach is top-down with memoization.
In this approach, we write the procedure recursively in a natural manner, but modified to save the result of each subproblem.
The procedure now first checks to see whether it has previously solved this subproblem.
If so, it returns the saved value, saving further computation at this level; if not, the procedure computes the value in the usual manner.
The second approach is the bottom-up method.
In this approach, we sort the subproblems by size and solve them in size order, smallest first.
When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions.
We solve each subproblem only once, and when we first see it, we have already solved all of its prerequisite subproblems.  
These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems.
The bottom-up approach often has much better constant factors, since it has less overhead for procedure calls.

>MEMOIZED-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; for $$ i = 0 $$ to $$ n $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[i] = -\infty $$  
>04&nbsp; return MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)

>MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)  
>01&nbsp; if $$ r[n] \ge 0 $$  
>01&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ r[n] $$  
>03&nbsp; if $$ n == 0 $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = 0 $$  
>05&nbsp; else $$ q = -\infty $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ MEMOIZED-CUT-ROD-AUX($$ p, \ n - i, \ r $$))  
>08&nbsp; $$ r[n] = q $$  
>09&nbsp; return $$ q $$

The main procedure MEMOIZED-CUT-ROD initializes a new auxiliary array $$ r[0..n] $$ with the value $$ -\infty $$, then calls MEMOIZED-CUT-ROD-AUX.
The procedure MEMOIZED-CUT-ROD-AUX is just the memoized version of the previous procedure, CUT-ROD.
It first checks in line 1 to see whether the desired value is already known and, if it is, then line 2 returns it.
Otherwise, lines 3–7 compute the desired value $$ q $$ in the usual manner, line 8 saves it in $$ r[n] $$, and line 9 returns it.

>BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; $$ r[0] = 0 $$  
>03&nbsp; for $$ j = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = -\infty $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] + r[j - i]) $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[j] = q $$  
>08&nbsp; return $$ r[n] $$

For the bottom-up dynamic-programming approach, BOTTOM-UP-CUT-ROD uses the natural ordering of the subproblems: a subproblem of size $$ i $$ is smaller than a subproblem of size $$ j $$ if $$ i < j $$.
Thus, the procedure solves subproblems of sizes $$ j = 0, \ 1, \dots, \ n $$, in that order.
Line 1 of the procedure creates a new array $$ r[0..n] $$ in which to save the results of the subproblems, and line 2 initializes $$ r[0] $$ to $$ 0 $$, since a rod of length 0 earns no revenue.
Lines 3–6 solve each subproblem of size $$ j $$, for $$ j = 1, \ 2, \dots, \ n $$, in order of increasing size.
Line 7 saves in $$ r[j] $$ the solution to the subproblem of size $$ j $$.
Finally, line 8 returns $$ r[n] $$, which equals the optimal value $$ r_n $$.

The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure BOTTOM-UP-CUT-ROD is $$ \Theta(n^2) $$, due to its doubly-nested loop structure.
The running time of its top-down counterpart, MEMOIZED-CUT-ROD, is also $$ \Theta(n^2) $$.
Because a recursive call to solve a previously solved subproblem returns immediately, MEMOIZED-CUT-ROD solves each subproblem just once.
It solves subproblems for sizes $$ 0, \ 1, \dots, \ n $$.
To solve a subproblem of size $$ n $$, the for loop of lines 6–7 iterates $$ n $$ times.
Thus, the total number of iterations of this for loop forms an arithmetic series, giving a total of $$ \Theta(n^2) $$ iterations. (We actually are using a form of aggregate analysis here.)

### Subproblem graphs

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.2.png){: width="300" }
_**Figure 15.2** The subproblem graph for the rod-cutting problem with $$ n = 4 $$. A directed edge ($$ x, \ y $$) indicates that we need a solution to subproblem $$ y $$ when solving subproblem $$ x $$._

When we think about a dynamic-programming problem, we should understand the set of subproblems involved and how subproblems depend on one another.
The subproblem graph for the problem embodies exactly this information.
The subproblem graph has a directed edge from the vertex for subproblem $$ x $$ to the vertex for subproblem $$ y $$ if determining an optimal soltuion for subproblem $$ x $$ involves directly considering an optimal solution for subproblem $$ y $$.
We can think of the subproblem graph as a reduced or collapsed version of the recursion tree for the top-down recursive method, in which we coalesce all nodes for the same subproblem into a single vertex and direct all edges from parent to child.
The bottom-up method for dynamic programming considers the vertices of the subproblem graph in such an order that we solve the subproblems $$ y $$ adjacent to a given subproblem $$ x $$ before we solve subproblem $$ x $$.
In other words, no subproblem is considered until all of the subproblems it depends upon have been solved.

The size of the subproblem graph $$ G = (V, \ E) $$ can help us determine the running time of the dynamic programming algorithm.
Since we solve each subproblem just once, the running time is the sum of the times needed to solve each subproblem.
Typically, the time to compute the solution to a subproblem is proportional to the degree (number of outgoing edges) of the corresponding vertex in the subproblem graph, and the number of subproblems is equal to the number of vertices in the subproblem graph.
In this common case, the running time of dynamic programming is linear the number of vertices and edges.

### Reconstructing a solution

>EXTENDED-BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ and $$ s[0..n] $$ be new arrays  
>02&nbsp; $$ r[0] = 0 $$  
>03&nbsp; for $$ j = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = -\infty $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ q < p[i] + r[j - i] $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = p[i] + r[j - i] $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ s[j] = i $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[j] = q $$  
>10&nbsp; return $$ r $$ and $$ s $$

This procedure is similar to BOTTOM-UP-CUT-ROD, except that it creates the array $$ s $$ in line 1, and it updates $$ s[j] $$ in line 8 to hold the optimal size $$ i $$ of the first piece to cut off when solving a subproblem of size $$ j $$.

>PRINT-CUT-ROD-SOLUTION($$ p, \ n $$)  
>01&nbsp; $$ (r, \ s) = $$ EXTENDED-BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>02&nbsp; while $$ n > 0 $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print $$ s[n] $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ n = n - s[n] $$

This procedure calls EXTENDED-BOTTOM-UP-CUT-ROD to compute the array $$ s[1..n] $$ of optimal first-piece sizes and then prints out the complete list of piece sizes in an optimal decomposition of a rod of length $$ n $$.

## 15.2 Matrix-chain multiplication

We are given a sequence $$ \langle A_1, \ A_2, \dots, \ A_n \rangle $$ of $$ n $$ matrices to be multiplied, and we wish to compute the product

$$
\begin{align*}
    A_1 A_2 \cdots A_n
\end{align*}
\label{eq:5}
\tag{15.5}
$$

We can evaluate the expression \eqref{eq:5} using the standard algorithm for multiplying pairs of matrices as a subroutine once we have parenthesized it to resolve all ambiguities in how the matrices are multiplied together.
Matrix multiplication is associative, and so all parenthesization yield the same product.
How we parenthesize a chain of matrices can have a dramatic impact on the cost of evaluating the product.  
Consider first the cost of multiplying two matrices.

>MATRIX-MULTIPLY($$ A, \ B $$)  
>01&nbsp; if $$ A.columns \neq B.rows $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;error "incompatible dimensions"  
>03&nbsp; else let $$ C $$ be a new $$ A.rows \times B.columns $$ matrix  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ A.rows $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ j = 1 $$ to $$ B.columns $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = 0 $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ k = 1 $$ to $$ A.columns $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = c_{ij} + a_{ik} \cdot b_{kj} $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ C $$

If $$ A $$ is a $$ p \times q $$ matrix and $$ B $$ is a $$ q \times r $$ matrix, the resulting matrix $$ C $$ is a $$ p \times r $$ matrix.
The time to compute $$ C $$ is dominated by the number of scalar multiplications in line 8, which is $$ pqr $$.

We state the matrix-chain multiplication problem as follows: given a chain $$ \langle A_1, \ A_2, \dots, \ A_n \rangle $$ of $$ n $$ matrices, where for $$ i = 1, \ 2, \dots, \ n $$, matrix $$ A_i $$ has dimension $$ p_{i - 1} \times p_i $$, fully parenthesize the product $$ A_1 A_2 \cdots A_n $$ in a way that minimizes the number of scalar multiplications.

### Counting the number of parenthesizations

Before solving the matrix-chain multiplication problem by dynamic programming, let's find out why exhaustively checking all possible parenthesizations does not yield an efficient algorithm.
Denote the number of alternative parenthesizations of a sequence of $$ n $$ matrices by $$ P(n) $$.
When $$ n = 1 $$, we have just one matrix and therefore only one way to fully parenthesize the matrix product.
When $$ n \ge 2 $$, a fully parenthesized matrix product is the product of two fully parenthesized matrix subproducts, and the split between the two subproducts may occur between the $$ k $$th and $$ (k + 1) $$st matrices for any $$ k = 1, \ 2, \dots, \ n - 1 $$.
Thus, we obtain the recurrence

$$
\begin{align*}
    P(n) =
    \begin{cases}
        1 & \text{if } n = 1 \\
        \displaystyle \sum_{k = 1}^{n - 1} P(k) P(n - k) & \text{if } n \ge 2
    \end{cases}
\end{align*}
\label{eq:6}
\tag{15.6}
$$

The solution to this recurrence is the sequence of Catalan numbers, which grows as $$ \Omega(4^n / n^{3/2}) $$.
The number of solutions is thus exponential in $$ n $$, and the brute-force method of exhaustive search makes for a poor strategy when determining how to optimally parenthesize a matrix chain.

### Applying dynamic programming

### Step 1: The structure of an optimal parenthesization

For our first step in the dynamic-programming paradigm, we find the optimal substructure and then use it to construct an optimal solution to the problem from optimal solutions to subproblems.
In the matrix-chain multiplication problem, we can perform this step as follows.
Observe that if the problem is nontrivial, i.e., $$ i < j $$, then to parenthesize the product $$ A_i A_{i + 1} \cdots A_j $$, we must split the product between $$ A_k $$ and $$ A_{k + 1} $$ for some integer $$ k $$ in the range $$ i \le k < j $$.
That is, for some value of $$ k $$, we first compute the matrices $$ A_{i..k} $$ and $$ A_{k + 1..j} $$ and then multiply them together to produce the final product $$ A_{i..j} $$.
The cost of parenthesizing this way is the cost of computing the matrix $$ A_{i..k} $$, plus the cost of computing $$ A_{k + 1..j} $$, plust the cost of multiplying them together.

The optimal substructure of this problem is as follows.
Suppose that to optimally parenthesize $$ A_i A_{i + 1} \cdots A_j $$, we split the product between $$ A_k $$ and $$ A_{k + 1} $$.
Then the way we parenthesize the prefix subchain $$ A_i A_{i + 1} \cdots A_k $$ within this optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$ must be an optimal parenthesization of $$ A_i A_{i + 1} \cdots A_k $$.
Because if there were a less costly way to parenthesize $$ A_i A_{i + 1} \cdots A_k $$, then we could substitute that parenthesization in the optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$: a contradiction.
A similar observation holds for how we parenthesize the subchain $$ A_{k + 1} A_{k + 2} \cdots A_j $$ in the optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$.

Now we use our optimal substructure to show that we can construct an optimal solution to the problem from optimal solutions to subproblems.
We can build an optimal solution to an instance of the matrix-chain multiplication problem by splitting the problem into two subproblems (optimally parenthesizing $$ A_i A_{i + 1} \cdots A_k $$ and $$ A_{k + 1} A_{k + 2} \cdots A_j $$), finding optimal solutions to subproblem instances, and then combining these optimal subproblem solutions.

### Step 2: A recursive solution

Next, we define the cost of an optimal solution recursively in terms of the optimal solutions to subproblems.
Let $$ m[i, \ j] $$ be the minimum number of scalar multiplications needed to compute the matrix $$ A_{i..j} $$; for the full problem, the lowest-cost way to compute $$ A_{1..n} $$ would thus be $$ m[1, \ n] $$.  
We can define $$ m[i, \ j] $$ recursively as follows.
If $$ i = j $$, the problem is trivial; the chain consists of just one matrix $$ A_{i..i} = A_i $$, so that no scalar multiplications are necessary to compute the product.
Thus, $$ m[i, \ i] = 0 $$ for $$ i = 1, \ 2, \dots, \ n $$.
To compute $$ m[i, \ j] $$ when $$ i < j $$, we take advantage of the structure of an optimal solution from step 1.
Let us assume that to optimally parenthesize, we split the product $$ A_i A_{i + 1} \cdots A_j $$ between $$ A_k $$ and $$ A_{k + 1} $$, where $$ i \le k < j $$.
Then, $$ m[i, \ j] $$ equals the minimum cost for computing the subproducts $$ A_{i..k} $$ and $$ A_{k + 1..j} $$, plus the cost of multiplying these two matrices together.
Recalling that each matrix $$ A_i $$ is $$ p_{i - 1} \times p_{i} $$, we see that computing the matrix product $$ A_{i..k} A_{k + 1..j} $$ takes $$ p_{i - 1} p_k p_j $$ scalar multiplications.
Thus, we obtain

$$
\begin{align*}
    m[i, \ j] = m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j
\end{align*}
$$

There are $$ j - 1 $$ possible values of $$ k $$, namely $$ k = i, \ i + 1, \dots, \ j - 1 $$.
Thus, our recursive definition for the minimum cost of parenthesizing the product $$ A_i A_{i + 1} \cdots A_j $$ becomes

$$
\begin{align*}
    m[i, \ j] =
    \begin{cases}
        0 & \text{if } i = j \\
        \displaystyle \min_{i \le k < j} \{ m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j \} & \text{if } i < j
    \end{cases}
\end{align*}
\label{eq:7}
\tag{15.7}
$$

### Step 3: Computing the optimal costs

Observe that we have relatively few distinct subproblems: one subproblem for each choice of $$ i $$ and $$ j $$ satisfying $$ i \le j \le n $$, or $$ \binom{n}{2} + n = \Theta(n^2) $$ in all.
A recursive algorithm may encounter each subproblem many times in different branches of its recursion tree.  
Instead of computing the solution to recurrence \eqref{eq:7} recursively, we compute the optimal cost by using a tabular, bottom-up approach.
In order to implement the bottom-up approach, we must determine which entries of the table we refer to when computing $$ m[i, \ j] $$.
Equation \eqref{eq:7} shows that the cost $$ m[i, \ j] $$ of computing a matrix-chain product of $$ j - i + 1 $$ matrices depends only on the costs of computing matrix-chain products of fewer than $$ j - i + 1 $$ matrices.
That is, for $$ k = i, \ i + 1, \dots, \ j - 1 $$, the matrix $$ A_{i..k} $$ is a product of $$ k - i + 1 < j - i + 1 $$ matrices and the matrix $$ A_{k + 1..j} $$ is a product of $$ j - k < j - i + 1 $$ matrices.
Thus, the algorithm should fill in the table $$ m $$ in a manner that corresponds to solving the parenthesization problem on matrix chains of increasing length.

>MATRIX-CHAIN-ORDER($$ p $$)  
>01&nbsp; $$ n = p.length - 1 $$  
>02&nbsp; let $$ m[1..n, \ 1..n] $$ and $$ s[1..n - 1, \ 2..n] $$ be new tables  
>03&nbsp; for $$ i = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ i] = 0 $$  
>05&nbsp; for $$ l = 2 $$ to $$ n $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n - l + 1 $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ j = i + l - 1 $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = \infty $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ k = i $$ to $$ j - 1 $$  
>10&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j $$  
>11&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ q < m[i, \ j] $$  
>12&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = q $$  
>13&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ s[i, \ j] = k $$  
>14&nbsp; return $$ m $$ and $$ s $$

The algorithm first computes $$ m[i, \ i] = 0 $$ for $$ i = 1, \ 2, \dots, \ n $$ in lines 3–4.
It then uses recurrence \eqref{eq:7} to compute $$ m[i, \ i + 1] $$ for $$ i = 1, \ 2, \dots, \ n - 1 $$ (the minimum costs for chains of length $$ l = 2 $$) during the first execution of the for loop in lines 5–13.
The second time through the loop, it computes $$ m[i, \ i + 2] $$ for $$ i = 1, \ 2, \dots, \ n - 2 $$ (the minimum costs for chains of length $$ l = 3 $$), and so forth.
At each step, the $$ m[i, \ j] $$ cost computed in lines 10–13 depends only on table entries $$ m[i, \ k] $$ and $$ m[k + 1, \ j] $$ already computed.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.3.png){: width="700" }
_**Figure 15.3** The $$ m $$ and $$ s $$ tables computed by MATRIX-CHAIN-ORDER for $$ n = 6 $$ and the following matrix dimensions._

Figure 15.3 illustrates this procedure on a chain of $$ n = 6 $$ matrices.
Since we have defined $$ m[i, \ j] $$ only for $$ i \le j $$, only the portion of the table $$ m $$ strictly above the main diagonal is used.
Using this layout, we can find the minimum cost $$ m[i, \ j] $$ for multiplying a subchain $$ A_i A_{i + 1} \cdots A_j $$ of matrices at the intersection of lines running northeast from $$ A_i $$ and northwest from $$ A_j $$.
MATRIX-CHAIN-ORDER computes each entry $$ m[i, \ j] $$ using the products $$ p_{i - 1} p_k p_j $$ for $$ k = i, \ i + 1, \dots, \ j - 1 $$, and all entries southwest and southeast from $$ m[i, \ j] $$.  
A simple inspection of the nested loop structure of MATRIX-CHAIN-ORDER yields a running time of $$ O(n^3) $$ for the algorithm.
The loops are nested three deep, and each loop index takes on at most $$ n - 1 $$ values.
The algorithm requires $$ \Theta(n^2) $$ space to store the $$ m $$ and $$ s $$ tables.
Thus, MATRIX-CHAIN-ORDER is much more efficient than the exponential-time method of enumerating all possible parenthesizations and checking each one.

### Step 4: Constructing an optimal solution

The table $$ s[i..n - 1, \ 2..n] $$ gives us the information we need to show how to multiply the matrices.
Each entry $$ s[i, \ j] $$ records a value of $$ k $$ such that an optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$ splits the product between $$ A_k $$ and $$ A_{k + 1} $$.
Thus, we know that the final matrix multiplication in computing $$ A_{1..n} $$ optimally is $$ A_{1..s[1, \ n]} A_{s[1, \ n] + 1..n} $$.
We can determine the earlier matrix multiplications recursively, since $$ s[1, \ s[1, \ n]] $$ determines the last matrix multiplication when computing $$ A_{1..s[1, \ n]} $$ and $$ s[s[1, \ n] + 1, \ n] $$ determines the last matrix multiplication when computing $$ A_{s[1, \ n] + 1..n} $$.

>PRINT-OPTIMAL-PARENS($$ s, \ i, \ j $$)  
>01&nbsp; if $$ i == j $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print "$$ A $$"$$ _i $$  
>03&nbsp; else print "("  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;PRINT-OPTIMAL-PARENS($$ s, \ i, \ s[i, \ j] $$)  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;PRINT-OPTIMAL-PARENS($$ s, \ s[i, \ j] + 1, \ j $$)  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print ")"

This recursive procedure prints an optimal parenthesization of $$ \langle A_i, \ A_{i + 1}, \dots, \ A_j \rangle $$, given the $$ s $$ table computed by MATRIX-CHAIN-ORDER and the indices $$ i $$ and $$ j $$.
In the example of Figure 15.3, the call PRINT-OPTIMAL-PARENS($$ s, \ 1, \ 6 $$) prints the parenthesization $$ ((A_1 (A_2 A_3)) ((A_4 A_5) A_6)) $$.

## 15.3 Elements of dynamic programming

### Optimal substructure

The first step in solving an optimization problem by dynamic programming is to characterize the structure of an optimal solution.
A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems.
You will find yourself following a common pattern in discovering optimal substructure:

1. You show that a solution to the problem consists of making a choice, and making this choice leaves one or more subproblems to be solved.
2. You suppose that for a given problem, you are given the choice that leads to an optimal solution.
You do not concern yourself yet with how to determine this choice.
3. Given this choice, you determine which subproblems ensue and how to best characterize the resulting space of subproblems.
4. You show that the solutions to the subproblems used within an optimal solution to the problem must themselves be optimal by using a cut-and-paste technique.
You do so by supposing that each of the subproblem solutions is not optimal and then deriving a contradiction.

Optimal substructure varies across problem domains in two ways:

1. how many subproblems an optimal solution to the original problem uses, and
2. how many choices we have in determining which subproblem(s) to use in an optimal solution.

In the rod-cutting problem, an optimal solution for cutting up a rod of size $$ n $$ uses just one subproblem (of size $$ n - i $$), but we must consider $$ n $$ choices for $$ i $$ in order to determine which one yields an optimal solution.
Matrix-chain multiplication for the subchain $$ A_i A_{i + 1} \cdots A_j $$ serves as an example with two subproblems and $$ j - i $$ choices.

Informally, the running time of a dynamic-programming algorithm depends on the product of two factors: the number of subproblems overall and how many choices we look at for each subproblem. In rod cutting, we had $$ \Theta(n) $$ subproblems overall, and at most $$ n $$ choices to examine for each, yielding an $$ O(n^2) $$ running time.
Matrix-chain multiplication had $$ \Theta(n^2) $$ subproblems overall, and in each we had at most $$ n - 1 $$ choices, giving an $$ O(n^3) $$ running time.  
The subproblem graph gives us an alternative way to perform the same analysis.
Each vertex corresponds to a subproblem, and the choices for a subproblem are the edges incident to that subproblem.
In rod cutting, the subproblem graph had $$ n $$ vertices and at most $$ n $$ edges per vertex, yielding an $$ O(n^2) $$ running time.
For matrix-chain multiplication, it would have $$ \Theta(n^2) $$ vertices and each vertex would have degree at most $$ n - 1 $$, giving a total of $$ O(n^3) $$ vertices and edges.

### Subtleties

You should be careful not to assume that optimal substructure applies when it does not.
Consider the following two problems in which we are given a directed graph $$ G = (V, \ E) $$ and vertices $$ u, \ v \in V $$.

**Unweighted shortest path:** Find a path from $$ u $$ to $$ v $$ consisting of the fewest edges.
Such a path must be simple, since removing a cycle from a path produces a path with fewer edges.  
**Unweighted longest simple path:** Find a simple path from $$ u $$ to $$ v $$ consisting of the most edges.
We need to include the requirement of simplicity because otherwise we can traverse a cycle as many times as we like to create paths with an arbitrarily large number of edges.

The unweighted shortest-path exhibits optimal substructure as follows.
Suppose that $$ u \neq v $$, so that the problem is nontrivial.
Then, any path from $$ p $$ from $$ u $$ to $$ v $$ must contain an intermediate vertex $$ w $$.
Thus, we can decompose the path $$ u \overset{p}{\rightsquigarrow} v $$ into subpaths $$ u \overset{p_1}{\rightsquigarrow} w \overset{p_2}{\rightsquigarrow} v $$.
Clearly, the number of edges in $$ p $$ equals the number of edges in $$ p_1 $$ plus the number of edges in $$ p_2 $$.
We claim that if $$ p $$ is a shortest path from $$ u $$ to $$ v $$, then $$ p_1 $$ must be a shortest path from $$ u $$ to $$ w $$.
We use a cut-and-paste argument: if there were another path, say $$ p'_1 $$, from $$ u $$ to $$ w $$ with fewer edges than $$ p_1 $$, then we could cut out $$ p_1 $$ and paste in $$ p'_1 $$ to produce a path $$ u \overset{p'_1}{\rightsquigarrow} w \overset{p_2}{\rightsquigarrow} v $$ with fewer edges than $$ p $$, thus contradicting $$ p $$'s optimality.
Symmetrically, $$ p_2 $$ must be a shortest path from $$ w $$ to $$ v $$.
Thus, we can find a shortest path from $$ u $$ to $$ v $$ by considering all intermediate vertices $$ w $$, finding a shortest path from $$ u $$ to $$ w $$ and a shortest path from $$ w $$ to $$ v $$, and choosing an intermediate vertex $$ w $$ that yields the overall shortest path.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.4.png){: width="400" }
_**Figure 15.4** A directed graph showing that the problem of finding a longest simple path in an unweighted directed graph does not have optimal substructure._

However, in the case of finding an unweighted longest simple path, the problem does not display the same optimal substructure.
Figure 15.4 supplies an example.
Consider the path $$ q \rightarrow r \rightarrow t $$, which is a longest simple path from $$ q $$ to $$ t $$.
Is $$ q \rightarrow r $$ a longest simple path from $$ q $$ to $$ r $$?
No, for the path $$ q \rightarrow s \rightarrow t \rightarrow r $$ is a simple path that is longer.  
This example shows that for longest simple paths, not only does the problem lack optimal substructure, but we cannot necessarily assemble a "legal" solution to the problem from solutions to subproblems.
If we combine the longest simple paths $$ q \rightarrow s \rightarrow t \rightarrow r $$ and $$ r \rightarrow q \rightarrow s \rightarrow t $$, we get the path $$ q \rightarrow s \rightarrow t \rightarrow r \rightarrow q \rightarrow s \rightarrow t $$, which is not simple.
In fact, no efficient dynamic-programming algorithm for this problem has ever been found.
This problem is NP-complete, which means that we are unlikely to find a way to solve it in polynomial time.

Although a solution to a problem for both longest and shortest paths uses two subproblems, the subproblems in finding the longest simple path are not independent, whereas for shortest paths they are.
The independence of subproblems means that solving one subproblem does not affect the solution to another subproblem of the same problem.  
For the example of Figure 15.4, we have the problem of finding a longest simple path from $$ q $$ to $$ t $$ with two subproblems: finding longest simple paths from $$ q $$ to $$ r $$ and from $$ r $$ to $$ t $$.
For the first of these subproblems, we choose the path $$ q \rightarrow s \rightarrow t \rightarrow r $$, and so we have also used the vertices $$ s $$ and $$ t $$.
We can no longer use these vertices in the second subproblem, since the combination of the two solutions to subproblem would yield a path that is not simple.  
On the other hand, the subproblems do not share resources when we are finding a shortest path.
We claim that if a vertex $$ w $$ is on a shortest path $$ p $$ from $$ u $$ to $$ v $$, then we can splice together any shortest path $$ u \overset{p_1}{\rightsquigarrow} w $$ and any shortest path $$ w \overset{p_2}{\rightsquigarrow} v $$ to produce a shortest path from $$ u $$ to $$ v $$.
We are assured that, other than $$ w $$, no vertex can appear in both paths $$ p_1 $$ and $$ p_2 $$.
Suppose that some vertex $$ x \neq w $$ appears in both $$ p_1 $$ and $$ p_2 $$, so that we can decompose $$ p_1 $$ as $$ u \overset{p_{ux}}{\rightsquigarrow} x \rightsquigarrow w $$ and $$ p_2 $$ as $$ w \rightsquigarrow x \overset{p_{xv}}{\rightsquigarrow} v $$.
By the optimal substructure of this problem, path $$ p $$ has as many edges as $$ p_1 $$ and $$ p_2 $$ together; let's say that $$ p $$ has $$ e $$ edges.
Now let us construct a path $$ p' = u \overset{p_{ux}}{\rightsquigarrow} x \overset{p_{xv}}{\rightsquigarrow} v $$ from $$ u $$ to $$ v $$.
Because we have excised the paths from $$ x $$ to $$ w $$ and from $$ w $$ to $$ x $$, each of which contains at least one edge, path $$ p' $$ contains at most $$ e - 2 $$ edges, which contradicts the assumption that $$ p $$ is a shortest path.
Thus, we are assured that the subproblems for the shortest-path problem are independent.

### Overlapping subproblems

When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problem has overlapping subproblems.
In contrast, a problem for which a divide-and-conquer approach is suitable usually generates brand-new problems at each step of the recursion.
Dynamic-programming algorithms typically take advantage of overlapping subproblems by solving each subproblem once and then storing the solution in a table where it can be looked up when needed, using constant time per lookup.

To illustrate the overlapping-subproblems property in greater detail, let us reexamine the matrix-chain multiplication problem.
Referring back to Figure 15.3, observe that MATRIX-CHAIN-ORDER repeatedly looks up the solution to subproblems in lower rows when solving subproblems in higher rows.
Consider the following inefficient recursive procedure that determines $$ m[i, \ j] $$, the minimum number of scalar multiplications needed to compute the matrix-chain product $$ A_{i..j} = A_i A_{i + 1} \cdots A_j $$.
The procedure is based directly on the recurrence \eqref{eq:7}.

>RECURSIVE-MATRIX-CHAIN($$ p, \ i, \ j $$)  
>01&nbsp; if $$ i == j $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ 0 $$  
>03&nbsp; $$ m[i, \ j] = \infty $$  
>04&nbsp; for $$ k = i $$ to $$ j - 1 $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = $$ RECURSIVE-MATRIX-CHAIN($$ p, \ i, \ k $$) $$ + $$ RECURSIVE-MATRIX-CHAIN($$ p, \ k + 1, \ j $$) $$ + \ p_{i - 1} p_k p_j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ q < m[i, \ j] $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = q $$  
>08&nbsp; return $$ m[i, \ j] $$

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.5.png){: width="700" }
_**Figure 15.5** The recursion tree for the computation of RECURSIVE-MATRIX-CHAIN($$ p, \ 1, \ 4 $$). The computations performed in a shaded subtree are replaced by a single table lookup in MEMOIZED-MATRIX-CHAIN._