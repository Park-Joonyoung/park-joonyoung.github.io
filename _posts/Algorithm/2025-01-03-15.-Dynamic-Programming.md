---
title: 15. Dynamic Programmming
# description: Short summary of the post
date: 2024-12-15 19:06
categories: [Computer Science, Algorithm]
tags: [dynamic-programming, rod-cutting-problem, top-down-approach, bottom-up-approach, subproblem-graph]     # TAG names should always be lowercase
math: true
pin: false
---

Dynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems.
Divide-and-conquer algorithms partition the problem into disjoint subproblems, solve the subproblems recursively, and then combine their solutions to solve the original problem.
In contrast, dynamic programming applies when the subproblems overlap—that is, when subproblems share subsubproblems.
A dynamic-programming algorithm solves each subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblems.
When developing a dynamic-programming algorithm, we follow a sequence of four steps:

1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.

## 15.1 Rod cutting

In a rod-cutting problem, we buy a long steel rod, cut it into shorter rods, and sell them.
Each cut is free.
We want to know the best way to cut up the rods.
We assume that we know, for $$ i = 1, \ 2, \dots $$, the price $$ p_i $$ in dollars that we can charge for a rod of length $$ i $$ inches.
Rod lengths are always an integral number of inches.

The rod-cutting problem is the following.
Given a rod of length $$ n $$ inches and a table of prices $$ p_i $$ for $$ i = 1, \ 2, \dots, \ n $$, determine the maximum revenue $$ r_n $$ obtainable by cutting up the rod and selling the pieces.
We can cut up a rod of length $$ n $$ in $$ 2^{n - 1} $$ different ways, since we have an independent option of cutting, or not cutting, at distance $$ i $$ inches from the left end, for $$ i = 1, \ 2, \dots, \ n - 1 $$.
If an optimal solution cuts the rod into $$ k $$ pieces, for some $$ 1 \le k \le n $$, then an optimal decomposition

$$
\begin{align*}
    n = i_1 + i_2 + \cdots + i_k
\end{align*}
$$

of the rod into pieces of lengths $$ i_1, \ i_2, \dots, \ i_k $$ provides maximum corresponding revenue

$$
\begin{align*}
    r_n = p_{i_1} + p_{i_2} + \cdots + p_{i_k}
\end{align*}
$$

Generally, we can frame the values $$ r_n $$ for $$ n \ge 1 $$ in terms of optimal revenues from shorter rods:

$$
\begin{align*}
    r_n = \max{(p_n, \ r_1 + r_{n - 1}, \ r_2 + r_{n - 2}, \dots, \ r_{n - 1} + r_1)}
\end{align*}
\label{eq:1}
\tag{15.1}
$$

The first argument, $$ p_n $$, corresponds to making no cuts at all and selling the rod of length $$ n $$ as is.
The other $$ n - 1 $$ arguments to max correspond to the maximum revenue obtained by making an initial cut of the rod into two pieces of size $$ i $$ and $$ n - i $$, for each $$ i = 1, \ 2, \dots, \ n - 1 $$, and then optimally cutting up those pieces further, obtaining revenues $$ r_i $$ and $$ r_{n - i} $$ from those two pieces.
Since we don't know ahead of time which value of $$ i $$ optimizes revenue, we have to consider all possible values for $$ i $$ and pick the one that maximizes revenue.

Note that to solve the original problem of size $$ n $$, we solve smaller problems of the same type, but of smaller sizes.
Once we make the first cut, we may consider the two pieces as independent instances of the rod-cutting problem.
The overall optimal solution incorporates optimal solutions to the two related subproblems, maximizing revenue from each of those two pieces.
We say that the rod-cutting problem exhibits optimal substructure: optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently.

In a related way to arrange a recursive structure for the rod-cutting problem, we view a decomposition as consisting of a first piece of length $$ i $$ cut off the left-hand end, and then a right-hand remainder of length $$ n - i $$.
Only the remainder, and not the first piece, may be further divided.
With this approach, we can couch the solution with no cuts at all as saying that the first piece has size $$ i = n $$ and revenue $$ p_n $$ and that the remainder has size 0 with corresponding $$ r_0 = 0 $$.
We thus obtain the following simpler version of equation \eqref{eq:1}:

$$
\begin{align*}
    r_n = \max_{1 \le i \le n} (p_i + r_{n - i})
\end{align*}
\label{eq:2}
\tag{15.2}
$$

In this formulation, an optimal solution embodies the solution to only one related subproblem—the remainder—rather than two.

### Recursive top-down implementation

The following procedure implements the computation implicit in equation \eqref{eq:2} in a straightforward, top-down, recursive manner.

>CUT-ROD($$ p, \ n $$)  
>01&nbsp; if $$ n == 0 $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ 0 $$  
>03&nbsp; $$ q = -\infty $$  
>04&nbsp; for $$ i = 1 $$ to $$ n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ CUT-ROD($$ p, \ n - i $$))  
>06&nbsp; return $$ q $$

If $$ n = 0 $$, no revenue is possible, and so CUT-ROD returns $$ 0 $$ in line 2.
Line 3 initializes the maximum revenue $$ q $$ to $$ -\infty $$, so that the for loop in lines 4–5 correctly computes $$ q = \max_{i \le i \le n} (p_i \ + $$ CUT-ROD($$ p, \ n - i $$)); line 6 then returns this value.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.1.png){: width="700" }
_**Figure 15.1** The recursion tree showing recursive calls resulting from a call CUT-ROD($$ p, \ n $$) for $$ n = 4 $$._

If you were to code up CUT-ROD and run it on your computer, you would find that once the input size becomes moderately large, your program would take a long time to run.
The problem is that CUT-ROD calls itself recursively over and over again with the same parameter values; it solves the same subproblems repeatedly.
Figure 15.1 illustrates what happens for $$ n = 4 $$: CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ n - i $$) for $$ i = 1, \ 2, \dots, \ n $$.
Equivalently, CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ j $$) for each $$ j = 0, \ 1, \dots, \ n - 1 $$.
When this process unfolds recursively, the amount of work done, as a function of $$ n $$, grows explosively.

To analyze the running time of CUT-ROD, let $$ T(n) $$ denote the total number of calls made to CUT-ROD when called with its second parameter equal to $$ n $$.
This expression equals the number of nodes in a subtree whose root is labeled $$ n $$ in the recursion tree.
The count includes the initial call at its root.
Thus, $$ T(0) = 1 $$ and

$$
\begin{align*}
    T(n) = 1 + \sum_{j = 0}^{n - 1} T(j)
\end{align*}
\label{eq:3}
\tag{15.3}
$$

The initial $$ 1 $$ is for the call at the root, and the term $$ T(j) $$ counts the number of calls due to the call CUT-ROD($$ p, \ n - i $$), where $$ j = n - i $$.
The solution of this recurrence is

$$
\begin{align*}
    T(n) = 2^n
\end{align*}
\label{eq:4}
\tag{15.4}
$$

and so the running time of CUT-ROD is exponential in $$ n $$.

### Using dynamic programming for optimal rod cutting

The dynamic-programming method works as follows.
Having observed that a naive recursive solution is inefficient because it solves the same problems repeatedly, we arrange for each subproblem to be solved only once, saving its solution.
If we need to refer to this subproblem's solution again later, we just look it up, rather than recompute it.
Dynamic programming thus uses additional memory to save computation time; it serves an example of a time-memory trade-off.

There are usually two equivalent ways to implement a dynamic-programming approach.
The first approach is top-down with memoization.
In this approach, we write the procedure recursively in a natural manner, but modified to save the result of each subproblem.
The procedure now first checks to see whether it has previously solved this subproblem.
If so, it returns the saved value, saving further computation at this level; if not, the procedure computes the value in the usual manner.
The second approach is the bottom-up method.
In this approach, we sort the subproblems by size and solve them in size order, smallest first.
When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions.
We solve each subproblem only once, and when we first see it, we have already solved all of its prerequisite subproblems.  
These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems.
The bottom-up approach often has much better constant factors, since it has less overhead for procedure calls.

>MEMOIZED-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; for $$ i = 0 $$ to $$ n $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[i] = -\infty $$  
>04&nbsp; return MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)

>MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)  
>01&nbsp; if $$ r[n] \ge 0 $$  
>01&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ r[n] $$  
>03&nbsp; if $$ n == 0 $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = 0 $$  
>05&nbsp; else $$ q = -\infty $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ MEMOIZED-CUT-ROD-AUX($$ p, \ n - i, \ r $$))  
>08&nbsp; $$ r[n] = q $$  
>09&nbsp; return $$ q $$

The main procedure MEMOIZED-CUT-ROD initializes a new auxiliary array $$ r[0..n] $$ with the value $$ -\infty $$, then calls MEMOIZED-CUT-ROD-AUX.
The procedure MEMOIZED-CUT-ROD-AUX is just the memoized version of the previous procedure, CUT-ROD.
It first checks in line 1 to see whether the desired value is already known and, if it is, then line 2 returns it.
Otherwise, lines 3–7 compute the desired value $$ q $$ in the usual manner, line 8 saves it in $$ r[n] $$, and line 9 returns it.

>BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; $$ r[0] = 0 $$  
>03&nbsp; for $$ j = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = -\infty $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] + r[j - i]) $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[j] = q $$  
>08&nbsp; return $$ r[n] $$

For the bottom-up dynamic-programming approach, BOTTOM-UP-CUT-ROD uses the natural ordering of the subproblems: a subproblem of size $$ i $$ is smaller than a subproblem of size $$ j $$ if $$ i < j $$.
Thus, the procedure solves subproblems of sizes $$ j = 0, \ 1, \dots, \ n $$, in that order.
Line 1 of the procedure creates a new array $$ r[0..n] $$ in which to save the results of the subproblems, and line 2 initializes $$ r[0] $$ to $$ 0 $$, since a rod of length 0 earns no revenue.
Lines 3–6 solve each subproblem of size $$ j $$, for $$ j = 1, \ 2, \dots, \ n $$, in order of increasing size.
Line 7 saves in $$ r[j] $$ the solution to the subproblem of size $$ j $$.
Finally, line 8 returns $$ r[n] $$, which equals the optimal value $$ r_n $$.

The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure BOTTOM-UP-CUT-ROD is $$ \Theta(n^2) $$, due to its doubly-nested loop structure.
The running time of its top-down counterpart, MEMOIZED-CUT-ROD, is also $$ \Theta(n^2) $$.
Because a recursive call to solve a previously solved subproblem returns immediately, MEMOIZED-CUT-ROD solves each subproblem just once.
It solves subproblems for sizes $$ 0, \ 1, \dots, \ n $$.
To solve a subproblem of size $$ n $$, the for loop of lines 6–7 iterates $$ n $$ times.
Thus, the total number of iterations of this for loop forms an arithmetic series, giving a total of $$ \Theta(n^2) $$ iterations. (We actually are using a form of aggregate analysis here.)