---
title: 15. Dynamic Programmming
# description: Short summary of the post
date: 2024-12-15 19:06
categories: [Computer Science, Algorithm]
tags: [dynamic-programming, rod-cutting-problem, subproblem-graph, matrix-chain-multiplication, longest-common-subsequence, optimal-binary-search-tree]     # TAG names should always be lowercase
math: true
pin: false
---

Dynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems.
Divide-and-conquer algorithms partition the problem into disjoint subproblems, solve the subproblems recursively, and then combine their solutions to solve the original problem.
In contrast, dynamic programming applies when the subproblems overlap—that is, when subproblems share subsubproblems.
A dynamic-programming algorithm solves each subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time it solves each subsubproblems.
When developing a dynamic-programming algorithm, we follow a sequence of four steps:

1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.

## 15.1 Rod cutting

In a rod-cutting problem, we buy a long steel rod, cut it into shorter rods, and sell them.
Each cut is free.
We want to know the best way to cut up the rods.
We assume that we know, for $$ i = 1, \ 2, \dots $$, the price $$ p_i $$ in dollars that we can charge for a rod of length $$ i $$ inches.
Rod lengths are always an integral number of inches.

The rod-cutting problem is the following.
Given a rod of length $$ n $$ inches and a table of prices $$ p_i $$ for $$ i = 1, \ 2, \dots, \ n $$, determine the maximum revenue $$ r_n $$ obtainable by cutting up the rod and selling the pieces.
We can cut up a rod of length $$ n $$ in $$ 2^{n - 1} $$ different ways, since we have an independent option of cutting, or not cutting, at distance $$ i $$ inches from the left end, for $$ i = 1, \ 2, \dots, \ n - 1 $$.
If an optimal solution cuts the rod into $$ k $$ pieces, for some $$ 1 \le k \le n $$, then an optimal decomposition

$$
\begin{align*}
    n = i_1 + i_2 + \cdots + i_k
\end{align*}
$$

of the rod into pieces of lengths $$ i_1, \ i_2, \dots, \ i_k $$ provides maximum corresponding revenue

$$
\begin{align*}
    r_n = p_{i_1} + p_{i_2} + \cdots + p_{i_k}
\end{align*}
$$

Generally, we can frame the values $$ r_n $$ for $$ n \ge 1 $$ in terms of optimal revenues from shorter rods:

$$
\begin{align*}
    r_n = \max{(p_n, \ r_1 + r_{n - 1}, \ r_2 + r_{n - 2}, \dots, \ r_{n - 1} + r_1)}
\end{align*}
\label{eq:1}
\tag{15.1}
$$

The first argument, $$ p_n $$, corresponds to making no cuts at all and selling the rod of length $$ n $$ as is.
The other $$ n - 1 $$ arguments to max correspond to the maximum revenue obtained by making an initial cut of the rod into two pieces of size $$ i $$ and $$ n - i $$, for each $$ i = 1, \ 2, \dots, \ n - 1 $$, and then optimally cutting up those pieces further, obtaining revenues $$ r_i $$ and $$ r_{n - i} $$ from those two pieces.
Since we don't know ahead of time which value of $$ i $$ optimizes revenue, we have to consider all possible values for $$ i $$ and pick the one that maximizes revenue.

Note that to solve the original problem of size $$ n $$, we solve smaller problems of the same type, but of smaller sizes.
Once we make the first cut, we may consider the two pieces as independent instances of the rod-cutting problem.
The overall optimal solution incorporates optimal solutions to the two related subproblems, maximizing revenue from each of those two pieces.
We say that the rod-cutting problem exhibits optimal substructure: optimal solutions to a problem incorporate optimal solutions to related subproblems, which we may solve independently.

In a related way to arrange a recursive structure for the rod-cutting problem, we view a decomposition as consisting of a first piece of length $$ i $$ cut off the left-hand end, and then a right-hand remainder of length $$ n - i $$.
Only the remainder, and not the first piece, may be further divided.
With this approach, we can couch the solution with no cuts at all as saying that the first piece has size $$ i = n $$ and revenue $$ p_n $$ and that the remainder has size 0 with corresponding $$ r_0 = 0 $$.
We thus obtain the following simpler version of equation \eqref{eq:1}:

$$
\begin{align*}
    r_n = \max_{1 \le i \le n} (p_i + r_{n - i})
\end{align*}
\label{eq:2}
\tag{15.2}
$$

In this formulation, an optimal solution embodies the solution to only one related subproblem—the remainder—rather than two.

### Recursive top-down implementation

The following procedure implements the computation implicit in equation \eqref{eq:2} in a straightforward, top-down, recursive manner.

>CUT-ROD($$ p, \ n $$)  
>01&nbsp; if $$ n == 0 $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ 0 $$  
>03&nbsp; $$ q = -\infty $$  
>04&nbsp; for $$ i = 1 $$ to $$ n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ CUT-ROD($$ p, \ n - i $$))  
>06&nbsp; return $$ q $$

If $$ n = 0 $$, no revenue is possible, and so CUT-ROD returns $$ 0 $$ in line 2.
Line 3 initializes the maximum revenue $$ q $$ to $$ -\infty $$, so that the for loop in lines 4–5 correctly computes $$ q = \max_{i \le i \le n} (p_i \ + $$ CUT-ROD($$ p, \ n - i $$)); line 6 then returns this value.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.1.png){: width="700" }
_**Figure 15.1** The recursion tree showing recursive calls resulting from a call CUT-ROD($$ p, \ n $$) for $$ n = 4 $$._

If you were to code up CUT-ROD and run it on your computer, you would find that once the input size becomes moderately large, your program would take a long time to run.
The problem is that CUT-ROD calls itself recursively over and over again with the same parameter values; it solves the same subproblems repeatedly.
Figure 15.1 illustrates what happens for $$ n = 4 $$: CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ n - i $$) for $$ i = 1, \ 2, \dots, \ n $$.
Equivalently, CUT-ROD($$ p, \ n $$) calls CUT-ROD($$ p, \ j $$) for each $$ j = 0, \ 1, \dots, \ n - 1 $$.
When this process unfolds recursively, the amount of work done, as a function of $$ n $$, grows explosively.

To analyze the running time of CUT-ROD, let $$ T(n) $$ denote the total number of calls made to CUT-ROD when called with its second parameter equal to $$ n $$.
This expression equals the number of nodes in a subtree whose root is labeled $$ n $$ in the recursion tree.
The count includes the initial call at its root.
Thus, $$ T(0) = 1 $$ and

$$
\begin{align*}
    T(n) = 1 + \sum_{j = 0}^{n - 1} T(j)
\end{align*}
\label{eq:3}
\tag{15.3}
$$

The initial $$ 1 $$ is for the call at the root, and the term $$ T(j) $$ counts the number of calls due to the call CUT-ROD($$ p, \ n - i $$), where $$ j = n - i $$.
The solution of this recurrence is

$$
\begin{align*}
    T(n) = 2^n
\end{align*}
\label{eq:4}
\tag{15.4}
$$

and so the running time of CUT-ROD is exponential in $$ n $$.

### Using dynamic programming for optimal rod cutting

The dynamic-programming method works as follows.
Having observed that a naive recursive solution is inefficient because it solves the same problems repeatedly, we arrange for each subproblem to be solved only once, saving its solution.
If we need to refer to this subproblem's solution again later, we just look it up, rather than recompute it.
Dynamic programming thus uses additional memory to save computation time; it serves an example of a time-memory trade-off.

There are usually two equivalent ways to implement a dynamic-programming approach.
The first approach is top-down with memoization.
In this approach, we write the procedure recursively in a natural manner, but modified to save the result of each subproblem.
The procedure now first checks to see whether it has previously solved this subproblem.
If so, it returns the saved value, saving further computation at this level; if not, the procedure computes the value in the usual manner.
The second approach is the bottom-up method.
In this approach, we sort the subproblems by size and solve them in size order, smallest first.
When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions.
We solve each subproblem only once, and when we first see it, we have already solved all of its prerequisite subproblems.  
These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems.
The bottom-up approach often has much better constant factors, since it has less overhead for procedure calls.

>MEMOIZED-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; for $$ i = 0 $$ to $$ n $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[i] = -\infty $$  
>04&nbsp; return MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)

>MEMOIZED-CUT-ROD-AUX($$ p, \ n, \ r $$)  
>01&nbsp; if $$ r[n] \ge 0 $$  
>01&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ r[n] $$  
>03&nbsp; if $$ n == 0 $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = 0 $$  
>05&nbsp; else $$ q = -\infty $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] \ + $$ MEMOIZED-CUT-ROD-AUX($$ p, \ n - i, \ r $$))  
>08&nbsp; $$ r[n] = q $$  
>09&nbsp; return $$ q $$

The main procedure MEMOIZED-CUT-ROD initializes a new auxiliary array $$ r[0..n] $$ with the value $$ -\infty $$, then calls MEMOIZED-CUT-ROD-AUX.
The procedure MEMOIZED-CUT-ROD-AUX is just the memoized version of the previous procedure, CUT-ROD.
It first checks in line 1 to see whether the desired value is already known and, if it is, then line 2 returns it.
Otherwise, lines 3–7 compute the desired value $$ q $$ in the usual manner, line 8 saves it in $$ r[n] $$, and line 9 returns it.

>BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ be a new array  
>02&nbsp; $$ r[0] = 0 $$  
>03&nbsp; for $$ j = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = -\infty $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = \max(q, \ p[i] + r[j - i]) $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[j] = q $$  
>08&nbsp; return $$ r[n] $$

For the bottom-up dynamic-programming approach, BOTTOM-UP-CUT-ROD uses the natural ordering of the subproblems: a subproblem of size $$ i $$ is smaller than a subproblem of size $$ j $$ if $$ i < j $$.
Thus, the procedure solves subproblems of sizes $$ j = 0, \ 1, \dots, \ n $$, in that order.
Line 1 of the procedure creates a new array $$ r[0..n] $$ in which to save the results of the subproblems, and line 2 initializes $$ r[0] $$ to $$ 0 $$, since a rod of length 0 earns no revenue.
Lines 3–6 solve each subproblem of size $$ j $$, for $$ j = 1, \ 2, \dots, \ n $$, in order of increasing size.
Line 7 saves in $$ r[j] $$ the solution to the subproblem of size $$ j $$.
Finally, line 8 returns $$ r[n] $$, which equals the optimal value $$ r_n $$.

The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure BOTTOM-UP-CUT-ROD is $$ \Theta(n^2) $$, due to its doubly-nested loop structure.
The running time of its top-down counterpart, MEMOIZED-CUT-ROD, is also $$ \Theta(n^2) $$.
Because a recursive call to solve a previously solved subproblem returns immediately, MEMOIZED-CUT-ROD solves each subproblem just once.
It solves subproblems for sizes $$ 0, \ 1, \dots, \ n $$.
To solve a subproblem of size $$ n $$, the for loop of lines 6–7 iterates $$ n $$ times.
Thus, the total number of iterations of this for loop forms an arithmetic series, giving a total of $$ \Theta(n^2) $$ iterations. (We actually are using a form of aggregate analysis here.)

### Subproblem graphs

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.2.png){: width="300" }
_**Figure 15.2** The subproblem graph for the rod-cutting problem with $$ n = 4 $$. A directed edge ($$ x, \ y $$) indicates that we need a solution to subproblem $$ y $$ when solving subproblem $$ x $$._

When we think about a dynamic-programming problem, we should understand the set of subproblems involved and how subproblems depend on one another.
The subproblem graph for the problem embodies exactly this information.
The subproblem graph has a directed edge from the vertex for subproblem $$ x $$ to the vertex for subproblem $$ y $$ if determining an optimal soltuion for subproblem $$ x $$ involves directly considering an optimal solution for subproblem $$ y $$.
We can think of the subproblem graph as a reduced or collapsed version of the recursion tree for the top-down recursive method, in which we coalesce all nodes for the same subproblem into a single vertex and direct all edges from parent to child.
The bottom-up method for dynamic programming considers the vertices of the subproblem graph in such an order that we solve the subproblems $$ y $$ adjacent to a given subproblem $$ x $$ before we solve subproblem $$ x $$.
In other words, no subproblem is considered until all of the subproblems it depends upon have been solved.

The size of the subproblem graph $$ G = (V, \ E) $$ can help us determine the running time of the dynamic programming algorithm.
Since we solve each subproblem just once, the running time is the sum of the times needed to solve each subproblem.
Typically, the time to compute the solution to a subproblem is proportional to the degree (number of outgoing edges) of the corresponding vertex in the subproblem graph, and the number of subproblems is equal to the number of vertices in the subproblem graph.
In this common case, the running time of dynamic programming is linear the number of vertices and edges.

### Reconstructing a solution

>EXTENDED-BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>01&nbsp; let $$ r[0..n] $$ and $$ s[0..n] $$ be new arrays  
>02&nbsp; $$ r[0] = 0 $$  
>03&nbsp; for $$ j = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = -\infty $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ q < p[i] + r[j - i] $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = p[i] + r[j - i] $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ s[j] = i $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ r[j] = q $$  
>10&nbsp; return $$ r $$ and $$ s $$

This procedure is similar to BOTTOM-UP-CUT-ROD, except that it creates the array $$ s $$ in line 1, and it updates $$ s[j] $$ in line 8 to hold the optimal size $$ i $$ of the first piece to cut off when solving a subproblem of size $$ j $$.

>PRINT-CUT-ROD-SOLUTION($$ p, \ n $$)  
>01&nbsp; $$ (r, \ s) = $$ EXTENDED-BOTTOM-UP-CUT-ROD($$ p, \ n $$)  
>02&nbsp; while $$ n > 0 $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print $$ s[n] $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ n = n - s[n] $$

This procedure calls EXTENDED-BOTTOM-UP-CUT-ROD to compute the array $$ s[1..n] $$ of optimal first-piece sizes and then prints out the complete list of piece sizes in an optimal decomposition of a rod of length $$ n $$.

## 15.2 Matrix-chain multiplication

We are given a sequence $$ \langle A_1, \ A_2, \dots, \ A_n \rangle $$ of $$ n $$ matrices to be multiplied, and we wish to compute the product

$$
\begin{align*}
    A_1 A_2 \cdots A_n
\end{align*}
\label{eq:5}
\tag{15.5}
$$

We can evaluate the expression \eqref{eq:5} using the standard algorithm for multiplying pairs of matrices as a subroutine once we have parenthesized it to resolve all ambiguities in how the matrices are multiplied together.
Matrix multiplication is associative, and so all parenthesization yield the same product.
How we parenthesize a chain of matrices can have a dramatic impact on the cost of evaluating the product.  
Consider first the cost of multiplying two matrices.

>MATRIX-MULTIPLY($$ A, \ B $$)  
>01&nbsp; if $$ A.columns \neq B.rows $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;error "incompatible dimensions"  
>03&nbsp; else let $$ C $$ be a new $$ A.rows \times B.columns $$ matrix  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ A.rows $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ j = 1 $$ to $$ B.columns $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = 0 $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ k = 1 $$ to $$ A.columns $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c_{ij} = c_{ij} + a_{ik} \cdot b_{kj} $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ C $$

If $$ A $$ is a $$ p \times q $$ matrix and $$ B $$ is a $$ q \times r $$ matrix, the resulting matrix $$ C $$ is a $$ p \times r $$ matrix.
The time to compute $$ C $$ is dominated by the number of scalar multiplications in line 8, which is $$ pqr $$.

We state the matrix-chain multiplication problem as follows: given a chain $$ \langle A_1, \ A_2, \dots, \ A_n \rangle $$ of $$ n $$ matrices, where for $$ i = 1, \ 2, \dots, \ n $$, matrix $$ A_i $$ has dimension $$ p_{i - 1} \times p_i $$, fully parenthesize the product $$ A_1 A_2 \cdots A_n $$ in a way that minimizes the number of scalar multiplications.

### Counting the number of parenthesizations

Before solving the matrix-chain multiplication problem by dynamic programming, let's find out why exhaustively checking all possible parenthesizations does not yield an efficient algorithm.
Denote the number of alternative parenthesizations of a sequence of $$ n $$ matrices by $$ P(n) $$.
When $$ n = 1 $$, we have just one matrix and therefore only one way to fully parenthesize the matrix product.
When $$ n \ge 2 $$, a fully parenthesized matrix product is the product of two fully parenthesized matrix subproducts, and the split between the two subproducts may occur between the $$ k $$th and $$ (k + 1) $$st matrices for any $$ k = 1, \ 2, \dots, \ n - 1 $$.
Thus, we obtain the recurrence

$$
\begin{align*}
    P(n) =
    \begin{cases}
        1 & \text{if } n = 1 \\
        \displaystyle \sum_{k = 1}^{n - 1} P(k) P(n - k) & \text{if } n \ge 2
    \end{cases}
\end{align*}
\label{eq:6}
\tag{15.6}
$$

The solution to this recurrence is the sequence of Catalan numbers, which grows as $$ \Omega(4^n / n^{3/2}) $$.
The number of solutions is thus exponential in $$ n $$, and the brute-force method of exhaustive search makes for a poor strategy when determining how to optimally parenthesize a matrix chain.

### Applying dynamic programming

### Step 1: The structure of an optimal parenthesization

For our first step in the dynamic-programming paradigm, we find the optimal substructure and then use it to construct an optimal solution to the problem from optimal solutions to subproblems.
In the matrix-chain multiplication problem, we can perform this step as follows.
Observe that if the problem is nontrivial, i.e., $$ i < j $$, then to parenthesize the product $$ A_i A_{i + 1} \cdots A_j $$, we must split the product between $$ A_k $$ and $$ A_{k + 1} $$ for some integer $$ k $$ in the range $$ i \le k < j $$.
That is, for some value of $$ k $$, we first compute the matrices $$ A_{i..k} $$ and $$ A_{k + 1..j} $$ and then multiply them together to produce the final product $$ A_{i..j} $$.
The cost of parenthesizing this way is the cost of computing the matrix $$ A_{i..k} $$, plus the cost of computing $$ A_{k + 1..j} $$, plust the cost of multiplying them together.

The optimal substructure of this problem is as follows.
Suppose that to optimally parenthesize $$ A_i A_{i + 1} \cdots A_j $$, we split the product between $$ A_k $$ and $$ A_{k + 1} $$.
Then the way we parenthesize the prefix subchain $$ A_i A_{i + 1} \cdots A_k $$ within this optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$ must be an optimal parenthesization of $$ A_i A_{i + 1} \cdots A_k $$.
Because if there were a less costly way to parenthesize $$ A_i A_{i + 1} \cdots A_k $$, then we could substitute that parenthesization in the optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$: a contradiction.
A similar observation holds for how we parenthesize the subchain $$ A_{k + 1} A_{k + 2} \cdots A_j $$ in the optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$.

Now we use our optimal substructure to show that we can construct an optimal solution to the problem from optimal solutions to subproblems.
We can build an optimal solution to an instance of the matrix-chain multiplication problem by splitting the problem into two subproblems (optimally parenthesizing $$ A_i A_{i + 1} \cdots A_k $$ and $$ A_{k + 1} A_{k + 2} \cdots A_j $$), finding optimal solutions to subproblem instances, and then combining these optimal subproblem solutions.

### Step 2: A recursive solution

Next, we define the cost of an optimal solution recursively in terms of the optimal solutions to subproblems.
Let $$ m[i, \ j] $$ be the minimum number of scalar multiplications needed to compute the matrix $$ A_{i..j} $$; for the full problem, the lowest-cost way to compute $$ A_{1..n} $$ would thus be $$ m[1, \ n] $$.  
We can define $$ m[i, \ j] $$ recursively as follows.
If $$ i = j $$, the problem is trivial; the chain consists of just one matrix $$ A_{i..i} = A_i $$, so that no scalar multiplications are necessary to compute the product.
Thus, $$ m[i, \ i] = 0 $$ for $$ i = 1, \ 2, \dots, \ n $$.
To compute $$ m[i, \ j] $$ when $$ i < j $$, we take advantage of the structure of an optimal solution from step 1.
Let us assume that to optimally parenthesize, we split the product $$ A_i A_{i + 1} \cdots A_j $$ between $$ A_k $$ and $$ A_{k + 1} $$, where $$ i \le k < j $$.
Then, $$ m[i, \ j] $$ equals the minimum cost for computing the subproducts $$ A_{i..k} $$ and $$ A_{k + 1..j} $$, plus the cost of multiplying these two matrices together.
Recalling that each matrix $$ A_i $$ is $$ p_{i - 1} \times p_{i} $$, we see that computing the matrix product $$ A_{i..k} A_{k + 1..j} $$ takes $$ p_{i - 1} p_k p_j $$ scalar multiplications.
Thus, we obtain

$$
\begin{align*}
    m[i, \ j] = m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j
\end{align*}
$$

There are $$ j - 1 $$ possible values of $$ k $$, namely $$ k = i, \ i + 1, \dots, \ j - 1 $$.
Thus, our recursive definition for the minimum cost of parenthesizing the product $$ A_i A_{i + 1} \cdots A_j $$ becomes

$$
\begin{align*}
    m[i, \ j] =
    \begin{cases}
        0 & \text{if } i = j \\
        \displaystyle \min_{i \le k < j} \{ m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j \} & \text{if } i < j
    \end{cases}
\end{align*}
\label{eq:7}
\tag{15.7}
$$

### Step 3: Computing the optimal costs

Observe that we have relatively few distinct subproblems: one subproblem for each choice of $$ i $$ and $$ j $$ satisfying $$ i \le j \le n $$, or $$ \binom{n}{2} + n = \Theta(n^2) $$ in all.
A recursive algorithm may encounter each subproblem many times in different branches of its recursion tree.  
Instead of computing the solution to recurrence \eqref{eq:7} recursively, we compute the optimal cost by using a tabular, bottom-up approach.
In order to implement the bottom-up approach, we must determine which entries of the table we refer to when computing $$ m[i, \ j] $$.
Equation \eqref{eq:7} shows that the cost $$ m[i, \ j] $$ of computing a matrix-chain product of $$ j - i + 1 $$ matrices depends only on the costs of computing matrix-chain products of fewer than $$ j - i + 1 $$ matrices.
That is, for $$ k = i, \ i + 1, \dots, \ j - 1 $$, the matrix $$ A_{i..k} $$ is a product of $$ k - i + 1 < j - i + 1 $$ matrices and the matrix $$ A_{k + 1..j} $$ is a product of $$ j - k < j - i + 1 $$ matrices.
Thus, the algorithm should fill in the table $$ m $$ in a manner that corresponds to solving the parenthesization problem on matrix chains of increasing length.

>MATRIX-CHAIN-ORDER($$ p $$)  
>01&nbsp; $$ n = p.length - 1 $$  
>02&nbsp; let $$ m[1..n, \ 1..n] $$ and $$ s[1..n - 1, \ 2..n] $$ be new tables  
>03&nbsp; for $$ i = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ i] = 0 $$  
>05&nbsp; for $$ l = 2 $$ to $$ n $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n - l + 1 $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ j = i + l - 1 $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = \infty $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ k = i $$ to $$ j - 1 $$  
>10&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ q = m[i, \ k] + m[k + 1, \ j] + p_{i - 1} p_k p_j $$  
>11&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ q < m[i, \ j] $$  
>12&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = q $$  
>13&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ s[i, \ j] = k $$  
>14&nbsp; return $$ m $$ and $$ s $$

The algorithm first computes $$ m[i, \ i] = 0 $$ for $$ i = 1, \ 2, \dots, \ n $$ in lines 3–4.
It then uses recurrence \eqref{eq:7} to compute $$ m[i, \ i + 1] $$ for $$ i = 1, \ 2, \dots, \ n - 1 $$ (the minimum costs for chains of length $$ l = 2 $$) during the first execution of the for loop in lines 5–13.
The second time through the loop, it computes $$ m[i, \ i + 2] $$ for $$ i = 1, \ 2, \dots, \ n - 2 $$ (the minimum costs for chains of length $$ l = 3 $$), and so forth.
At each step, the $$ m[i, \ j] $$ cost computed in lines 10–13 depends only on table entries $$ m[i, \ k] $$ and $$ m[k + 1, \ j] $$ already computed.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.3.png){: width="700" }
_**Figure 15.3** The $$ m $$ and $$ s $$ tables computed by MATRIX-CHAIN-ORDER for $$ n = 6 $$ and the following matrix dimensions._

Figure 15.3 illustrates this procedure on a chain of $$ n = 6 $$ matrices.
Since we have defined $$ m[i, \ j] $$ only for $$ i \le j $$, only the portion of the table $$ m $$ strictly above the main diagonal is used.
Using this layout, we can find the minimum cost $$ m[i, \ j] $$ for multiplying a subchain $$ A_i A_{i + 1} \cdots A_j $$ of matrices at the intersection of lines running northeast from $$ A_i $$ and northwest from $$ A_j $$.
MATRIX-CHAIN-ORDER computes each entry $$ m[i, \ j] $$ using the products $$ p_{i - 1} p_k p_j $$ for $$ k = i, \ i + 1, \dots, \ j - 1 $$, and all entries southwest and southeast from $$ m[i, \ j] $$.  
A simple inspection of the nested loop structure of MATRIX-CHAIN-ORDER yields a running time of $$ O(n^3) $$ for the algorithm.
The loops are nested three deep, and each loop index takes on at most $$ n - 1 $$ values.
The algorithm requires $$ \Theta(n^2) $$ space to store the $$ m $$ and $$ s $$ tables.
Thus, MATRIX-CHAIN-ORDER is much more efficient than the exponential-time method of enumerating all possible parenthesizations and checking each one.

### Step 4: Constructing an optimal solution

The table $$ s[i..n - 1, \ 2..n] $$ gives us the information we need to show how to multiply the matrices.
Each entry $$ s[i, \ j] $$ records a value of $$ k $$ such that an optimal parenthesization of $$ A_i A_{i + 1} \cdots A_j $$ splits the product between $$ A_k $$ and $$ A_{k + 1} $$.
Thus, we know that the final matrix multiplication in computing $$ A_{1..n} $$ optimally is $$ A_{1..s[1, \ n]} A_{s[1, \ n] + 1..n} $$.
We can determine the earlier matrix multiplications recursively, since $$ s[1, \ s[1, \ n]] $$ determines the last matrix multiplication when computing $$ A_{1..s[1, \ n]} $$ and $$ s[s[1, \ n] + 1, \ n] $$ determines the last matrix multiplication when computing $$ A_{s[1, \ n] + 1..n} $$.

>PRINT-OPTIMAL-PARENS($$ s, \ i, \ j $$)  
>01&nbsp; if $$ i == j $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print "$$ A $$"$$ _i $$  
>03&nbsp; else print "("  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;PRINT-OPTIMAL-PARENS($$ s, \ i, \ s[i, \ j] $$)  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;PRINT-OPTIMAL-PARENS($$ s, \ s[i, \ j] + 1, \ j $$)  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print ")"

This recursive procedure prints an optimal parenthesization of $$ \langle A_i, \ A_{i + 1}, \dots, \ A_j \rangle $$, given the $$ s $$ table computed by MATRIX-CHAIN-ORDER and the indices $$ i $$ and $$ j $$.
In the example of Figure 15.3, the call PRINT-OPTIMAL-PARENS($$ s, \ 1, \ 6 $$) prints the parenthesization $$ ((A_1 (A_2 A_3)) ((A_4 A_5) A_6)) $$.

## 15.3 Elements of dynamic programming

### Optimal substructure

The first step in solving an optimization problem by dynamic programming is to characterize the structure of an optimal solution.
A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems.
You will find yourself following a common pattern in discovering optimal substructure:

1. You show that a solution to the problem consists of making a choice, and making this choice leaves one or more subproblems to be solved.
2. You suppose that for a given problem, you are given the choice that leads to an optimal solution.
You do not concern yourself yet with how to determine this choice.
3. Given this choice, you determine which subproblems ensue and how to best characterize the resulting space of subproblems.
4. You show that the solutions to the subproblems used within an optimal solution to the problem must themselves be optimal by using a cut-and-paste technique.
You do so by supposing that each of the subproblem solutions is not optimal and then deriving a contradiction.

Optimal substructure varies across problem domains in two ways:

1. how many subproblems an optimal solution to the original problem uses, and
2. how many choices we have in determining which subproblem(s) to use in an optimal solution.

In the rod-cutting problem, an optimal solution for cutting up a rod of size $$ n $$ uses just one subproblem (of size $$ n - i $$), but we must consider $$ n $$ choices for $$ i $$ in order to determine which one yields an optimal solution.
Matrix-chain multiplication for the subchain $$ A_i A_{i + 1} \cdots A_j $$ serves as an example with two subproblems and $$ j - i $$ choices.

Informally, the running time of a dynamic-programming algorithm depends on the product of two factors: the number of subproblems overall and how many choices we look at for each subproblem. In rod cutting, we had $$ \Theta(n) $$ subproblems overall, and at most $$ n $$ choices to examine for each, yielding an $$ O(n^2) $$ running time.
Matrix-chain multiplication had $$ \Theta(n^2) $$ subproblems overall, and in each we had at most $$ n - 1 $$ choices, giving an $$ O(n^3) $$ running time.  
The subproblem graph gives us an alternative way to perform the same analysis.
Each vertex corresponds to a subproblem, and the choices for a subproblem are the edges incident to that subproblem.
In rod cutting, the subproblem graph had $$ n $$ vertices and at most $$ n $$ edges per vertex, yielding an $$ O(n^2) $$ running time.
For matrix-chain multiplication, it would have $$ \Theta(n^2) $$ vertices and each vertex would have degree at most $$ n - 1 $$, giving a total of $$ O(n^3) $$ vertices and edges.

### Subtleties

You should be careful not to assume that optimal substructure applies when it does not.
Consider the following two problems in which we are given a directed graph $$ G = (V, \ E) $$ and vertices $$ u, \ v \in V $$.

**Unweighted shortest path:** Find a path from $$ u $$ to $$ v $$ consisting of the fewest edges.
Such a path must be simple, since removing a cycle from a path produces a path with fewer edges.  
**Unweighted longest simple path:** Find a simple path from $$ u $$ to $$ v $$ consisting of the most edges.
We need to include the requirement of simplicity because otherwise we can traverse a cycle as many times as we like to create paths with an arbitrarily large number of edges.

The unweighted shortest-path exhibits optimal substructure as follows.
Suppose that $$ u \neq v $$, so that the problem is nontrivial.
Then, any path from $$ p $$ from $$ u $$ to $$ v $$ must contain an intermediate vertex $$ w $$.
Thus, we can decompose the path $$ u \overset{p}{\rightsquigarrow} v $$ into subpaths $$ u \overset{p_1}{\rightsquigarrow} w \overset{p_2}{\rightsquigarrow} v $$.
Clearly, the number of edges in $$ p $$ equals the number of edges in $$ p_1 $$ plus the number of edges in $$ p_2 $$.
We claim that if $$ p $$ is a shortest path from $$ u $$ to $$ v $$, then $$ p_1 $$ must be a shortest path from $$ u $$ to $$ w $$.
We use a cut-and-paste argument: if there were another path, say $$ p'_1 $$, from $$ u $$ to $$ w $$ with fewer edges than $$ p_1 $$, then we could cut out $$ p_1 $$ and paste in $$ p'_1 $$ to produce a path $$ u \overset{p'_1}{\rightsquigarrow} w \overset{p_2}{\rightsquigarrow} v $$ with fewer edges than $$ p $$, thus contradicting $$ p $$'s optimality.
Symmetrically, $$ p_2 $$ must be a shortest path from $$ w $$ to $$ v $$.
Thus, we can find a shortest path from $$ u $$ to $$ v $$ by considering all intermediate vertices $$ w $$, finding a shortest path from $$ u $$ to $$ w $$ and a shortest path from $$ w $$ to $$ v $$, and choosing an intermediate vertex $$ w $$ that yields the overall shortest path.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.4.png){: width="400" }
_**Figure 15.4** A directed graph showing that the problem of finding a longest simple path in an unweighted directed graph does not have optimal substructure._

However, in the case of finding an unweighted longest simple path, the problem does not display the same optimal substructure.
Figure 15.4 supplies an example.
Consider the path $$ q \rightarrow r \rightarrow t $$, which is a longest simple path from $$ q $$ to $$ t $$.
Is $$ q \rightarrow r $$ a longest simple path from $$ q $$ to $$ r $$?
No, for the path $$ q \rightarrow s \rightarrow t \rightarrow r $$ is a simple path that is longer.  
This example shows that for longest simple paths, not only does the problem lack optimal substructure, but we cannot necessarily assemble a "legal" solution to the problem from solutions to subproblems.
If we combine the longest simple paths $$ q \rightarrow s \rightarrow t \rightarrow r $$ and $$ r \rightarrow q \rightarrow s \rightarrow t $$, we get the path $$ q \rightarrow s \rightarrow t \rightarrow r \rightarrow q \rightarrow s \rightarrow t $$, which is not simple.
In fact, no efficient dynamic-programming algorithm for this problem has ever been found.
This problem is NP-complete, which means that we are unlikely to find a way to solve it in polynomial time.

Although a solution to a problem for both longest and shortest paths uses two subproblems, the subproblems in finding the longest simple path are not independent, whereas for shortest paths they are.
The independence of subproblems means that solving one subproblem does not affect the solution to another subproblem of the same problem.  
For the example of Figure 15.4, we have the problem of finding a longest simple path from $$ q $$ to $$ t $$ with two subproblems: finding longest simple paths from $$ q $$ to $$ r $$ and from $$ r $$ to $$ t $$.
For the first of these subproblems, we choose the path $$ q \rightarrow s \rightarrow t \rightarrow r $$, and so we have also used the vertices $$ s $$ and $$ t $$.
We can no longer use these vertices in the second subproblem, since the combination of the two solutions to subproblem would yield a path that is not simple.  
On the other hand, the subproblems do not share resources when we are finding a shortest path.
We claim that if a vertex $$ w $$ is on a shortest path $$ p $$ from $$ u $$ to $$ v $$, then we can splice together any shortest path $$ u \overset{p_1}{\rightsquigarrow} w $$ and any shortest path $$ w \overset{p_2}{\rightsquigarrow} v $$ to produce a shortest path from $$ u $$ to $$ v $$.
We are assured that, other than $$ w $$, no vertex can appear in both paths $$ p_1 $$ and $$ p_2 $$.
Suppose that some vertex $$ x \neq w $$ appears in both $$ p_1 $$ and $$ p_2 $$, so that we can decompose $$ p_1 $$ as $$ u \overset{p_{ux}}{\rightsquigarrow} x \rightsquigarrow w $$ and $$ p_2 $$ as $$ w \rightsquigarrow x \overset{p_{xv}}{\rightsquigarrow} v $$.
By the optimal substructure of this problem, path $$ p $$ has as many edges as $$ p_1 $$ and $$ p_2 $$ together; let's say that $$ p $$ has $$ e $$ edges.
Now let us construct a path $$ p' = u \overset{p_{ux}}{\rightsquigarrow} x \overset{p_{xv}}{\rightsquigarrow} v $$ from $$ u $$ to $$ v $$.
Because we have excised the paths from $$ x $$ to $$ w $$ and from $$ w $$ to $$ x $$, each of which contains at least one edge, path $$ p' $$ contains at most $$ e - 2 $$ edges, which contradicts the assumption that $$ p $$ is a shortest path.
Thus, we are assured that the subproblems for the shortest-path problem are independent.

### Overlapping subproblems

When a recursive algorithm revisits the same problem repeatedly, we say that the optimization problem has overlapping subproblems.
In contrast, a problem for which a divide-and-conquer approach is suitable usually generates brand-new problems at each step of the recursion.
Dynamic-programming algorithms typically take advantage of overlapping subproblems by solving each subproblem once and then storing the solution in a table where it can be looked up when needed, using constant time per lookup.

To illustrate the overlapping-subproblems property in greater detail, let us reexamine the matrix-chain multiplication problem.
Referring back to Figure 15.3, observe that MATRIX-CHAIN-ORDER repeatedly looks up the solution to subproblems in lower rows when solving subproblems in higher rows.
Consider the following inefficient recursive procedure that determines $$ m[i, \ j] $$, the minimum number of scalar multiplications needed to compute the matrix-chain product $$ A_{i..j} = A_i A_{i + 1} \cdots A_j $$.
The procedure is based directly on the recurrence \eqref{eq:7}.

>RECURSIVE-MATRIX-CHAIN($$ p, \ i, \ j $$)  
>01&nbsp; if $$ i == j $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ 0 $$  
>03&nbsp; $$ m[i, \ j] = \infty $$  
>04&nbsp; for $$ k = i $$ to $$ j - 1 $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = $$ RECURSIVE-MATRIX-CHAIN($$ p, \ i, \ k $$) $$ + $$ RECURSIVE-MATRIX-CHAIN($$ p, \ k + 1, \ j $$) $$ + \ p_{i - 1} p_k p_j $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ q < m[i, \ j] $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = q $$  
>08&nbsp; return $$ m[i, \ j] $$

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.5.png){: width="700" }
_**Figure 15.5** The recursion tree for the computation of RECURSIVE-MATRIX-CHAIN($$ p, \ 1, \ 4 $$). The computations performed in a shaded subtree are replaced by a single table lookup in MEMOIZED-MATRIX-CHAIN._

Figure 15.5 shows the recursion tree produced by the call RECURSIVE-MATRIX-CHAIN($$ p, \ 1, \ 4 $$).
Each node is labeled by the values of the parameters $$ i $$ and $$ j $$.
Observe that some pairs of values occur many times.  
We can show that the time to compute $$ m[1, \ n] $$ by this recursive procedure is at least exponential in $$ n $$.
Let T(n) denote the time taken by RECURSIVE-MATRIX-CHAIN to compute an optimal parenthesization of a chain of $$ n $$ matrices.
Because the execution of lines 1–2 and of lines 6–7 each take at least unit time, as does the multiplication in line 5, inspection of the procedure yields the recurrence

$$
\begin{align*}
    T(1) &\ge 1 \\
    T(n) &\ge 1 + \sum_{k = 1}^{n - 1} (T(k) + T(n - k) + 1) & \text{for } n > 1
\end{align*}
$$

Noting that for $$ i = 1, \ 2, \dots, \ n - 1 $$, each term $$ T(i) $$ appears once as $$ T(k) $$ and once as $$ T(n - k) $$, we can rewrite the recurrence as

$$
\begin{align*}
    T(n) \ge 2 \sum_{i = 1}^{n - 1} T(i) + n
\end{align*}
\label{eq:8}
\tag{15.8}
$$

We shall prove that $$ T(n) = \Omega(2^n) $$ using the substitution method.
Specifically, we shall show that $$ T(n) \ge 2^{n - 1} $$ for all $$ n \ge 1 $$.
The basis is $$ T(1) \ge 1 = 2^0 $$.
Inductively, for $$ n \ge 2 $$ we have

$$
\begin{align*}
    T(n) &\ge 2 \sum_{i = 1}^{n - 1} 2^{i - 1} + n \\
         &= 2 \sum_{i = 0}^{n - 2} 2^i + n \\
         &= 2(2^{n - 1} - 1) + n \\
         &= 2^n - 2 + n \\
         &\ge 2^{n - 1}
\end{align*}
$$

which completes the proof.
Thus, the total amount of work performed by the call RECURSIVE-MATRIX-CHAIN($$ p, \ 1, \ n $$) is at least exponential in $$ n $$.

Compare this top-down, recursive algorithm without memoization with the bottom-up dynamic-programming algorithm.
The latter is more efficient because it takes advantage of the overlapping-subproblems property.
Matrix-chain multiplication has only $$ \Theta(n^2) $$ distinct subproblems, and the dynamic-programming algorithm solves each exactly once.
The recursive algorithm, on the other hand, must again solve each subproblem every time it reappears in the recursion tree.

### Reconstructing an optimal solution

### Memoization

A memoized recursive algorithm maintains an entry in a table for the solution to each subproblem.
Each table entry initially contains a special value to indicate that the entry has yet to be filled in.
When the subproblem is first encountered as the recursive algorithm unfolds, its solution is computed and then stored in the table.
Each subsequent time that we encounter this subproblem, we simply look up the value stored in the table and return it.

>MEMOIZED-MATRIX-CHAIN($$ p $$)  
>01&nbsp; $$ n = p.length - 1 $$  
>02&nbsp; let $$ m[1..n, \ 1..n] $$ be a new table  
>03&nbsp; for $$ i = 1 $$ to $$ n $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ j = i $$ to $$ n $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = \infty $$  
>06&nbsp; return LOOKUP-CHAIN($$ m, \ p, \ 1, \ n $$)

>LOOKUP-CHAIN($$ m, \ p, \ i, \ j $$)  
>01&nbsp; if $$ m[i, \ j] < \infty $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return $$ m[i, \ j] $$  
>03&nbsp; if $$ i == j $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = 0 $$  
>05&nbsp; else for $$ k = i $$ to $$ j - 1 $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ q = $$ LOOKUP-CHAIN($$ m, \ p, \ i, \ k $$) + LOOKUP-CHAIN($$ m, \ p, \ k + 1, \ j $$) $$ + \ p_{i - 1} p_k p_j $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;if $$ q < m[i, \ j] $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ m[i, \ j] = q $$  
>09&nbsp; return $$ m[i, \ j] $$

The MEMOIZED-MATRIX-CHAIN procedure, like MATRIX-CHAIN-ORDER, maintains a table $$ m[1..n, \ 1..n] $$ of computed values of $$ m[i, \ j] $$, the minimum number of scalar multiplications needed to compute the matrix $$ A_{i..j} $$.
Upon calling LOOKUP-CHAIN($$ m, \ p, \ i, \ j $$), if line 1 finds that $$ m[i, \ j] < \infty $$, then the procedure simply returns the previously computed cost $$ m[i, \ j] $$ in line 2.
Otherwise, the cost is computed as in RECURSIVE-MATRIX-CHAIN, stored in $$ m[i, \ j] $$, and returned.  
Figure 15.5 illustrates how MEMOIZED-MATRIX-CHAIN saves time compared with RECURSIVE-MATIRX-CHAIN.
Shaded subtrees represent values that it looks up rather than recomputes.
Like the bottom-up dynamic-programming algorithm MATRIX-CHAIN-ORDER, the procedure MEMOIZED-MATRIX-CHAIN runs in $$ O(n^3) $$ tine.
Line 5 of MEMOIZED-MATRIX-CHAIN executes $$ \Theta(n^2) $$ times.
We can categorize the calls of LOOKUP-CHAIN into two types:

1. calls in which $$ m[i, \ j] = \infty $$, so that lines 3–9 execute, and
2. calls in which $$ m[i, \ j] < \infty $$, so that LOOKUP-CHAIN simply returns in line 2.

There are $$ \Theta(n^2) $$ calls of the first type, one per table entry.
All calls of the second type are made as recursive calls by calls of the first type.
Whenever a given call of LOOKUP-CHAIN makes recursive calls, it makes $$ O(n) $$ of them.
Therefore, there are $$ O(n^3) $$ calls of the second type in all.
Each call of the second type takes $$ O(1) $$ time, and each call of the first type takes $$ O(n) $$ time plus the time spent in its recursive calls.
The total time, therefore, is $$ O(n^3) $$.  
In summary, we can solve the matrix-chain multiplication problem by either a top-down, memoized dynamic-programming algorithm or a bottom-up dynamic-programming algorithm in $$ O(n^3) $$ time.
Both methods take advantage of the overlapping-subproblems property.
There are only $$ \Theta(n^2) $$ distinct subproblems in total, and either of these methods computes the solution to each subproblem only once.

In general practice, if all subproblems must be solved at least once, a bottom-up dynamic-programming algorithm usually outperforms the corresponding top-down memoized algorithm by a constant factor, because the bottom-up algorithm has no overhead for recursion and less overhead for maintaining the table.
Moreover, for some problems we can exploit the regular pattern of table accesses in the dynamic-programming algorithm to reduce time or space requirements even further.
Alternatively, if some subproblems in the subproblem space need not be solved at all, the memoized solution has the advantage of solving only those subproblems that are definitely required.

## 15.4 Longest common subsequence

A subsequence of a given sequence is the given sequence with zero or more elements left out.
Formally, given a sequence $$ X = \langle x_1, \ x_2, \dots, \ x_m \rangle $$, another sequence $$ Z = \langle z_1, \ z_2, \dots, \ z_k \rangle $$ is a subsequence of $$ X $$ if there exists a strictly increasing sequence $$ \langle i_1, \ i_2, \dots, \ i_k \rangle $$ of indices of $$ X $$ such that for all $$ j = 1, \ 2, \dots, \ k $$, we have $$ x_{i_j} = z_j $$. Given two sequences $$ X $$ and $$ Y $$, we say that a sequence $$ Z $$ is a common subsequence of $$ X $$ and $$ Y $$ if $$ Z $$ is a subsequence of both $$ X $$ and $$ Y $$.
In the longest-common-subsequence problem, we are given two sequences $$ X = \langle x_1, \ x_2, \dots, \ x_m \rangle $$ and $$ Y = \langle y_1, \ y_2, \dots, \ y_n \rangle $$ and wish to find a maximum-length common subsequence of $$ X $$ and $$ Y $$.

### Step 1: Characterizing a longest common subsequence

In a brute-force approach to solving the LCS problem, we would enumerate all subsequences of $$ X $$ and check each subsequence to see whether it is also a subsequence of $$ Y $$, keeping track of the longest subsequence we find.
Each subsequence of $$ X $$ corresponds to a subset of the indices $$ \{ 1, \ 2, \dots, \ m \} $$ of $$ X $$.
Because $$ X $$ has $$ 2^m $$ subsequences, this approach requires exponential time.  
The LCS problem has an optimal-substructure property, however, as the following theorem shows.
Given a sequence $$ X = \langle x_1, \ x_2, \dots, \ x_m \rangle $$, we define the $$ i $$th prefix of $$ X $$, for $$ i = 0, \ 1, \dots, \ m $$, as $$ X_i = \langle x_1, \ x_2, \dots, \ x_i \rangle $$.

### Theorem 15.1 (Optimal substructure of an LCS)

Let $$ X = \langle x_1, \ x_2, \dots, \ x_m \rangle $$ and $$ Y = \langle y_1, \ y_2, \dots, \ y_n \rangle $$ be sequences, and let $$ Z = \langle z_1, \ z_2, \dots, \ z_k \rangle $$ be any LCS of $$ X $$ and $$ Y $$.

1. If $$ x_m = y_n $$, then $$ z_k = x_m = y_n $$ and $$ Z_{k - 1} $$ is an LCS of $$ X_{m - 1} $$ and $$ Y_{n - 1} $$.
2. If $$ x_m \neq y_n $$, then $$ z_k \neq x_m $$ implies that $$ Z $$ is an LCS of $$ X_{m - 1} $$ and $$ Y $$.
3. If $$ x_m \neq y_n $$, then $$ z_k \neq y_n $$ implies that $$ Z $$ is an LCS of $$ X $$ and $$ Y_{n - 1} $$.

**Proof**  
(1) If $$ z_k \neq x_m $$, then we could append $$ x_m = y_n $$ to $$ Z $$ to obtain a common subsequence of $$ X $$ and $$ Y $$ of length $$ k + 1 $$, contradicting the supposition that $$ Z $$ is a longest common subsequence of $$ X $$ and $$ Y $$.
Thus, we must have $$ z_k = x_m = y_n $$.
Now, the prefix $$ Z_{k - 1} $$ is a length-($$ k - 1 $$) common subsequence of $$ X_{m - 1} $$ and $$ Y_{n - 1} $$.
We wish to show that it is an LCS.
Suppose that there exists a common subsequence $$ W $$ of $$ X_{m - 1} $$ and $$ Y_{n - 1} $$ with length greater than $$ k - 1 $$.
Then, appending $$ x_m = y_n $$ to $$ W $$ produces a common subsequence of $$ X $$ and $$ Y $$ whose length is greater than $$ k $$, which is a contradiction.  
(2) If $$ z_k \neq x_m $$, then $$ Z $$ is a common subsequence of $$ X_{m - 1} $$ and $$ Y $$.
If there were a common subsequence $$ W $$ of $$ X_{m - 1} $$ and $$ Y $$ with length greater than $$ k $$, then $$ W $$ would also be a common subsequence of $$ X_m $$ and $$ Y $$, contradicting the assumption that $$ Z $$ is an LCS of $$ X $$ and $$ Y $$.  
(3) The proof is symmetric to (2). $$ \blacksquare $$

The way that Theorem 15.1 characterizes longest common subsequences tells us that an LCS of two sequences contains within it an LCS of prefixes of the two sequences.
Thus, the LCS problem has an optimal-substructure property.

### Step 2: A recursive solution

We can readily see the overlapping-subproblems property in the LCS problem.
To find an LCS of $$ X $$ and $$ Y $$, we may need to find the LCSs of $$ X $$ and $$ Y_{n - 1} $$ and of $$ X_{m - 1} $$ and $$ Y $$.
But each of these subproblems has the subsubproblems of finding an LCS of $$ X_{m - 1} $$ and $$ Y_{n - 1} $$.
Many other subproblems share subsubproblems.  
The recursive solution to the LCS problem involves establishing a recurrence for the value of an optimal solution.
Let us define $$ c[i, \ j] $$ to be the length of an LCS of the sequences $$ X_i $$ and $$ Y_j $$.
The optimal substructure of the LCS problem gives the recursive formula

$$
\begin{align*}
    c[i, \ j] =
    \begin{cases}
        0 & \text{if } i = 0 \text{ or } j = 0 \\
        c[i - 1, \ j - 1] + 1 & \text{if } i, \ j > 0 \text{ and } x_i = y_j \\
        \max(c[i, \ j - 1], \ c[i - 1, \ j]) & \text{if } i, \ j > 0 \text{ and } x_i \neq y_j
    \end{cases}
\end{align*}
\label{eq:9}
\tag{15.9}
$$

When $$ x_i = y_j $$, we should consider the subproblem of finding an LCS of $$ X_{i - 1} $$ and $$ Y_{j - 1} $$.
Otherwise, we instead consider the two subproblems of finding an LCS of $$ X_i $$ and $$ Y_{j - 1} $$ and of $$ X_{i - 1} $$ and $$ Y_j $$.

### Step 3: Computing the length of an LCS

>LCS-LENGTH($$ X, \ Y $$)  
>01&nbsp; $$ m = X.length $$  
>02&nbsp; $$ n = Y.length $$  
>03&nbsp; let $$ b[1..m, \ 1..n] $$ and $$ c[0..m, \ 0..n] $$ be new tables  
>04&nbsp; for $$ i = 1 $$ to $$ m $$  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ c[i, \ 0] = 0 $$  
>06&nbsp; for $$ j = 0 $$ to $$ n $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ c[0, \ j] = 0 $$  
>08&nbsp; for $$ i = 1 $$ to $$ m $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ j = 1 $$ to $$ n $$  
>10&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ x_i == y_j $$  
>11&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c[i, \ j] = c[i - 1, \ j - 1] + 1 $$  
>12&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ b[i, \ j] = $$ "$$ \nwarrow $$"  
>13&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elseif $$ c[i - 1, \ j] \ge c[i, \ j - 1] $$  
>14&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ c[i, \ j] = c[i - 1, \ j] $$  
>15&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ b[i, \ j] = $$ "$$ \uparrow $$"  
>16&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else $$ c[i, \ j] = c[i, \ j - 1] $$  
>17&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ b[i, \ j] = $$ "$$ \leftarrow $$"  
>18&nbsp; return $$ c $$ and $$ b $$

Procedure LCS-LENGTH stores the $$ c[i, \ j] $$ values in a table $$ c[0..m, \ 0..n] $$, and also maintains the table $$ b[i..m, \ 1..n] $$ to help us construct an optimal solution.
$$ b[i, \ j] $$ points to the table entry corresponding to the optimal subproblem solution chosen when computing $$ c[i, \ j] $$.
The procedure returns the $$ b $$ and $$ c $$ tables; $$ c[m, \ n] $$ contains the length of an LCS of $$ X $$ and $$ Y $$.
The running time of the procedure is $$ \Theta(mn) $$, since each table entry takes $$ \Theta(1) $$ time to compute.

### Step 4: Constructing an LCS

The $$ b $$ table enables us to quickly construct an LCS of $$ X $$ and $$ Y $$.
We simply begin at $$ b[m, \ n] $$ and trace through the table by following the arrows.
Whenever we encounter a "$$ \nwarrow $$" in entry $$ b[i, \ j] $$, it implies that $$ x_i = y_j $$ is an element of the LCS that LCS-LENGTH found.
With this method, we encounter the elements of this LCS in reverse order.

>PRINT-LCS($$ b, \ X, \ i, \ j $$)  
>01&nbsp; if $$ i == 0 $$ or $$ j == 0 $$  
>02&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;return  
>03&nbsp; if $$ b[i, \ j] == $$ "$$ \nwarrow $$"  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;PRINT-LCS($$ b, \ X, \ i - 1, \ j - 1 $$)  
>05&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;print $$ x_i $$  
>06&nbsp; elseif $$ b[i, \ j] == $$ "$$ \uparrow $$"  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;PRINT-LCS($$ b, \ X, \ i - 1, \ j $$)  
>08&nbsp; else PRINT-LCS($$ b, \ X, \ i, \ j - 1 $$)

The procedure takes time $$ O(m + n) $$, since it decrements at least one of $$ i $$ and $$ j $$ in each recursive call.

## 15.5 Optimal binary search trees

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.6.png){: width="700" }
_**Figure 15.6** The binary search tree for a set of $$ n = 5 $$ keys._

We are given a sequence $$ K = \langle k_1, \ k_2, \dots, \ k_n \rangle $$ of $$ n $$ distinct keys in sorted order ($$ k_1 < k_2 < \cdots < k_n $$), and we wish to build a binary search tree from these keys.
For each key $$ k_i $$, we have a probability $$ p_i $$ that a search will be for $$ k_i $$.
Some searches may be for values not in $$ K $$, and so we also have $$ n + 1 $$ dummy keys $$ d_0, \ d_1, d_2, \dots, \ d_n $$ representing values not in $$ K $$.
In particular, $$ d_0 $$ represents all values less than $$ k_1 $$, $$ d_n $$ represents all values greater than $$ k_n $$, and for $$ i = 1, \ 2, \dots, \ n - 1 $$, the dummy key $$ d_i $$ represents all values between $$ k_i $$ and $$ k_{i + 1} $$.
For each dummy key $$ d_i $$, we have a probability $$ q_i $$ that a search will correspond to $$ d_i $$.
Every search is either successful (finding $$ k_i $$) or unsuccessful (finding $$ d_i $$), and so we have

$$
\begin{align*}
    \sum_{i = 1}^n p_i + \sum_{i = 0}^n q_i = 1
\end{align*}
\label{eq:10}
\tag{15.10}
$$

Because we have probabilities of searches for each key and each dummy key, we can determine the expected cost of a search in a given binary search tree $$ T $$.
Let us assume that the actual cost of a search equals the number of nodes examined.
Then the expected cost of a search in $$ T $$ is

$$
\begin{align*}
    E[\text{search cost in } T] &= \sum_{i = 1}^n (\text{depth}_T (k_i) + 1) \cdot p_i + \sum_{i = 0}^n (\text{depth}_T (d_i) + 1) \cdot q_i \\
    &= 1 + \sum_{i = 1}^n \text{depth}_T (k_i) \cdot p_i + \sum_{i = 0}^n \text{depth}_T (d_i) \cdot q_i
\end{align*}
\label{eq:11}
\tag{15.11}
$$

where depth$$ _T $$ denotes a node's depth in the tree $$ T $$.

For a given set of probabilities, we wish to construct a binary search tree whose expected search cost is smallest.
We call such a tree an optimal binary search tree.
An optimal binary search tree is not necessarily a tree whose overall height is smallest.
Nor can we necessarily construct an optimal binary search tree by always putting the key with the greatest probability at the root.
As with matrix-chain multiplication, exhaustive checking of all possibilities fails to yield an efficient algorithm.
The number of binary trees with $$ n $$ nodes is $$ \Omega(4^n / n^{3 / 2}) $$, and so we would have to examine an exponential number of binary search trees in an exhaustive search.
Not surprisingly, we shall solve this problem with dynamic programming.

### Step 1: The structure of an optimal binary search tree

Consider any subtree of a binary search tree.
It must contain keys in a contiguous range $$ k_i, \dots, k_j $$, for some $$ 1 \le i \le j \le n $$.
In addition, a subtree that contains keys $$ k_i, \dots, k_j $$ must also have as its leaves the dummy keys $$ d_{i - 1}, \dots, d_j $$.  
If an optimal binary search tree $$ T $$ has a subtree $$ T' $$ containing keys $$ k_i, \dots, k_j $$, then this subtree $$ T' $$ must be optimal as well for the subproblem with keys $$ k_i, \dots, k_j $$ and dummy keys $$ d_{i - 1}, \dots, d_j $$.
If there were a subtree $$ T'' $$ whose expected cost is lower than that of $$ T' $$, then we could cut $$ T' $$ out of $$ T $$ and paste in $$ T'' $$, resulting in a binary search tree of lower expected cost than $$ T $$, thus contradicting the optimality of $$ T $$.

We need to use the optimal substructure to show that we can construct an optimal solution to the problem from optimal solutions to subproblems.
Given keys $$ k_i, \dots, k_j $$, one of these keys, say $$ k_r $$ ($$ i \le r \le j $$), is the root of an optimal subtree containing these keys.
The left subtree of the root $$ k_r $$ contains the keys $$ k_i, \dots, k_{r - 1} $$ (and dummy keys $$ d_{i - 1}, \dots, d_{r - 1} $$), and the right subtree contains the keys $$ k_{r + 1}, \dots, k_j $$ (and dummy keys $$ d_r \dots, \ d_j $$).
As long as we examine all candidate roots $$ k_r $$, where $$ i \le r \le j $$, and we determine all optimal binary search trees containing $$ k_i, \dots, k_{r - 1} $$ and those containing $$ k_{r + 1}, \dots, k_j $$, we are guaranteed that we will find an optimal binary search tree.

There is one detail worth noting about empty subtrees.
Suppose that in a subtree with keys $$ k_i, \dots, k_j $$, we select $$ k_i $$ as the root.
By the above argument, $$ k_i $$'s left subtree contains the keys $$ k_i, \dots, k_{i - 1} $$.
We interpret this sequence as containing no keys.
However, subtrees also contain dummy keys.
We adopt the convention that a subtree containing keys $$ k_i, \dots, k_{i - 1} $$ has no actual keys but does contain the single dummy key $$ d_{i - 1} $$.
Symmetrically, if we select $$ k_j $$ as the root, then the $$ k_j $$'s right subtree contains no actual keys, but it does contain the dummy key $$ d_j $$.

### Step 2: A recursive solution

We pick our subproblem domain as finding an optimal binary search tree containing the keys $$ k_i, \dots, k_j $$, where $$ i \ge 1 $$, $$ j \le n $$, and $$ j \ge i - 1 $$.
Let us define $$ e[i, \ j] $$ as the expected cost of searching an optimal binary search tree containing the keys $$ k_i, \dots, k_j $$.
Ultimately, we wish to compute $$ e[1, \ n] $$.  
When $$ j = i - 1 $$, then we have just the dummy key $$ d_{i - 1} $$.
The expected search cost is $$ e[i, \ i - 1] = q_{i - 1} $$.
When $$ j \ge i $$, we need to select a root $$ k_r $$ from among $$ k_i, \dots, k_j $$ and then make an optimal binary search tree with keys $$ k_i, \dots, k_{r - 1} $$ as its left subtree and an optimal binary search tree with keys $$ k_{r + 1}, \dots, k_j $$ as its right subtree.
By equation \eqref{eq:11}, the expected search cost of this subtree increases by the sum of all the probabilities in the subtree.
For a subtree with keys $$ k_i, \dots, k_j $$, let us denote this sum of probabilities as

$$
\begin{align*}
    w(i, \ j) = \sum_{l = i}^j p_l + \sum_{l = i - 1}^j q_l
\end{align*}
\label{eq:12}
\tag{15.12}
$$

Thus, if $$ k_r $$ is the root of an optimal subtree containing keys $$ k_i, \dots, k_j $$, we have

$$
\begin{align*}
    e[i, \ j] = p_r + (e[i, \ r - 1] + w(i, \ r - 1)) + (e[r + 1, \ j] + w(r + 1, \ j))
\end{align*}
$$

Noting that

$$
\begin{align*}
    w(i, \ j) = w(i, \ r - 1) + p_r + w(r + 1, \ j)
\end{align*}
$$

we rewrite $$ e[i, \ j] $$ as

$$
\begin{align*}
    e[i, \ j] = e[i, \ r - 1] + e[r + 1, \ j] + w(i, \ j)
\end{align*}
\label{eq:13}
\tag{15.13}
$$

The recursive equation \eqref{eq:13} assumes that we know which node $$ k_r $$ to use as the root.
We choose the root that gives the lowest expected search cost, giving us the final recursive formulation:

$$
\begin{align*}
    e[i, \ j] =
    \begin{cases}
        q_{i - 1} & \text{if } j = i - 1 \\
        \displaystyle \min_{i \le r \le j} \{ e[i, \ r - 1] + e[r + 1, \ j] + w(i, \ j) \} & \text{if } i \le j
    \end{cases}
\end{align*}
\label{eq:14}
\tag{15.14}
$$

### Step 3: Computing the expected search cost of an optimal binary search tree

A direct, recursive implementation of equation \eqref{eq:14} would be inefficient.
Instead, we store the $$ e[i, \ j] $$ values in a table $$ e[1..n + 1, \ 0..n] $$.
The first index needs to run to $$ n + 1 $$ rather than $$ n $$ because in order to have a subtree containing only the dummy key $$ d_n $$, we need to compute and store $$ e[n + 1, \ n] $$.
The second index needs to start from $$ 0 $$ because in order to have a subtree containing only the dummy key $$ d_0 $$ we need to compute and store $$ e[1, \ 0] $$.
We also use a table $$ root[i, \ j] $$, for recording the root of the subtree containing keys $$ k_i, \dots, k_j $$.

We will need one other table for efficiency.
Rather than compute the value of $$ w(i, \ j) $$ from scratch every time we are computing $$ e[i, \ j] $$—we store these values in a table $$ w[1..n + 1, \ 0..n] $$.
For the base case, we compute $$ w[i, \ i - 1] = q_{i - 1} $$ for $$ 1 \le i \le n + 1 $$.
For $$ j \ge i $$, we compute

$$
\begin{align*}
    w[i, \ j] = w[i, \ j - 1] + p_j + q_j
\end{align*}
\label{eq:15}
\tag{15.15}
$$

Thus, we can compute the $$ \Theta(n^2) $$ values of $$ w[i, \ j] $$ in $$ \Theta(1) $$ time each.

>OPTIMAL-BST($$ p, \ q, \ n $$)  
>01&nbsp; let $$ e[1..n + 1, \ 0..n] $$, $$ w[1..n + 1, \ 0..n] $$, and $$ root[1..n, \ 1..n] $$ be new tables  
>02&nbsp; for $$ i = 1 $$ to $$ n + 1 $$  
>03&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ e[i, \ i - 1] = q_{i - 1} $$  
>04&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;$$ w[i, \ i - 1] = q_{i - 1} $$  
>05&nbsp; for $$ l = 1 $$ to $$ n $$  
>06&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;for $$ i = 1 $$ to $$ n - l + 1 $$  
>07&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ j = i + l - 1 $$  
>08&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ e[i, \ j] = \infty $$  
>09&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ w[i, \ j] = w[i, \ j - 1] + p_j + q_j $$  
>10&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $$ r = i $$ to $$ j $$  
>11&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ t = e[i, \ r - 1] + e[r + 1, \ j] + w[i, \ j] $$  
>12&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $$ t < e[i, \ j] $$  
>13&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ e[i, \ j] = t $$  
>14&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$$ root[i, \ j] = r $$  
>15&nbsp; return $$ e $$ and $$ root $$

The procedure works similarly to the MATRIX-CHAIN-ORDER procedure.
The for loop of lines 2–4 initializes the values of $$ e[i, \ i - 1] $$ and $$ w[i, \ i - 1] $$.
The for loop of lines 5–14 then uses the recurrences \eqref{eq:14} and \eqref{eq:15} to compute $$ e[i, \ j] $$ and $$ w[i, \ j] $$ for all $$ 1 \le i \le j \le n $$.
In the first iteration, when $$ l = 1 $$, the loop computes $$ e[i, \ i] $$ and $$ w[i, \ i] $$ for $$ i = 1, \ 2, \dots, n $$.
The second iteration, with $$ l = 2 $$, computes $$ e[i, \ i + 1] $$ and $$ w[i, \ i + 1] $$ for $$ i = 1, \ 2, \dots, n - 1 $$, and so forth.
The innermost for loop, in lines 10–14, tries each candidate index $$ r $$ to determine which key $$ k_r $$ to use as the root of an optimal binary search tree containing keys $$ k_i, \dots, k_j $$.
This for loop saves the current value of the index $$ r $$ in $$ root[i, \ j] $$ whenever it finds a better key to use as the root.

![Desktop View](/assets/img/15-Dynamic-Programming/Figure 15.7.png){: width="700" }
_**Figure 15.7** The tables $$ e[i, \ j] $$, $$ w[i, \ j] $$, and $$ root[i, \ j] $$ computed by OPTIMAL-BST on the key distribution shown in Figure 15.6._

The OPTIMAL-BST procedure takes $$ \Theta(n^3) $$ time, just like MATRIX-CHAIN-ORDER.
We can easily see that its running time is $$ O(n^3) $$, since its for loops are nested three deep and each loop index takes on at most $$ n $$ values.
The loop indices in OPTIMAL-BST do not have exactly the same bounds as those in MATRIX-CHAIN-ORDER, but they are within at most 1 in all directions.
Thus, like MATRIX-CHAIN-ORDER, the OPTIMAL-BST procedure takes $$ \Omega(n^3) $$ time.

---

Sources:
- [Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms (3rd ed.). MIT Press.](https://a.co/d/62TQVO9)