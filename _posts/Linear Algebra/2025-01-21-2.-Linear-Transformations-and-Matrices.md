---
title: 2. Linear Transformations and Matrices
# description: Short summary of the post
date: 2025-01-02 13:20
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, linear-transformation]     # TAG names should always be lowercase
math: true
pin: false
---

## 2.1 Linear transformations, null spaces, and ranges

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over the same field $$ F $$.
We call a function $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ a linear transformation from $$ \mathsf{V} $$ to $$ \mathsf{W} $$ if, for all $$ x, \ y \in \mathsf{V} $$ and $$ c \in F $$, we have  
(a) $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) $$ and  
(b) $$ \mathsf{T}(cx) = c \mathsf{T}(x) $$.

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
We define the null space (or kernel) $$ \mathsf{N}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the set of all vectors $$ x $$ in $$ \mathsf{V} $$ such that $$\mathsf{T}(x) = \mathit{0} $$; that is, $$ \mathsf{N}(\mathsf{T}) = \{ x \in \mathsf{V} : \mathsf{T}(x) = \mathit{0} \} $$.
We define the range (or image) $$ \mathsf{R}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the subset of $$ \mathsf{W} $$ consisting of all images (under $$ \mathsf{T} $$) of vectors in $$ \mathsf{V} $$; that is, $$ \mathsf{R}(\mathsf{T}) = \{ \mathsf{T}(x) : x \in \mathsf{V} \} $$.

### Theorem 2.1

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are subspaces of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.

**Proof**  
We use the symbols $$ \mathit{0}_{\mathsf{V}} $$ and $$ \mathit{0}_{\mathsf{W}} $$ to denote the zero vectors of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
Since $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_\mathsf{W} $$, we have that $$ \mathit{0}_{\mathsf{V}} \in \mathsf{N}(\mathsf{T}) $$.
Let $$ x, \ y \in \mathsf{N}(\mathsf{T}) $$ and $$ c \in F $$.
Then $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) = \mathit{0}_{\mathsf{W}} + \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$, and $$ \mathsf{T}(cx) = c \mathsf{T}(x) = c \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$.
Hence $$ x + y \in \mathsf{N}(\mathsf{T}) $$ and $$ cx \in \mathsf{N}(\mathsf{T}) $$, so that $$ \mathsf{N}(\mathsf{T}) $$ is a subspace of $$ \mathsf{V} $$.  
Because $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_{\mathsf{W}} $$, we have that $$ \mathit{0}_{\mathsf{W}} \in \mathsf{R}(\mathsf{T}) $$.
Now let $$ x, \ y \in \mathsf{R}(\mathsf{T}) $$ and $$ c \in F $$.
Then there exists $$ v $$ and $$ w $$ in $$ \mathsf{V} $$ such that $$ \mathsf{T}(v) = x $$ and $$ \mathsf{T}(w) = y $$.
So $$ \mathsf{T}(v + w) = \mathsf{T}(v) + \mathsf{T}(w) = x + y $$, and $$ \mathsf{T}(cv) = c \mathsf{T}(v) = cx $$.
Thus $$ x + y \in \mathsf{R}(\mathsf{T}) $$ and $$ cx \in \mathsf{R}(\mathsf{T}) $$, so $$ \mathsf{R}(\mathsf{T}) $$ is a subspace of $$ \mathsf{W} $$. $$ \blacksquare $$

### Theorem 2.2

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$, then

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) = \text{span}(\mathsf{T}(\beta)) = \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \})
\end{align*}
$$

**Proof**  
Clearly $$ \mathsf{T}(v_i) \in \mathsf{R}(\mathsf{T}) $$.
Then $$ w = \mathsf{T}(v) $$ for some $$ v \in \mathsf{V} $$.
Because $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have

$$
\begin{align*}
    v = \sum_{i = 1}^{n} a_i v_i && \text{for some } a_1, \ a_2, \dots, a_n \in F
\end{align*}
$$

Since $$ \mathsf{T} $$ is linear, it follows that

$$
\begin{align*}
    w = \mathsf{T}(v) = \sum_{i = 1}^{n} a_i \mathsf{T}(v_i) \in \text{span}(\mathsf{T}(\beta))
\end{align*}
$$

so $$ \mathsf{R}(\mathsf{T}) $$ is contained in span($$ \mathsf{T}(\beta) $$). $$ \blacksquare $$

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are finite-dimensional, then we define the nullity of $$ \mathsf{T} $$ and the rank of $$ \mathsf{T} $$ to be the dimensions of $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$, respectively.

### Theorem 2.3 (Dimension Theorem)

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{V} $$ is finite-dimensional, then

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

**Proof**  
Suppose that $$ \dim(\mathsf{V}) = n $$, $$ \dim(\mathsf{N}(\mathsf{T})) = k $$, and $$ \{ v_1, \ v_2, \dots, v_k \} $$ is a basis for $$ \mathsf{N}(\mathsf{T}) $$.
By the corollary to Theorem 1.11, we may extend $$ \{ v_1, \ v_2, \dots, v_k \} $$ to a basis $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ for $$ \mathsf{V} $$.
We claim that $$ S = \{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \} $$ is a basis for $$ \mathsf{R}(\mathsf{T}) $$.  
First we prove that $$ S $$ generates $$ \mathsf{R}(\mathsf{T}) $$.
Using Theorem 2.2 and the fact that $$ \mathsf{T}(v_i) = \mathit{0} $$ for $$ 1 \le i \le k $$, we have

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) &= \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \}) \\
    &= \text{span}(\{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \}) = \text{span}(S)
\end{align*}
$$

Now we prove that $$ S $$ is linearly independent.
Suppose that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i \mathsf{T}(v_i) = \mathit{0} && \text{for } b_{k + 1}, \ b_{k + 2}, \dots, b_n \in F
\end{align*}
$$

Using the fact that $$ \mathsf{T} $$ is linear, we have

$$
\begin{align*}
    \mathsf{T} \left( \sum_{i = k + 1}^{n} b_i v_i \right) = \mathit{0}
\end{align*}
$$

So

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i \in \mathsf{N}(\mathsf{T})
\end{align*}
$$

Hence there exist $$ c_1, \ c_2, \dots, c_k \in F $$ such that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i = \sum_{i = 1}^{k} c_i v_i
\end{align*}
$$

or

$$
\begin{align*}
    \sum_{i = 1}^{k} (-c_i) v_i + \sum_{i = k + 1}^{n} b_i v_i = \mathit{0}
\end{align*}
$$

Since $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have $$ b_i = 0 $$ for all $$ i $$.
Hence $$ S $$ is linearly independent.
This argument also shows that $$ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) $$ are distinct; therefore rank($$ \mathsf{T} $$) $$ = n - k $$. $$ \blacksquare $$

### Theorem 2.4

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.

**Proof**  
Suppose that $$ \mathsf{T} $$ is one-to-one and $$ x \in \mathsf{N}(\mathsf{T}) $$.
Then $$ \mathsf{T}(x) = \mathit{0} = \mathsf{T}(\mathit{0}) $$.
Since $$ \mathsf{T} $$ is one-to-one, we have $$ x = \mathit{0} $$.
Hence $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.  
Now assume that $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, and suppose that $$ \mathsf{T}(x) = \mathsf{T}(y) $$.
Then $$ \mathit{0} = \mathsf{T}(x) - \mathsf{T}(y) = \mathsf{T}(x - y) $$.
Therefore $$ x - y \in \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.
So $$ x - y = \mathit{0} $$, or $$ x = y $$.
This means $$ \mathsf{T} $$ is one-to-one. $$ \blacksquare $$

### Theorem 2.5

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces of equal dimension, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then the following are equivalent.  
(a) $$ \mathsf{T} $$ is one-to-one.  
(b) $$ \mathsf{T} $$ is onto.  
(c) rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{V}) $$

**Proof**  
From the dimension theorem, we have

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

Now, with the use of Theorem 2.4, we have that $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, if and only if nullity($$ \mathsf{T} $$) $$ = 0 $$, if and only if rank($$ \mathsf{T} $$) $$ = \dim(\mathsf{V}) $$, if and only if rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{W}) $$, and if and only if $$ \dim(\mathsf{R}(\mathsf{T})) = \dim(\mathsf{W}) $$.
By Theorem 1.11, this equality is equivalent to $$ \mathsf{R}(\mathsf{T}) = \mathsf{W} $$, the definition of $$ \mathsf{T} $$ being onto. $$ \blacksquare $$

We note that if $$ \mathsf{V} $$ is not finite-dimensional and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{V} $$ is linear, then it does not follow that one-to-one and onto are equivalent.

### Theorem 2.6

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over $$ F $$, and suppose that $$ \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$.
For $$ w_1, \ w_2, \dots, w_n $$ in $$ \mathsf{W} $$, there exists exactly one linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \mathsf{T}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.

**Proof**  
Let $$ x \in \mathsf{V} $$.
Then

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

where $$ a_1, \ a_2, \dots, a_n $$ are unique scalars.
Define $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ by

$$
\begin{align*}
    \mathsf{T}(x) = \sum_{i = 1}^{n} a_i w_i
\end{align*}
$$

(a) $$ \mathsf{T} $$ is linear: Suppose that $$ u, \ v \in \mathsf{V} $$ and $$ d \in F $$.
Then we may write

$$
\begin{align*}
    u = \sum_{i = 1}^{n} b_i v_i
\end{align*}
$$

and

$$
\begin{align*}
    v = \sum_{i = 1}^{n} c_i v_i
\end{align*}
$$

for some scalars $$ b_1, \ b_2, \dots, b_n, \ c_1, \ c_2, \dots, c_n $$.
Thus

$$
\begin{align*}
    du + v = \sum_{i = 1}^{n} (d b_i + c_i) v_i
\end{align*}
$$

So

$$
\begin{align*}
    \mathsf{T}(du + v) = \sum_{i = 1}^{n} (d b_i + c_i) w_i = d \sum_{i = 1}^{n} b_i w_i + \sum_{i = 1}^{n} c_i w_i = d \mathsf{T}(u) + \mathsf{T}(v)
\end{align*}
$$

(b) Clearly

$$
\begin{align*}
    \mathsf{T}(v_i) = w_i && \text{for } i = 1, \ 2, \dots, n
\end{align*}
$$

(c) $$ \mathsf{T} $$ is unique: Suppose that $$ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ is linear and $$ \mathsf{U}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.
Then for $$ x \in \mathsf{V} $$ with

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

we have

$$
\begin{align*}
    \mathsf{U}(x) = \sum_{i = 1}^{n} a_i \mathsf{U}(v_i) = \sum_{i = 1}^{n} a_i w_i = \mathsf{T}(x)
\end{align*}
$$

Hence $$ \mathsf{U} = \mathsf{T} $$. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and suppose that $$ \mathsf{V} $$ has a finite basis $$ \{ v_1, \ v_2, \dots, v_n \} $$.
If $$ \mathsf{U}, \ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ are linear and $$ \mathsf{U}(v_i) = \mathsf{T}(v_i) $$ for $$ i = 1, \ 2, \dots, n $$, then $$ \mathsf{U} = \mathsf{T} $$.

## 2.2 The matrix representation of a linear transformation

### Definition

Let $$ \mathsf{V} $$ be a finite-dimensional vector space.
An ordered basis for $$ \mathsf{V} $$ is a basis for $$ \mathsf{V} $$ endowed with a specific order; that is, an ordered basis for $$ \mathsf{V} $$ is a finite sequence of linearly independent vectors in $$ \mathsf{V} $$ that generates $$ \mathsf{V} $$.

### Definition

Let $$ \beta = \{ u_1, \ u_2, \dots, u_n \} $$ be an ordered basis for a finite-dimensional vector space $$ \mathsf{V} $$.
For $$ x \in \mathsf{V} $$, let $$ a_1, \ a_2, \dots, a_n $$ be the unique scalars such that

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i u_i
\end{align*}
$$

We define the coordinate vector of $$ x $$ relative to $$ \beta $$, denoted $$ {[x]}_{\beta} $$, by

$$
\begin{align*}
    {[x]}_{\beta} =
    \begin{pmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{pmatrix}
\end{align*}
$$

### Definition

Suppose that $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional vector spaces with ordered bases $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_n \} $$, respectively.
Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then for each $$ j = 1, \ 2, \dots, n $$, there exist unique scalars $$ a_{ij} \in F $$, $$ i = 1, \ 2, \dots, m $$, such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} a_{ij} w_i && \text{for } j = 1, \ 2, \dots, n
\end{align*}
$$

Using the notation above, we call the $$ m \times n $$ matrix $$ A $$ defined by $$ A_{ij} = a_{ij} $$ the matrix representation of $$ \mathsf{T} $$ in the ordered bases $$ \beta $$ and $$ \gamma $$ and write $$ A = [\mathsf{T}]_{\beta}^{\gamma} $$.
If $$ \mathsf{V} = \mathsf{W} $$ and $$ \beta = \gamma $$, then we write $$ A = [\mathsf{T}]_{\beta} $$.  
Notice that the $$ j $$th column of $$ A $$ is simply $$ [\mathsf{T}(v_j)]_{\gamma} $$.
Also observe that if $$ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ is a linear transformation such that $$ {[\mathsf{U}]}_{\beta}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} $$, then $$ \mathsf{U} = \mathsf{T} $$ by the corollary to Theorem 2.6.

### Definition

We define the Kronecker delta $$ \delta_{ij} $$ by $$ \delta_{ij} = 1 $$ if $$ i = j $$ and $$ \delta_{ij} = 0 $$ if $$ i \neq j $$.
The $$ n \times n $$ identity matrix $$ I_n $$ is defined by $$ {(I_n)}_{ij} = \delta_{ij} $$.

### Definition

Let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be arbitrary functions, where $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are vector spaces over $$ F $$, and let $$ a \in F $$.
We define $$ \mathsf{T} + \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ by $$ (\mathsf{T} + \mathsf{U})(x) = \mathsf{T}(x) + \mathsf{U}(x) $$ for all $$ x \in \mathsf{V} $$, and $$ a \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ by $$ (a \mathsf{T})(x) = a \mathsf{T}(x) $$ for all $$ x \in \mathsf{V} $$.

### Theorem 2.7

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over a field $$ F $$, and let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.  
(a) For all $$ a \in F $$, $$ a \mathsf{T} + \mathsf{U} $$ is linear.  
(b) Using the operations of addition and scalar multiplication in the preceding definition, the collection of all linear transformations from $$ \mathsf{V} $$ to $$ \mathsf{W} $$ is a vector space over $$ F $$.

**Proof**  
(a) Let $$ x, \ y \in \mathsf{V} $$ and $$ c \in F $$.
Then

$$
\begin{align*}
    (a \mathsf{T} + \mathsf{U})(cx + y) &= a \mathsf{T}(cx + y) + \mathsf{U}(cx + y) \\
    &= a[\mathsf{T}(cx + y)] + c \mathsf{U}(x) + \mathsf{U}(y) \\
    &= a[c \mathsf{T}(x) + \mathsf{T}(y)] + c \mathsf{U}(x) + \mathsf{U}(y) \\
    &= ac \mathsf{T}(x) + c \mathsf{U}(x) + a \mathsf{T}(y) + \mathsf{U}(y) \\
    &= c(a \mathsf{T} + \mathsf{U})(x) + (a \mathsf{T} + \mathsf{U}) (y)
\end{align*}
$$

So $$ a \mathsf{T} + \mathsf{U} $$ is linear.  
(b) Noting that $$ \mathsf{T}_0 $$, the zero transformation, plays the role of the zero vector, it is easy to verify that axioms of a vector space are satisfied, and hence that the collection of all linear transformations from $$ \mathsf{V} $$ into $$ \mathsf{W} $$ is a vector space over $$ F $$. $$ \blacksquare $$

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over $$ F $$.
We denote the vector space of all linear transformations from $$ \mathsf{V} $$ into $$ \mathsf{W} $$ by $$ \mathcal{L}(\mathsf{V}, \ \mathsf{W}) $$.
In the case that $$ \mathsf{V} = \mathsf{W} $$ we write $$ \mathcal{L}(\mathsf{V}) $$ instead of $$ \mathcal{L}(\mathsf{V}, \ \mathsf{V}) $$.

### Theorem 2.8

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces with ordered bases $$ \beta $$ and $$ \gamma $$, respectively, and let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear transformations.
Then  
(a) $$ {[\mathsf{T} + \mathsf{U}]}_{\beta}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} + {[\mathsf{U}]}_{\beta}^{\gamma} $$  
(b) $$ {[a \mathsf{T}]}_{\beta}^{\gamma} = a {[\mathsf{T}]}_{\beta}^{\gamma} $$ for all scalars $$ a $$

**Proof**  
Let $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_m \} $$.
There exist unique scalars $$ a_{ij} $$ and $$ b_{ij} $$ ($$ 1 \le i \le m $$, $$ 1 \le j \le n $$) such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} a_{ij} w_i && \text{and} && \mathsf{U}(v_j) = \sum_{i = 1}^{m} b_{ij} w_i && \text{for } 1 \le j \le n
\end{align*}
$$

Hence

$$
\begin{align*}
    (\mathsf{T} + \mathsf{U})(v_j) = \sum_{i = 1}^{m} (a_{ij} + b_{ij}) w_i
\end{align*}
$$

Thus

$$
\begin{align*}
    {({[\mathsf{T} + \mathsf{U}]}_{\beta}^{\gamma})}_{ij} = a_{ij} + b_{ij} = {({[\mathsf{T}]}_{\beta}^{\gamma} + {[\mathsf{U}]}_{\beta}^{\gamma})}_{ij}
\end{align*}
$$

So (a) is proved, and the proof of (b) is similar. $$ \blacksquare $$

## 2.3 Composition of linear transformations and matrix multiplication

### Theorem 2.9

Let $$ \mathsf{V} $$, $$ \mathsf{W} $$, and $$ \mathsf{Z} $$ be vector spaces over the same field $$ F $$, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ and $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{Z} $$ be linear.
Then $$ \mathsf{U} \mathsf{T} : \mathsf{V} \rightarrow \mathsf{Z} $$ is linear.

**Proof**  
Let $$ x, \ y \in \mathsf{V} $$ and $$ a \in F $$.
Then

$$
\begin{align*}
    \mathsf{U} \mathsf{T} (ax + y) &= \mathsf{U}(\mathsf{T}(ax + y)) = \mathsf{U}(a \mathsf{T}(x) + \mathsf{T}(y)) \\
    &= a \mathsf{U}(\mathsf{T}(x)) + \mathsf{U}(\mathsf{T}(y)) = a(\mathsf{U} \mathsf{T})(x) + \mathsf{U} \mathsf{T}(y)
\end{align*}
$$

### Theorem 2.10

Let $$ \mathsf{V} $$ be a vector space.
Let $$ \mathsf{T}, \ \mathsf{U}_1, \ \mathsf{U}_2 \in \mathcal{L}(\mathsf{V}) $$.
Then  
(a) $$ \mathsf{T}(\mathsf{U}_1 + \mathsf{U}_2) = \mathsf{T} \mathsf{U}_1 + \mathsf{T} \mathsf{U}_2 $$ and $$ (\mathsf{U}_1 + \mathsf{U}_2) \mathsf{T} = \mathsf{U}_1 \mathsf{T} + \mathsf{U}_2 \mathsf{T} $$  
(b) $$ \mathsf{T} (\mathsf{U}_1 \mathsf{U}_2) = (\mathsf{T} \mathsf{U}_1) \mathsf{U}_2 $$  
(c) $$ \mathsf{T} \mathsf{I} = \mathsf{I} \mathsf{T} = \mathsf{T} $$  
(d) $$ a (\mathsf{U}_1 \mathsf{U}_2) = (a \mathsf{U}_1) \mathsf{U}_2 = \mathsf{U}_1 (a \mathsf{U}_2) $$ for all scalars $$ a $$.

### Definition

Let $$ A $$ be an $$ m \times n $$ matrix and $$ B $$ be an $$ n \times p $$ matrix.
We define the product of $$ A $$ and $$ B $$, denoted $$ AB $$, to be the $$ m \times p $$ matrix such that

$$
\begin{align*}
    {(AB)}_{ij} = \sum_{}
\end{align*}
$$