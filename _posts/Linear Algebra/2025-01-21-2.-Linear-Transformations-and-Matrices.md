---
title: 2. Linear Transformations and Matrices
# description: Short summary of the post
date: 2025-01-02 13:20
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, linear-transformation]     # TAG names should always be lowercase
math: true
pin: false
---

## 2.1 Linear transformations, null spaces, and ranges

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over the same field $$ F $$.
We call a function $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ a linear transformation from $$ \mathsf{V} $$ to $$ \mathsf{W} $$ if, for all $$ x, \ y \in \mathsf{V} $$ and $$ c \in F $$, we have  
(a) $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) $$ and  
(b) $$ \mathsf{T}(cx) = c \mathsf{T}(x) $$.

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
We define the null space (or kernel) $$ \mathsf{N}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the set of all vectors $$ x $$ in $$ \mathsf{V} $$ such that $$\mathsf{T}(x) = \mathit{0} $$; that is, $$ \mathsf{N}(\mathsf{T}) = \{ x \in \mathsf{V} : \mathsf{T}(x) = \mathit{0} \} $$.
We define the range (or image) $$ \mathsf{R}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the subset of $$ \mathsf{W} $$ consisting of all images (under $$ \mathsf{T} $$) of vectors in $$ \mathsf{V} $$; that is, $$ \mathsf{R}(\mathsf{T}) = \{ \mathsf{T}(x) : x \in \mathsf{V} \} $$.

### Theorem 2.1

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are subspaces of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.

**Proof**  
We use the symbols $$ \mathit{0}_{\mathsf{V}} $$ and $$ \mathit{0}_{\mathsf{W}} $$ to denote the zero vectors of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
Since $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_\mathsf{W} $$, we have that $$ \mathit{0}_{\mathsf{V}} \in \mathsf{N}(\mathsf{T}) $$.
Let $$ x, \ y \in \mathsf{N}(\mathsf{T}) $$ and $$ c \in F $$.
Then $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) = \mathit{0}_{\mathsf{W}} + \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$, and $$ \mathsf{T}(cx) = c \mathsf{T}(x) = c \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$.
Hence $$ x + y \in \mathsf{N}(\mathsf{T}) $$ and $$ cx \in \mathsf{N}(\mathsf{T}) $$, so that $$ \mathsf{N}(\mathsf{T}) $$ is a subspace of $$ \mathsf{V} $$.  
Because $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_{\mathsf{W}} $$, we have that $$ \mathit{0}_{\mathsf{W}} \in \mathsf{R}(\mathsf{T}) $$.
Now let $$ x, \ y \in \mathsf{R}(\mathsf{T}) $$ and $$ c \in F $$.
Then there exists $$ v $$ and $$ w $$ in $$ \mathsf{V} $$ such that $$ \mathsf{T}(v) = x $$ and $$ \mathsf{T}(w) = y $$.
So $$ \mathsf{T}(v + w) = \mathsf{T}(v) + \mathsf{T}(w) = x + y $$, and $$ \mathsf{T}(cv) = c \mathsf{T}(v) = cx $$.
Thus $$ x + y \in \mathsf{R}(\mathsf{T}) $$ and $$ cx \in \mathsf{R}(\mathsf{T}) $$, so $$ \mathsf{R}(\mathsf{T}) $$ is a subspace of $$ \mathsf{W} $$. $$ \blacksquare $$

### Theorem 2.2

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$, then

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) = \text{span}(\mathsf{T}(\beta)) = \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \})
\end{align*}
$$

**Proof**  
Clearly $$ \mathsf{T}(v_i) \in \mathsf{R}(\mathsf{T}) $$.
Then $$ w = \mathsf{T}(v) $$ for some $$ v \in \mathsf{V} $$.
Because $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have

$$
\begin{align*}
    v = \sum_{i = 1}^{n} a_i v_i && \text{for some } a_1, \ a_2, \dots, a_n \in F
\end{align*}
$$

Since $$ \mathsf{T} $$ is linear, it follows that

$$
\begin{align*}
    w = \mathsf{T}(v) = \sum_{i = 1}^{n} a_i \mathsf{T}(v_i) \in \text{span}(\mathsf{T}(\beta))
\end{align*}
$$

so $$ \mathsf{R}(\mathsf{T}) $$ is contained in span($$ \mathsf{T}(\beta) $$). $$ \blacksquare $$

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are finite-dimensional, then we define the nullity of $$ \mathsf{T} $$ and the rank of $$ \mathsf{T} $$ to be the dimensions of $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$, respectively.

### Theorem 2.3 (Dimension Theorem)

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{V} $$ is finite-dimensional, then

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

**Proof**  
Suppose that $$ \dim(\mathsf{V}) = n $$, $$ \dim(\mathsf{N}(\mathsf{T})) = k $$, and $$ \{ v_1, \ v_2, \dots, v_k \} $$ is a basis for $$ \mathsf{N}(\mathsf{T}) $$.
By the corollary to Theorem 1.11, we may extend $$ \{ v_1, \ v_2, \dots, v_k \} $$ to a basis $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ for $$ \mathsf{V} $$.
We claim that $$ S = \{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \} $$ is a basis for $$ \mathsf{R}(\mathsf{T}) $$.  
First we prove that $$ S $$ generates $$ \mathsf{R}(\mathsf{T}) $$.
Using Theorem 2.2 and the fact that $$ \mathsf{T}(v_i) = \mathit{0} $$ for $$ 1 \le i \le k $$, we have

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) &= \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \}) \\
    &= \text{span}(\{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \}) = \text{span}(S)
\end{align*}
$$

Now we prove that $$ S $$ is linearly independent.
Suppose that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i \mathsf{T}(v_i) = \mathit{0} && \text{for } b_{k + 1}, \ b_{k + 2}, \dots, b_n \in F
\end{align*}
$$

Using the fact that $$ \mathsf{T} $$ is linear, we have

$$
\begin{align*}
    \mathsf{T} \left( \sum_{i = k + 1}^{n} b_i v_i \right) = \mathit{0}
\end{align*}
$$

So

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i \in \mathsf{N}(\mathsf{T})
\end{align*}
$$

Hence there exist $$ c_1, \ c_2, \dots, c_k \in F $$ such that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i = \sum_{i = 1}^{k} c_i v_i
\end{align*}
$$

or

$$
\begin{align*}
    \sum_{i = 1}^{k} (-c_i) v_i + \sum_{i = k + 1}^{n} b_i v_i = \mathit{0}
\end{align*}
$$

Since $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have $$ b_i = 0 $$ for all $$ i $$.
Hence $$ S $$ is linearly independent.
This argument also shows that $$ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) $$ are distinct; therefore rank($$ \mathsf{T} $$) $$ = n - k $$. $$ \blacksquare $$

### Theorem 2.4

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.

**Proof**  
Suppose that $$ \mathsf{T} $$ is one-to-one and $$ x \in \mathsf{N}(\mathsf{T}) $$.
Then $$ \mathsf{T}(x) = \mathit{0} = \mathsf{T}(\mathit{0}) $$.
Since $$ \mathsf{T} $$ is one-to-one, we have $$ x = \mathit{0} $$.
Hence $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.  
Now assume that $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, and suppose that $$ \mathsf{T}(x) = \mathsf{T}(y) $$.
Then $$ \mathit{0} = \mathsf{T}(x) - \mathsf{T}(y) = \mathsf{T}(x - y) $$.
Therefore $$ x - y \in \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.
So $$ x - y = \mathit{0} $$, or $$ x = y $$.
This means $$ \mathsf{T} $$ is one-to-one. $$ \blacksquare $$

### Theorem 2.5

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces of equal dimension, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then the following are equivalent.  
(a) $$ \mathsf{T} $$ is one-to-one.  
(b) $$ \mathsf{T} $$ is onto.  
(c) rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{V}) $$

**Proof**  
From the dimension theorem, we have

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

Now, with the use of Theorem 2.4, we have that $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, if and only if nullity($$ \mathsf{T} $$) $$ = 0 $$, if and only if rank($$ \mathsf{T} $$) $$ = \dim(\mathsf{V}) $$, if and only if rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{W}) $$, and if and only if $$ \dim(\mathsf{R}(\mathsf{T})) = \dim(\mathsf{W}) $$.
By Theorem 1.11, this equality is equivalent to $$ \mathsf{R}(\mathsf{T}) = \mathsf{W} $$, the definition of $$ \mathsf{T} $$ being onto. $$ \blacksquare $$

We note that if $$ \mathsf{V} $$ is not finite-dimensional and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{V} $$ is linear, then it does not follow that one-to-one and onto are equivalent.

### Theorem 2.6

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over $$ F $$, and suppose that $$ \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$.
For $$ w_1, \ w_2, \dots, w_n $$ in $$ \mathsf{W} $$, there exists exactly one linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \mathsf{T}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.

**Proof**  
Let $$ x \in \mathsf{V} $$.
Then

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

where $$ a_1, \ a_2, \dots, a_n $$ are unique scalars.
Define $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ by

$$
\begin{align*}
    \mathsf{T}(x) = \sum_{i = 1}^{n} a_i w_i
\end{align*}
$$

(a) $$ \mathsf{T} $$ is linear: Suppose that $$ u, \ v \in \mathsf{V} $$ and $$ d \in F $$.
Then we may write

$$
\begin{align*}
    u = \sum_{i = 1}^{n} b_i v_i
\end{align*}
$$

and

$$
\begin{align*}
    v = \sum_{i = 1}^{n} c_i v_i
\end{align*}
$$

for some scalars $$ b_1, \ b_2, \dots, b_n, \ c_1, \ c_2, \dots, c_n $$.
Thus

$$
\begin{align*}
    du + v = \sum_{i = 1}^{n} (d b_i + c_i) v_i
\end{align*}
$$

So

$$
\begin{align*}
    \mathsf{T}(du + v) = \sum_{i = 1}^{n} (d b_i + c_i) w_i = d \sum_{i = 1}^{n} b_i w_i + \sum_{i = 1}^{n} c_i w_i = d \mathsf{T}(u) + \mathsf{T}(v)
\end{align*}
$$

(b) Clearly

$$
\begin{align*}
    \mathsf{T}(v_i) = w_i && \text{for } i = 1, \ 2, \dots, n
\end{align*}
$$

(c) $$ \mathsf{T} $$ is unique: Suppose that $$ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ is linear and $$ \mathsf{U}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.
Then for $$ x \in \mathsf{V} $$ with

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

we have

$$
\begin{align*}
    \mathsf{U}(x) = \sum_{i = 1}^{n} a_i \mathsf{U}(v_i) = \sum_{i = 1}^{n} a_i w_i = \mathsf{T}(x)
\end{align*}
$$

Hence $$ \mathsf{U} = \mathsf{T} $$. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and suppose that $$ \mathsf{V} $$ has a finite basis $$ \{ v_1, \ v_2, \dots, v_n \} $$.
If $$ \mathsf{U}, \ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ are linear and $$ \mathsf{U}(v_i) = \mathsf{T}(v_i) $$ for $$ i = 1, \ 2, \dots, n $$, then $$ \mathsf{U} = \mathsf{T} $$.

## 2.2 The matrix representation of a linear transformation

### Definition

Let $$ \mathsf{V} $$ be a finite-dimensional vector space.
An ordered basis for $$ \mathsf{V} $$ is a basis for $$ \mathsf{V} $$ endowed with a specific order; that is, an ordered basis for $$ \mathsf{V} $$ is a finite sequence of linearly independent vectors in $$ \mathsf{V} $$ that generates $$ \mathsf{V} $$.

### Definition

Let $$ \beta = \{ u_1, \ u_2, \dots, u_n \} $$ be an ordered basis for a finite-dimensional vector space $$ \mathsf{V} $$.
For $$ x \in \mathsf{V} $$, let $$ a_1, \ a_2, \dots, a_n $$ be the unique scalars such that

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i u_i
\end{align*}
$$

We define the coordinate vector of $$ x $$ relative to $$ \beta $$, denoted $$ {[x]}_{\beta} $$, by

$$
\begin{align*}
    {[x]}_{\beta} =
    \begin{pmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{pmatrix}
\end{align*}
$$

### Definition

Suppose that $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional vector spaces with ordered bases $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_n \} $$, respectively.
Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then for each $$ j = 1, \ 2, \dots, n $$, there exist unique scalars $$ a_{ij} \in F $$, $$ i = 1, \ 2, \dots, m $$, such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} a_{ij} w_i && \text{for } j = 1, \ 2, \dots, n
\end{align*}
$$

Using the notation above, we call the $$ m \times n $$ matrix $$ A $$ defined by $$ A_{ij} = a_{ij} $$ the matrix representation of $$ \mathsf{T} $$ in the ordered bases $$ \beta $$ and $$ \gamma $$ and write $$ A = [\mathsf{T}]_{\beta}^{\gamma} $$.
If $$ \mathsf{V} = \mathsf{W} $$ and $$ \beta = \gamma $$, then we write $$ A = [\mathsf{T}]_{\beta} $$.  
Notice that the $$ j $$th column of $$ A $$ is simply $$ [\mathsf{T}(v_j)]_{\gamma} $$.
Also observe that if $$ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ is a linear transformation such that $$ {[\mathsf{U}]}_{\beta}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} $$, then $$ \mathsf{U} = \mathsf{T} $$ by the corollary to Theorem 2.6.

### Definition

We define the Kronecker delta $$ \delta_{ij} $$ by $$ \delta_{ij} = 1 $$ if $$ i = j $$ and $$ \delta_{ij} = 0 $$ if $$ i \neq j $$.
The $$ n \times n $$ identity matrix $$ I_n $$ is defined by $$ {(I_n)}_{ij} = \delta_{ij} $$.

### Definition

Let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be arbitrary functions, where $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are vector spaces over $$ F $$, and let $$ a \in F $$.
We define $$ \mathsf{T} + \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ by $$ (\mathsf{T} + \mathsf{U})(x) = \mathsf{T}(x) + \mathsf{U}(x) $$ for all $$ x \in \mathsf{V} $$, and $$ a \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ by $$ (a \mathsf{T})(x) = a \mathsf{T}(x) $$ for all $$ x \in \mathsf{V} $$.

### Theorem 2.7

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over a field $$ F $$, and let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.  
(a) For all $$ a \in F $$, $$ a \mathsf{T} + \mathsf{U} $$ is linear.  
(b) Using the operations of addition and scalar multiplication in the preceding definition, the collection of all linear transformations from $$ \mathsf{V} $$ to $$ \mathsf{W} $$ is a vector space over $$ F $$.

**Proof**  
(a) Let $$ x, \ y \in \mathsf{V} $$ and $$ c \in F $$.
Then

$$
\begin{align*}
    (a \mathsf{T} + \mathsf{U})(cx + y) &= a \mathsf{T}(cx + y) + \mathsf{U}(cx + y) \\
    &= a[\mathsf{T}(cx + y)] + c \mathsf{U}(x) + \mathsf{U}(y) \\
    &= a[c \mathsf{T}(x) + \mathsf{T}(y)] + c \mathsf{U}(x) + \mathsf{U}(y) \\
    &= ac \mathsf{T}(x) + c \mathsf{U}(x) + a \mathsf{T}(y) + \mathsf{U}(y) \\
    &= c(a \mathsf{T} + \mathsf{U})(x) + (a \mathsf{T} + \mathsf{U}) (y)
\end{align*}
$$

So $$ a \mathsf{T} + \mathsf{U} $$ is linear.  
(b) Noting that $$ \mathsf{T}_0 $$, the zero transformation, plays the role of the zero vector, it is easy to verify that axioms of a vector space are satisfied, and hence that the collection of all linear transformations from $$ \mathsf{V} $$ into $$ \mathsf{W} $$ is a vector space over $$ F $$. $$ \blacksquare $$

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over $$ F $$.
We denote the vector space of all linear transformations from $$ \mathsf{V} $$ into $$ \mathsf{W} $$ by $$ \mathcal{L}(\mathsf{V}, \ \mathsf{W}) $$.
In the case that $$ \mathsf{V} = \mathsf{W} $$ we write $$ \mathcal{L}(\mathsf{V}) $$ instead of $$ \mathcal{L}(\mathsf{V}, \ \mathsf{V}) $$.

### Theorem 2.8

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces with ordered bases $$ \beta $$ and $$ \gamma $$, respectively, and let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear transformations.
Then  
(a) $$ {[\mathsf{T} + \mathsf{U}]}_{\beta}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} + {[\mathsf{U}]}_{\beta}^{\gamma} $$  
(b) $$ {[a \mathsf{T}]}_{\beta}^{\gamma} = a {[\mathsf{T}]}_{\beta}^{\gamma} $$ for all scalars $$ a $$

**Proof**  
Let $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_m \} $$.
There exist unique scalars $$ a_{ij} $$ and $$ b_{ij} $$ ($$ 1 \le i \le m $$, $$ 1 \le j \le n $$) such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} a_{ij} w_i && \text{and} && \mathsf{U}(v_j) = \sum_{i = 1}^{m} b_{ij} w_i && \text{for } 1 \le j \le n
\end{align*}
$$

Hence

$$
\begin{align*}
    (\mathsf{T} + \mathsf{U})(v_j) = \sum_{i = 1}^{m} (a_{ij} + b_{ij}) w_i
\end{align*}
$$

Thus

$$
\begin{align*}
    {({[\mathsf{T} + \mathsf{U}]}_{\beta}^{\gamma})}_{ij} = a_{ij} + b_{ij} = {({[\mathsf{T}]}_{\beta}^{\gamma} + {[\mathsf{U}]}_{\beta}^{\gamma})}_{ij}
\end{align*}
$$

So (a) is proved, and the proof of (b) is similar. $$ \blacksquare $$

## 2.3 Composition of linear transformations and matrix multiplication

### Theorem 2.9

Let $$ \mathsf{V} $$, $$ \mathsf{W} $$, and $$ \mathsf{Z} $$ be vector spaces over the same field $$ F $$, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ and $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{Z} $$ be linear.
Then $$ \mathsf{U} \mathsf{T} : \mathsf{V} \rightarrow \mathsf{Z} $$ is linear.

**Proof**  
Let $$ x, \ y \in \mathsf{V} $$ and $$ a \in F $$.
Then

$$
\begin{align*}
    \mathsf{U} \mathsf{T} (ax + y) &= \mathsf{U}(\mathsf{T}(ax + y)) = \mathsf{U}(a \mathsf{T}(x) + \mathsf{T}(y)) \\
    &= a \mathsf{U}(\mathsf{T}(x)) + \mathsf{U}(\mathsf{T}(y)) = a(\mathsf{U} \mathsf{T})(x) + \mathsf{U} \mathsf{T}(y)
\end{align*}
$$

### Theorem 2.10

Let $$ \mathsf{V} $$ be a vector space.
Let $$ \mathsf{T}, \ \mathsf{U}_1, \ \mathsf{U}_2 \in \mathcal{L}(\mathsf{V}) $$.
Then  
(a) $$ \mathsf{T}(\mathsf{U}_1 + \mathsf{U}_2) = \mathsf{T} \mathsf{U}_1 + \mathsf{T} \mathsf{U}_2 $$ and $$ (\mathsf{U}_1 + \mathsf{U}_2) \mathsf{T} = \mathsf{U}_1 \mathsf{T} + \mathsf{U}_2 \mathsf{T} $$  
(b) $$ \mathsf{T} (\mathsf{U}_1 \mathsf{U}_2) = (\mathsf{T} \mathsf{U}_1) \mathsf{U}_2 $$  
(c) $$ \mathsf{T} \mathsf{I} = \mathsf{I} \mathsf{T} = \mathsf{T} $$  
(d) $$ a (\mathsf{U}_1 \mathsf{U}_2) = (a \mathsf{U}_1) \mathsf{U}_2 = \mathsf{U}_1 (a \mathsf{U}_2) $$ for all scalars $$ a $$.

### Definition

Let $$ A $$ be an $$ m \times n $$ matrix and $$ B $$ be an $$ n \times p $$ matrix.
We define the product of $$ A $$ and $$ B $$, denoted $$ AB $$, to be the $$ m \times p $$ matrix such that

$$
\begin{align*}
    {(AB)}_{ij} = \sum_{k = 1}^{n} A_{ik} B_{kj} && \text{for } 1 \le i \le m, \ 1 \le j \le p
\end{align*}
$$

### Theorem 2.11

Let $$ \mathsf{V} $$, $$ \mathsf{W} $$, and $$ \mathsf{Z} $$ be finite-dimensional vector spaces with ordered basis $$ \alpha $$, $$ \beta $$, and $$ \gamma $$, respectively.
Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ and $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{Z} $$ be linear transformations.
Then

$$
\begin{align*}
    {[\mathsf{U} \mathsf{T}]}_{\alpha}^{\gamma} = {[\mathsf{U}]}_{\beta}^{\gamma} {[\mathsf{T}]}_{\alpha}^{\beta}
\end{align*}
$$

### Corollary

Let $$ \mathsf{V} $$ be a finite-dimensional vector space with an ordered basis $$ \beta $$.
Let $$ \mathsf{T}, \ \mathsf{U} \in \mathcal{L}(\mathsf{V}) $$.
Then $$ {[\mathsf{U} \mathsf{T}]}_{\beta} = {[\mathsf{U}]}_{\beta} {[\mathsf{T}]}_{\beta} $$.

### Theorem 2.12

Let $$ A $$ be an $$ m \times n $$ matrix, $$ B $$ and $$ C $$ be $$ n \times p $$ matrices, and $$ D $$ and $$ E $$ be $$ q \times m $$ matrices.
Then  
(a) $$ A(B + C) = AB + AC $$ and $$ (D + E)A = DA + EA $$.  
(b) $$ a(AB) = (aA)B = A(aB) $$ for any scalar $$ a $$.  
(c) $$ I_m A = A = A I_n $$.

**Proof**  
(a) We have

$$
\begin{align*}
    {[A(B + C)]}_{ij} &= \sum_{k = 1}^{n} A_{ik} {(B + C)}_{kj} = \sum_{k = 1}^{n} A_{ik} (B_{kj} + C_{kj}) \\
    &= \sum_{k = 1}^{n} (A_{ik} B_{kj} + A_{ik} C_{kj}) = \sum_{k = 1}^{n} A_{ik} B_{kj} + \sum_{k = 1}^{n} A_{ik} C_{kj} \\
    &= {(AB)}_{ij} + {(AC)}_{ij} = {[AB + AC]}_{ij}
\end{align*}
$$

So $$ A(B + C) = AB + AC $$.  
(c) We have

$$
\begin{align*}
    {(I_m A)}_{ij} = \sum_{k = 1}^{m} {(I_m)}_{ik} A_{kj} = \sum_{k = 1}^{m} \delta_{ik} A_{kj} = A_{ij}
\end{align*}
$$

The remainders of proofs are left as an exercise. $$ \blacksquare $$

### Corollary

Let $$ A $$ be an $$ m \times n $$ matrix, $$ B_1, \ B_2, \dots, B_k $$ be $$ n \times p $$ matrices, $$ C_1, \ C_2, \dots, C_k $$ be $$ q \times m $$ matrices, and $$ a_1, \ a_2, \dots, a_k $$ be scalars.
Then

$$
\begin{align*}
    A \left( \sum_{i = 1}^{k} a_i B_i \right) = \sum_{i = 1}^{k} a_i A B_i
\end{align*}
$$

and

$$
\begin{align*}
    \left( \sum_{i = 1}^{k} a_i C_i \right) A = \sum_{i = 1}^{k} a_i C_i A
\end{align*}
$$

### Theorem 2.13

Let $$ A $$ be an $$ m \times n $$ matrix and $$ B $$ an $$ n \times p $$ matrix.
For each $$ j $$, $$ j = 1, \ 2, \dots, p $$, let $$ u_j $$ and $$ v_j $$ denote the $$ j $$th columns of $$ AB $$ and $$ B $$, respectively.
Then  
(a) $$ u_j = A v_j $$  
(b) $$ v_j = B e_j $$, where $$ e_j $$ is the $$ j $$th standard vector of $$ {\mathsf{F}}^p $$.

**Proof**  
(a) We have

$$
\begin{align*}
    u_j =
    \begin{pmatrix}
        (AB)_{1j} \\
        (AB)_{2j} \\
        \vdots \\
        (AB)_{mj}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \displaystyle \sum_{k = 1}^{n} A_{1k} B_{kj} \\
        \displaystyle \sum_{k = 1}^{n} A_{2k} B_{kj} \\
        \vdots \\
        \displaystyle \sum_{k = 1}^{n} A_{mk} B_{kj}
    \end{pmatrix}
    = A
    \begin{pmatrix}
        B_{1j} \\
        B_{2j} \\
        \vdots \\
        B_{nj}
    \end{pmatrix}
    = A v_j
\end{align*}
$$

The proof of (b) is similar to (a). $$ \blacksquare $$

### Theorem 2.14

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces having ordered bases $$ \beta $$ and $$ \gamma $$, respectively, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then, for each $$ u \in \mathsf{V} $$, we have

$$
\begin{align*}
    {[\mathsf{T}(u)]}_{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} {[u]}_{\beta}
\end{align*}
$$

**Proof**  
Fix $$ u \in \mathsf{V} $$, and define the linear transformations $$ f : F \rightarrow \mathsf{V} $$ by $$ f(a) = au $$ and $$ g : F \rightarrow \mathsf{W} $$ by $$ g(a) = a \mathsf{T}(u) $$ for all $$ a \in F $$.
Let $$ \alpha = \{ 1 \} $$ be the standard ordered basis for $$ F $$.
Notice that $$ g = \mathsf{T} f $$.
Identifying column vectors as matrices and using Theorem 2.11, we obtain

$$
\begin{align*}
    {[\mathsf{T}(u)]}_{\gamma} = {[g(1)]}_{\gamma} = {[g]}_{\alpha}^{\gamma} = {[\mathsf{T} f]}_{\alpha}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} {[f]}_{\alpha}^{\beta} = {[\mathsf{T}]}_{\beta}^{\gamma} {[f(1)]}_{\beta} = {[\mathsf{T}]}_{\beta}^{\gamma} {[u]}_{\beta}
\end{align*}
$$

### Definition

Let $$ A $$ be an $$ m \times n $$ matrix with entries from a field $$ F $$.
We denote by $$ {\mathsf{L}}_A $$ the mapping $$ {\mathsf{L}}_A : \mathsf{F}^n \rightarrow \mathsf{F}^m $$ defined by $$ \mathsf{L}_A(x) = Ax $$ (the matrix product of $$ A $$ and $$ x $$) for each column vector $$ x \in \mathsf{F}^n $$.
We call $$ \mathsf{L}_A $$ a left-multiplication transformation.

### Theorem 2.15

Let $$ A $$ be an $$ m \times n $$ matrix with entries from $$ F $$.
Then the left-multiplication transformation $$ \mathsf{L}_A : \mathsf{F}^n \rightarrow \mathsf{F}^m $$ is linear.
Furthermore, if $$ B $$ is any other $$ m \times n $$ matrix (with entries from $$ F $$) and $$ \beta $$ and $$ \gamma $$ are the standard ordered bases for $$ \mathsf{F}^n $$ and $$ \mathsf{F}^m $$, respectively, then we have the following properties.  
(a) $$ {[\mathsf{L}_A]}_{\beta}^{\gamma} = A $$.  
(b) $$ \mathsf{L}_A = \mathsf{L}_B $$ if and only if $$ A = B $$.  
(c) $$ \mathsf{L}_{A + B} = \mathsf{L}_A + \mathsf{L}_B $$ and $$ \mathsf{L}_{aA} = a \mathsf{L}_A $$ for all $$ a \in F $$.  
(d) If $$ \mathsf{T} : \mathsf{F}^n \rightarrow \mathsf{F}^m $$ is linear, then there exists a unique $$ m \times n $$ matrix $$ C $$ such that $$ \mathsf{T} = \mathsf{L}_C $$.
In fact, $$ C = {[\mathsf{T}]}_{\beta}^{\gamma} $$.  
(e) If $$ E $$ is an $$ n \times p $$ matrix, then $$ \mathsf{L}_{AE} = \mathsf{L}_A \mathsf{L}_E $$.  
(f) If $$ m = n $$, then $$ \mathsf{L}_{I_n} = \mathsf{I}_{\mathsf{F}^n} $$.

**Proof**  
The fact that $$ \mathsf{L}_A $$ is linear follows immediately from Theorem 2.12.  
(a) The $$ j $$th column of $$ {[\mathsf{L}_A]}_{\beta}^{\gamma} $$ is equal to $$ \mathsf{L}_A(e_j) $$.
However, $$ \mathsf{L}_A(e_j) = A e_j $$, which is also the $$ j $$th column of $$ A $$ by Theorem 2.13(b).
So $$ {[\mathsf{L}_A]}_{\beta}^{\gamma} = A $$.  
(b) If $$ \mathsf{L}_A = \mathsf{L}_B $$, then we may use (a) to write $$ A = {[\mathsf{L}_A]}_{\beta}^{\gamma} = {[\mathsf{L}_B]}_{\beta}^{\gamma} = B $$.
Hence $$ A = B $$.
The proof of the converse is trivial.  
(c) Exercise.  
(d) Let $$ C = {[\mathsf{T}]}_{\beta}^{\gamma} $$.
By Theorem 2.14, we have $$ {[\mathsf{T}(x)]}_{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} {[x]}_{\beta} $$, or $$ \mathsf{T}(x) = Cx = \mathsf{L}_C(x) $$ for all $$ x \in F^n $$.
So $$ \mathsf{T} = \mathsf{L}_C $$.
The uniqueness of $$ C $$ follows from $$ (b) $$.  
(e) For any $$ j $$ ($$ 1 \le j \le p $$), we may apply Theorem 2.13 several times to note that $$ (AE)e_j $$ is the $$ j $$th column of $$ AE $$ and that the $$ j $$th column of $$ AE $$ is also equal to $$ A(E e_j) $$.
So $$ (AE)e_j = A(E e_j) $$.
Thus

$$
\begin{align*}
    \mathsf{L}_{AE}(e_j) = (AE)e_j = A(E e_j) = \mathsf{L}_A (E e_j) = \mathsf{L}_A (\mathsf{L}_E(e_j))
\end{align*}
$$

Hence $$ \mathsf{L}_{AE} = \mathsf{L}_A \mathsf{L}_E $$ by the corollary to Theorem 2.6.  
(f) Exercise. $$ \blacksquare $$

### Theorem 2.16

Let $$ A $$, $$ B $$, and $$ C $$ be matrices such that $$ A(BC) $$ is defined.
Then $$ (AB)C $$ is also defined and $$ A(BC) = (AB)C $$; that is, matrix multiplication is associative.

**Proof**  
Using (e) of Theorem 2.15 and the associativity of functional composition, we have

$$
\begin{align*}
    \mathsf{L}_{A(BC)} = \mathsf{L}_A \mathsf{L}_{BC} = \mathsf{L}_A (\mathsf{L}_B \mathsf{L}_C) = (\mathsf{L}_A \mathsf{L}_B) \mathsf{L}_C = \mathsf{L}_{AB} \mathsf{L}_C = \mathsf{L}_{(AB)C}
\end{align*}
$$

So from (b) of Theorem 2.15, it follows that $$ A(BC) = (AB)C $$. $$ \blacksquare $$

## 2.4 Invertibility and isomorphisms

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
A function $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{V} $$ is said to be an inverse of $$ \mathsf{T} $$ if $$ \mathsf{T} \mathsf{U} = \mathsf{I}_{\mathsf{W}} $$ and $$ \mathsf{U} \mathsf{T} = \mathsf{I}_{\mathsf{V}} $$.
If $$ \mathsf{T} $$ has an inverse, then $$ \mathsf{T} $$ is said to be invertible.
If $$ \mathsf{T} $$ is invertible, then the inverse of $$ \mathsf{T} $$ is unique and is denoted by $$ \mathsf{T}^{-1} $$.

The following facts hold for invertible functions $$ \mathsf{T} $$ and $$ \mathsf{U} $$.

1. $$ {(\mathsf{T} \mathsf{U})}^{-1} = \mathsf{U}^{-1} \mathsf{T}^{-1} $$.
2. $$ (\mathsf{T}^{-1})^{-1} = \mathsf{T} $$; in particular, $$ \mathsf{T}^{-1} $$ is invertible.
3. Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be a linear transformation, where $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional spaces of equal dimension.
Then $$ \mathsf{T} $$ is invertible if and only if rank($$ \mathsf{T} $$) $$ = \dim(\mathsf{V}) $$.

### Theorem 2.17

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear and invertible.
Then $$ \mathsf{T}^{-1} : \mathsf{W} \rightarrow \mathsf{V} $$ is linear.

**Proof**  
Let $$ y_1, \ y_2 \in \mathsf{W} $$ and $$ c \in F $$.
Since $$ \mathsf{T} $$ is onto and one-to-one, there exist unique vectors $$ x_1 $$ and $$ x_2 $$ such that $$ \mathsf{T}(x_1) = y_1 $$ and $$ \mathsf{T}(x_2) = y_2 $$.
Thus $$ x_1 = \mathsf{T}^{-1}(y_1) $$ and $$ x_2 = \mathsf{T}^{-1}(y_2) $$; so

$$
\begin{align*}
    \mathsf{T}^{-1}(cy_1 + y_2) &= \mathsf{T}^{-1}[c \mathsf{T}(x_1) + \mathsf{T}(x_2)] = \mathsf{T}^{-1} [\mathsf{T}(cx_1 + x_2)] \\
    &= cx_1 + x_2 = c \mathsf{T}^{-1}(y_1) + \mathsf{T}^{-1}(y_2)
\end{align*}
$$

### Corollary

Let $$ \mathsf{T} $$ be an invertible linear transformation from $$ \mathsf{V} $$ to $$ \mathsf{W} $$.
Then $$ \mathsf{V} $$ is finite-dimensional if and only if $$ \mathsf{W} $$ is finite-dimensional.
In this case, $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.

**Proof**  
Suppose that $$ \mathsf{V} $$ is finite-dimensional.
Let $$ \beta = \{ x_1, \ x_2, \dots, x_n \} $$ be a basis for $$ \mathsf{V} $$.
By Theorem 2.2, $$ \mathsf{T}(\beta) $$ spans $$ \mathsf{R}(\mathsf{T}) = \mathsf{W} $$; hence $$ \mathsf{W} $$ is finite-dimensional by Theorem 1.9.
Conversely, if $$ \mathsf{W} $$ is finite-dimensional, then so is $$ \mathsf{V} $$ by a similar argument, using $$ \mathsf{T}^{-1} $$.  
Now suppose that $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional.
Because $$ \mathsf{T} $$ is one-to-one and onto, we have

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) = 0 && \text{and} && \text{rank}(\mathsf{T}) = \dim(\mathsf{R}(\mathsf{T})) = \dim(\mathsf{W})
\end{align*}
$$

So by the dimension Theorem, it follows that $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$. $$ \blacksquare $$

### Definition

Let $$ A $$ be an $$ n \times n $$ matrix.
Then $$ A $$ is invertible if there exists an $$ n \times n $$ matrix $$ B $$ such that $$ AB = BA = I $$.

### Theorem 2.18

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces with ordered bases $$ \beta $$ and $$ \gamma $$, respectively.
Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{T} $$ is invertible if and only if $$ {[\mathsf{T}]}_{\beta}^{\gamma} $$ is invertible.
Furthermore, $$ {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} = {({[\mathsf{T}]}_{\beta}^{\gamma})}^{-1} $$.

**Proof**  
Suppose that $$ \mathsf{T} $$ is invertible.
By the corollary to Theorem 2.17, we have $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.
Let $$ n = \dim(\mathsf{V}) $$.
So $$ {[\mathsf{T}]}_{\beta}^{\gamma} $$ is an $$ n \times n $$ matrix.
Now $$ \mathsf{T}^{-1} : \mathsf{W} \rightarrow \mathsf{V} $$ satisfies $$ \mathsf{T} \mathsf{T}^{-1} = \mathsf{I}_{\mathsf{W}} $$ and $$ \mathsf{T}^{-1} \mathsf{T} = \mathsf{I}_{\mathsf{V}} $$.
Thus

$$
\begin{align*}
    I_n = {[\mathsf{I}_{\mathsf{V}}]}_{\beta} = {[\mathsf{T}^{-1} \mathsf{T}]}_{\beta} = {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} {[\mathsf{T}]}_{\beta}^{\gamma}
\end{align*}
$$

Similarly, $$ {[\mathsf{T}]}_{\beta}^{\gamma} {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} = I_n $$.
So $$ {[\mathsf{T}]}_{\beta}^{\gamma} $$ is invertible and $$ {\left( {[\mathsf{T}]}_{\beta}^{\gamma} \right)}^{-1} = {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} $$.  
Now suppose that $$ A = {[\mathsf{T}]}_{\beta}^{\gamma} $$ is invertible.
Then there exists an $$ n \times n $$ matrix $$ B $$ such that $$ AB = BA = I_n $$.
By Theorem 2.6, there exists $$ U \in \mathcal{L}(\mathsf{W}, \ \mathsf{V}) $$ such that

$$
\begin{align*}
    \mathsf{U}(w_j) = \sum_{i = 1}^{n} B_{ij} v_i && \text{for } j = 1, \ 2, \dots, n
\end{align*}
$$

where $$ \gamma = \{ w_1, \ w_2, \dots, w_n \} $$ and $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$.
It follows that $$ {[\mathsf{U}]}_{\gamma}^{\beta} = B $$.
To show that $$ \mathsf{U} = \mathsf{T}^{-1} $$, observe that

$$
\begin{align*}
    {[\mathsf{U} \mathsf{T}]}_{\beta} = {[\mathsf{U}]}_{\gamma}^{\beta} {[\mathsf{T}]}_{\beta}^{\gamma} = BA = I_n = {[\mathsf{I}_{\mathsf{V}}]}_{\beta}
\end{align*}
$$

by Theorem 2.11.
So $$ \mathsf{U} \mathsf{T} = \mathsf{I}_{\mathsf{V}} $$, and similarly, $$ \mathsf{T} \mathsf{U} = \mathsf{I}_{\mathsf{W}} $$. $$ \blacksquare $$

### Corollary 1

Let $$ \mathsf{V} $$ be a finite-dimensional vector space with an ordered basis $$ \beta $$, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{V} $$ be linear.
Then $$ \mathsf{T} $$ is invertible if and only if $$ {[\mathsf{T}]}_{\beta} $$ is invertible.
Furthermore, $$ {[\mathsf{T}^{-1}]}_{\beta} = {\left( {[\mathsf{T}]}_{\beta} \right)}^{-1} $$.

### Corollary 2

Let $$ A $$ be an $$ n \times n $$ matrix.
Then $$ A $$ is invertible if and only if $$ \mathsf{L}_A $$ is invertible.
Furthermore, $$ {(\mathsf{L}_A)}^{-1} = \mathsf{L}_{A^{-1}} $$.

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces.
We say that $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{W} $$ if there exists a linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ that is invertible.
Such a linear transformation is called an isomorphism from $$ \mathsf{V} $$ onto $$ \mathsf{W} $$.

### Theorem 2.19

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces (over the same field).
Then $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{W} $$ if and only if $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.

**Proof**  
Suppose that $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{W} $$ and that $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ is an isomorphism from $$ \mathsf{V} $$ to $$ \mathsf{W} $$.
By the lemma preceding Theorem 2.18, we have that $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.  
Now suppose that $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$, and let $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_n \} $$ be bases for $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
By Theorem 2.6, there exists $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \mathsf{T} $$ is linear and $$ \mathsf{T}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.
Using Theorem 2.2, we have

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) = \text{span}(\mathsf{T}(\beta)) = \text{span}(\gamma) = \mathsf{W}
\end{align*}
$$

So $$ \mathsf{T} $$ is onto.
From Theorem 2.5, we have that $$ \mathsf{T} $$ is also one-to-one.
Hence $$ \mathsf{T} $$ is an isomorphism. $$ \blacksquare $$

By the lemma to Theorem 2.18, if $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are isomorphic, then either both of $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional or both are infinite-dimensional.

### Corollary

Let $$ \mathsf{V} $$ be a vector space over $$ F $$.
Then $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{F}^n $$ if and only if $$ \dim(\mathsf{V}) = n $$.

### Theorem 2.20

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces over $$ F $$ of dimensions $$ n $$ and $$ m $$, respectively, and let $$ \beta $$ and $$ \gamma $$ be ordered bases for $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
Then the function $$ \Phi_{\beta}^{\gamma} : \mathcal{L}(\mathsf{V}, \ \mathsf{W}) \rightarrow \mathsf{M}_{m \times n}(F) $$, defined by $$ \Phi_{\beta}^{\gamma}(\mathsf{T}) = {[\mathsf{T}]}_{\beta}^{\gamma} $$ for $$ \mathsf{T} \in \mathcal{L}(\mathsf{V}, \ \mathsf{W}) $$, is an isomorphism.

**Proof**  
By Theorem 2.8, $$ \Phi_{\beta}^{\gamma} $$ is linear.
Hence we must show that $$ \Phi_{\beta}^{\gamma} $$ is one-to-one and onto.
This is accomplished if we show that for every $$ m \times n $$ matrix $$ A $$, there exists a unique linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \Phi_{\beta}^{\gamma}(\mathsf{T}) = A $$.
Let $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$, $$ \gamma = \{ w_1, \ w_2, \dots, w_m \} $$, and let $$ A $$ be a given $$ m \times n $$ matrix.
By Theorem 2.6, there exists a unique linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} A_{ij} w_i && \text{for } 1 \le j \le n
\end{align*}
$$

But this means that $$ {[\mathsf{T}]}_{\beta}^{\gamma} = A $$, or $$ \Phi_{\beta}^{\gamma}(\mathsf{T}) = A $$.
So $$ \Phi_{\beta}^{\gamma} $$ is an isomorphism. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces of dimensions $$ n $$ and $$ m $$, respectively.
Then $$ \mathcal{L}(\mathsf{V}, \ \mathsf{W}) $$ is finite-dimensional of dimension $$ mn $$.

**Proof**  
The proof follows from Theorems 2.20 and 2.19 and the fact that $$ \dim(\mathsf{M}_{m \times n}(F)) = mn $$. $$ \blacksquare $$

### Definition

Let $$ \beta $$ be an ordered basis for an $$ n $$-dimensional vector space $$ \mathsf{V} $$ over the field $$ F $$.
The standard representation of $$ \mathsf{V} $$ with respect to $$ \beta $$ is the function $$ \Phi_{\beta} : \mathsf{V} \rightarrow \mathsf{F}^n $$ defined by $$ \phi_{\beta}(x) = {[x]}_{\beta} $$ for each $$ x \in \mathsf{V} $$.

### Theorem 2.21

For any finite-dimensional vector space $$ \mathsf{V} $$ with ordered basis $$ \beta $$, $$ \phi_{\beta} $$ is an isomorphism.

![Desktop View](/assets/img/Linear Algebra/2.-Linear-Transformations-and-Matrices/Figure 2.1.png){: width="700" }
_**Figure 2.1**_

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces of dimension $$ n $$ and $$ m $$, respectively, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be a linear transformation.
Define $$ A = {[\mathsf{T}]}_{\beta}^{\gamma} $$, where $$ \beta $$ and $$ \gamma $$ are arbitrary ordered bases of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
To study the relationship between the linear transformations $$ \mathsf{T} $$ and $$ \mathsf{L}_A : \mathsf{F}^n \rightarrow \mathsf{F}^m $$, first consider Figure 2.2.
Notice that there are two composites of linear transformations that map $$ \mathsf{V} $$ info $$ \mathsf{F}^m $$:

1. Map $$ \mathsf{V} $$ into $$ \mathsf{F}^n $$ with $$ \phi_{\beta} $$ and follow this transformation with $$ \mathsf{L}_A $$; this yields the composite $$ \mathsf{L}_A \phi_{\beta} $$.
2. Map $$ \mathsf{V} $$ into $$ \mathsf{W} $$ with $$ \mathsf{T} $$ and follow it by $$ \phi_{\gamma} $$ to obtain the composite $$ \phi_{\gamma} \mathsf{T} $$.

By a reformulation of Theorem 2.14, we may conclude that

$$
\begin{align*}
    \mathsf{L}_A \phi_{\beta} = \phi_{\gamma} \mathsf{T}
\end{align*}
$$

## 2.5 The change of coordinate matrix

### Theorem 2.22

Let $$ \beta $$ and $$ \beta' $$ be two ordered bases for a finite-dimensional vector space $$ \mathsf{V} $$, and let $$ Q = {[\mathsf{I}_{\mathsf{V}}]}_{\beta'}^{\beta} $$.
Then  
(a) $$ Q $$ invertible.  
(b) For any $$ v \in \mathsf{V} $$, $$ {[v]}_{\beta} = Q {[v]}_{\beta'} $$.

**Proof**  
(a) Since $$ \mathsf{I}_{\mathsf{V}} $$ is invertible, $$ Q $$ is invertible by Theorem 2.18.  
(b) For any $$ v \in \mathsf{V} $$,

$$
\begin{align*}
    {[v]}_{\beta} = {[\mathsf{I}_{\mathsf{V}}]}_{\beta} = {[\mathsf{I}_{\mathsf{V}}]}_{\beta'}^{\beta} {[v]}_{\beta'} = Q {[v]}_{\beta'}
\end{align*}
$$

by Theorem 2.14. $$ \blacksquare $$

Observe that if $$ \beta = \{ x_1, \ x_2, \dots, x_n \} $$ and $$ \beta' = \{ x_1', \ x_2', \dots, x_n' \} $$, then

$$
\begin{align*}
    x_j' = \sum_{i = 1}^{n} Q_{ij} x_i
\end{align*}
$$

for $$ j = 1, \ 2, \dots, n $$; that is, the $$ j $$th column of $$ Q $$ is $$ {[x_j']}_{\beta} $$.

### Theorem 2.23

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$, and let $$ \beta $$ and $$ \beta' $$ be ordered bases for $$ \mathsf{V} $$.
Suppose that $$ Q $$ is the change of coordinate matrix that changes $$ \beta' $$-coordinates into $$ \beta $$-coordinates.
Then

$$
\begin{align*}
    {[\mathsf{T}]}_{\beta'} = Q^{-1} {[\mathsf{T}]}_{\beta} Q
\end{align*}
$$

**Proof**  
Let $$ \mathsf{I} $$ be the identity transformation on $$ \mathsf{V} $$.
Then $$ \mathsf{T} = \mathsf{I} \mathsf{T} = \mathsf{T} \mathsf{I} $$; hence, by Theorem 2.11,

$$
\begin{align*}
    Q {[\mathsf{T}]}_{\beta'} = {[\mathsf{I}]}_{\beta'}^{\beta} {[\mathsf{T}]}_{\beta'}^{\beta'} = {[\mathsf{I} \mathsf{T}]}_{\beta'}^{\beta} = {[\mathsf{T} \mathsf{I}]}_{\beta'}^{\beta} = {[\mathsf{T}]}_{\beta}^{\beta} {[\mathsf{I}]}_{\beta'}^{\beta} = {[\mathsf{T}]}_{\beta} Q
\end{align*}
$$

Therefore $$ {[\mathsf{T}]}_{\beta'} = Q^{-1} {[\mathsf{T}]}_{\beta} Q $$. $$ \blacksquare $$

### Corollary

Let $$ A \in \mathsf{M}_{n \times n}(F) $$, and let $$ \gamma $$ be an ordered basis for $$ \mathsf{F}^n $$.
Then $$ {[\mathsf{L}_A]}_{\gamma} = Q^{-1} AQ $$, where $$ Q $$ is the $$ n \times n $$ matrix whose $$ j $$th column is the $$ j $$th vector of $$ \gamma $$.

### Definition

Let $$ A $$ and $$ B $$ be matrices in $$ \mathsf{M}_{n \times n}(F) $$.
We say that $$ B $$ is similar to $$ A $$ if there exists an invertible matrix $$ Q $$ such that $$ B = Q^{-1} AQ $$.

## 2.6 Dual spaces