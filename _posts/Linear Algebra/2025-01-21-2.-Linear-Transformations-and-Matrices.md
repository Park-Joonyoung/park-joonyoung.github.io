---
title: 2. Linear Transformations and Matrices
# description: Short summary of the post
date: 2025-01-02 13:20
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, linear-transformation]     # TAG names should always be lowercase
math: true
pin: false
---

## 2.1 Linear transformations, null spaces, and ranges

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over the same field $$ F $$.
We call a function $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ a linear transformation from $$ \mathsf{V} $$ to $$ \mathsf{W} $$ if, for all $$ x, \ y \in \mathsf{V} $$ and $$ c \in F $$, we have  
(a) $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) $$ and  
(b) $$ \mathsf{T}(cx) = c \mathsf{T}(x) $$.

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
We define the null space (or kernel) $$ \mathsf{N}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the set of all vectors $$ x $$ in $$ \mathsf{V} $$ such that $$\mathsf{T}(x) = \mathit{0} $$; that is, $$ \mathsf{N}(\mathsf{T}) = \{ x \in \mathsf{V} : \mathsf{T}(x) = \mathit{0} \} $$.
We define the range (or image) $$ \mathsf{R}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the subset of $$ \mathsf{W} $$ consisting of all images (under $$ \mathsf{T} $$) of vectors in $$ \mathsf{V} $$; that is, $$ \mathsf{R}(\mathsf{T}) = \{ \mathsf{T}(x) : x \in \mathsf{V} \} $$.

### Theorem 2.1

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are subspaces of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.

**Proof**  
We use the symbols $$ \mathit{0}_{\mathsf{V}} $$ and $$ \mathit{0}_{\mathsf{W}} $$ to denote the zero vectors of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
Since $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_\mathsf{W} $$, we have that $$ \mathit{0}_{\mathsf{V}} \in \mathsf{N}(\mathsf{T}) $$.
Let $$ x, \ y \in \mathsf{N}(\mathsf{T}) $$ and $$ c \in F $$.
Then $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) = \mathit{0}_{\mathsf{W}} + \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$, and $$ \mathsf{T}(cx) = c \mathsf{T}(x) = c \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$.
Hence $$ x + y \in \mathsf{N}(\mathsf{T}) $$ and $$ cx \in \mathsf{N}(\mathsf{T}) $$, so that $$ \mathsf{N}(\mathsf{T}) $$ is a subspace of $$ \mathsf{V} $$.  
Because $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_{\mathsf{W}} $$, we have that $$ \mathit{0}_{\mathsf{W}} \in \mathsf{R}(\mathsf{T}) $$.
Now let $$ x, \ y \in \mathsf{R}(\mathsf{T}) $$ and $$ c \in F $$.
Then there exists $$ v $$ and $$ w $$ in $$ \mathsf{V} $$ such that $$ \mathsf{T}(v) = x $$ and $$ \mathsf{T}(w) = y $$.
So $$ \mathsf{T}(v + w) = \mathsf{T}(v) + \mathsf{T}(w) = x + y $$, and $$ \mathsf{T}(cv) = c \mathsf{T}(v) = cx $$.
Thus $$ x + y \in \mathsf{R}(\mathsf{T}) $$ and $$ cx \in \mathsf{R}(\mathsf{T}) $$, so $$ \mathsf{R}(\mathsf{T}) $$ is a subspace of $$ \mathsf{W} $$. $$ \blacksquare $$

### Theorem 2.2

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$, then

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) = \text{span}(\mathsf{T}(\beta)) = \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \})
\end{align*}
$$

**Proof**  
Clearly $$ \mathsf{T}(v_i) \in \mathsf{R}(\mathsf{T}) $$.
Then $$ w = \mathsf{T}(v) $$ for some $$ v \in \mathsf{V} $$.
Because $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have

$$
\begin{align*}
    v = \sum_{i = 1}^{n} a_i v_i && \text{for some } a_1, \ a_2, \dots, a_n \in F
\end{align*}
$$

Since $$ \mathsf{T} $$ is linear, it follows that

$$
\begin{align*}
    w = \mathsf{T}(v) = \sum_{i = 1}^{n} a_i \mathsf{T}(v_i) \in \text{span}(\mathsf{T}(\beta))
\end{align*}
$$

so $$ \mathsf{R}(\mathsf{T}) $$ is contained in span($$ \mathsf{T}(\beta) $$). $$ \blacksquare $$

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are finite-dimensional, then we define the nullity of $$ \mathsf{T} $$ and the rank of $$ \mathsf{T} $$ to be the dimensions of $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$, respectively.

### Theorem 2.3 (Dimension Theorem)

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{V} $$ is finite-dimensional, then

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

**Proof**  
Suppose that $$ \dim(\mathsf{V}) = n $$, $$ \dim(\mathsf{N}(\mathsf{T})) = k $$, and $$ \{ v_1, \ v_2, \dots, v_k \} $$ is a basis for $$ \mathsf{N}(\mathsf{T}) $$.
By the corollary to Theorem 1.11, we may extend $$ \{ v_1, \ v_2, \dots, v_k \} $$ to a basis $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ for $$ \mathsf{V} $$.
We claim that $$ S = \{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \} $$ is a basis for $$ \mathsf{R}(\mathsf{T}) $$.  
First we prove that $$ S $$ generates $$ \mathsf{R}(\mathsf{T}) $$.
Using Theorem 2.2 and the fact that $$ \mathsf{T}(v_i) = \mathit{0} $$ for $$ 1 \le i \le k $$, we have

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) &= \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \}) \\
    &= \text{span}(\{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \}) = \text{span}(S)
\end{align*}
$$

Now we prove that $$ S $$ is linearly independent.
Suppose that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i \mathsf{T}(v_i) = \mathit{0} && \text{for } b_{k + 1}, \ b_{k + 2}, \dots, b_n \in F
\end{align*}
$$

Using the fact that $$ \mathsf{T} $$ is linear, we have

$$
\begin{align*}
    \mathsf{T} \left( \sum_{i = k + 1}^{n} b_i v_i \right) = \mathit{0}
\end{align*}
$$

So

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i \in \mathsf{N}(\mathsf{T})
\end{align*}
$$

Hence there exist $$ c_1, \ c_2, \dots, c_k \in F $$ such that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i = \sum_{i = 1}^{k} c_i v_i
\end{align*}
$$

or

$$
\begin{align*}
    \sum_{i = 1}^{k} (-c_i) v_i + \sum_{i = k + 1}^{n} b_i v_i = \mathit{0}
\end{align*}
$$

Since $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have $$ b_i = 0 $$ for all $$ i $$.
Hence $$ S $$ is linearly independent.
This argument also shows that $$ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) $$ are distinct; therefore rank($$ \mathsf{T} $$) $$ = n - k $$. $$ \blacksquare $$

### Theorem 2.4

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.

**Proof**  
Suppose that $$ \mathsf{T} $$ is one-to-one and $$ x \in \mathsf{N}(\mathsf{T}) $$.
Then $$ \mathsf{T}(x) = \mathit{0} = \mathsf{T}(\mathit{0}) $$.
Since $$ \mathsf{T} $$ is one-to-one, we have $$ x = \mathit{0} $$.
Hence $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.  
Now assume that $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, and suppose that $$ \mathsf{T}(x) = \mathsf{T}(y) $$.
Then $$ \mathit{0} = \mathsf{T}(x) - \mathsf{T}(y) = \mathsf{T}(x - y) $$.
Therefore $$ x - y \in \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.
So $$ x - y = \mathit{0} $$, or $$ x = y $$.
This means $$ \mathsf{T} $$ is one-to-one. $$ \blacksquare $$

### Theorem 2.5

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces of equal dimension, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then the following are equivalent.  
(a) $$ \mathsf{T} $$ is one-to-one.  
(b) $$ \mathsf{T} $$ is onto.  
(c) rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{V}) $$

**Proof**  
From the dimension theorem, we have

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

Now, with the use of Theorem 2.4, we have that $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, if and only if nullity($$ \mathsf{T} $$) $$ = 0 $$, if and only if rank($$ \mathsf{T} $$) $$ = \dim(\mathsf{V}) $$, if and only if rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{W}) $$, and if and only if $$ \dim(\mathsf{R}(\mathsf{T})) = \dim(\mathsf{W}) $$.
By Theorem 1.11, this equality is equivalent to $$ \mathsf{R}(\mathsf{T}) = \mathsf{W} $$, the definition of $$ \mathsf{T} $$ being onto. $$ \blacksquare $$

We note that if $$ \mathsf{V} $$ is not finite-dimensional and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{V} $$ is linear, then it does not follow that one-to-one and onto are equivalent.

### Theorem 2.6

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over $$ F $$, and suppose that $$ \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$.
For $$ w_1, \ w_2, \dots, w_n $$ in $$ \mathsf{W} $$, there exists exactly one linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \mathsf{T}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.

**Proof**  
Let $$ x \in \mathsf{V} $$.
Then

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

where $$ a_1, \ a_2, \dots, a_n $$ are unique scalars.
Define $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ by

$$
\begin{align*}
    \mathsf{T}(x) = \sum_{i = 1}^{n} a_i w_i
\end{align*}
$$

(a) $$ \mathsf{T} $$ is linear: Suppose that $$ u, \ v \in \mathsf{V} $$ and $$ d \in F $$.
Then we may write

$$
\begin{align*}
    u = \sum_{i = 1}^{n} b_i v_i
\end{align*}
$$

and

$$
\begin{align*}
    v = \sum_{i = 1}^{n} c_i v_i
\end{align*}
$$

for some scalars $$ b_1, \ b_2, \dots, b_n, \ c_1, \ c_2, \dots, c_n $$.
Thus

$$
\begin{align*}
    du + v = \sum_{i = 1}^{n} (d b_i + c_i) v_i
\end{align*}
$$

So

$$
\begin{align*}
    \mathsf{T}(du + v) = \sum_{i = 1}^{n} (d b_i + c_i) w_i = d \sum_{i = 1}^{n} b_i w_i + \sum_{i = 1}^{n} c_i w_i = d \mathsf{T}(u) + \mathsf{T}(v)
\end{align*}
$$

(b) Clearly

$$
\begin{align*}
    \mathsf{T}(v_i) = w_i && \text{for } i = 1, \ 2, \dots, n
\end{align*}
$$

(c) $$ \mathsf{T} $$ is unique: Suppose that $$ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ is linear and $$ \mathsf{U}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.
Then for $$ x \in \mathsf{V} $$ with

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

we have

$$
\begin{align*}
    \mathsf{U}(x) = \sum_{i = 1}^{n} a_i \mathsf{U}(v_i) = \sum_{i = 1}^{n} a_i w_i = \mathsf{T}(x)
\end{align*}
$$

Hence $$ \mathsf{U} = \mathsf{T} $$. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and suppose that $$ \mathsf{V} $$ has a finite basis $$ \{ v_1, \ v_2, \dots, v_n \} $$.
If $$ \mathsf{U}, \ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ are linear and $$ \mathsf{U}(v_i) = \mathsf{T}(v_i) $$ for $$ i = 1, \ 2, \dots, n $$, then $$ \mathsf{U} = \mathsf{T} $$.

## 2.2 The matrix representation of a linear transformation

### Definition

Let $$ \mathsf{V} $$ be a finite-dimensional vector space.
An ordered basis for $$ \mathsf{V} $$ is a basis for $$ \mathsf{V} $$ endowed with a specific order; that is, an ordered basis for $$ \mathsf{V} $$ is a finite sequence of linearly independent vectors in $$ \mathsf{V} $$ that generates $$ \mathsf{V} $$.

### Definition

Let $$ \beta = \{ u_1, \ u_2, \dots, u_n \} $$ be an ordered basis for a finite-dimensional vector space $$ \mathsf{V} $$.
For $$ x \in \mathsf{V} $$, let $$ a_1, \ a_2, \dots, a_n $$ be the unique scalars such that

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i u_i
\end{align*}
$$

We define the coordinate vector of $$ x $$ relative to $$ \beta $$, denoted $$ {[x]}_{\beta} $$, by

$$
\begin{align*}
    {[x]}_{\beta} =
    \begin{pmatrix}
        a_1 \\ a_2 \\ \vdots \\ a_n
    \end{pmatrix}
\end{align*}
$$

### Definition

Suppose that $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional vector spaces with ordered bases $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_n \} $$, respectively.
Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then for each $$ j = 1, \ 2, \dots, n $$, there exist unique scalars $$ a_{ij} \in F $$, $$ i = 1, \ 2, \dots, m $$, such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} a_{ij} w_i && \text{for } j = 1, \ 2, \dots, n
\end{align*}
$$

Using the notation above, we call the $$ m \times n $$ matrix $$ A $$ defined by $$ A_{ij} = a_{ij} $$ the matrix representation of $$ \mathsf{T} $$ in the ordered bases $$ \beta $$ and $$ \gamma $$ and write $$ A = [\mathsf{T}]_{\beta}^{\gamma} $$.
If $$ \mathsf{V} = \mathsf{W} $$ and $$ \beta = \gamma $$, then we write $$ A = [\mathsf{T}]_{\beta} $$.  
Notice that the $$ j $$th column of $$ A $$ is simply $$ [\mathsf{T}(v_j)]_{\gamma} $$.
Also observe that if $$ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ is a linear transformation such that $$ {[\mathsf{U}]}_{\beta}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} $$, then $$ \mathsf{U} = \mathsf{T} $$ by the corollary to Theorem 2.6.

### Definition

We define the Kronecker delta $$ \delta_{ij} $$ by $$ \delta_{ij} = 1 $$ if $$ i = j $$ and $$ \delta_{ij} = 0 $$ if $$ i \neq j $$.
The $$ n \times n $$ identity matrix $$ I_n $$ is defined by $$ {(I_n)}_{ij} = \delta_{ij} $$.

### Definition

Let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be arbitrary functions, where $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are vector spaces over $$ F $$, and let $$ a \in F $$.
We define $$ \mathsf{T} + \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ by $$ (\mathsf{T} + \mathsf{U})(x) = \mathsf{T}(x) + \mathsf{U}(x) $$ for all $$ x \in \mathsf{V} $$, and $$ a \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ by $$ (a \mathsf{T})(x) = a \mathsf{T}(x) $$ for all $$ x \in \mathsf{V} $$.

### Theorem 2.7

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over a field $$ F $$, and let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.  
(a) For all $$ a \in F $$, $$ a \mathsf{T} + \mathsf{U} $$ is linear.  
(b) Using the operations of addition and scalar multiplication in the preceding definition, the collection of all linear transformations from $$ \mathsf{V} $$ to $$ \mathsf{W} $$ is a vector space over $$ F $$.

**Proof**  
(a) Let $$ x, \ y \in \mathsf{V} $$ and $$ c \in F $$.
Then

$$
\begin{align*}
    (a \mathsf{T} + \mathsf{U})(cx + y) &= a \mathsf{T}(cx + y) + \mathsf{U}(cx + y) \\
    &= a[\mathsf{T}(cx + y)] + c \mathsf{U}(x) + \mathsf{U}(y) \\
    &= a[c \mathsf{T}(x) + \mathsf{T}(y)] + c \mathsf{U}(x) + \mathsf{U}(y) \\
    &= ac \mathsf{T}(x) + c \mathsf{U}(x) + a \mathsf{T}(y) + \mathsf{U}(y) \\
    &= c(a \mathsf{T} + \mathsf{U})(x) + (a \mathsf{T} + \mathsf{U}) (y)
\end{align*}
$$

So $$ a \mathsf{T} + \mathsf{U} $$ is linear.  
(b) Noting that $$ \mathsf{T}_0 $$, the zero transformation, plays the role of the zero vector, it is easy to verify that axioms of a vector space are satisfied, and hence that the collection of all linear transformations from $$ \mathsf{V} $$ into $$ \mathsf{W} $$ is a vector space over $$ F $$. $$ \blacksquare $$

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over $$ F $$.
We denote the vector space of all linear transformations from $$ \mathsf{V} $$ into $$ \mathsf{W} $$ by $$ \mathcal{L}(\mathsf{V}, \ \mathsf{W}) $$.
In the case that $$ \mathsf{V} = \mathsf{W} $$ we write $$ \mathcal{L}(\mathsf{V}) $$ instead of $$ \mathcal{L}(\mathsf{V}, \ \mathsf{V}) $$.

### Theorem 2.8

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces with ordered bases $$ \beta $$ and $$ \gamma $$, respectively, and let $$ \mathsf{T}, \ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear transformations.
Then  
(a) $$ {[\mathsf{T} + \mathsf{U}]}_{\beta}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} + {[\mathsf{U}]}_{\beta}^{\gamma} $$  
(b) $$ {[a \mathsf{T}]}_{\beta}^{\gamma} = a {[\mathsf{T}]}_{\beta}^{\gamma} $$ for all scalars $$ a $$

**Proof**  
Let $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_m \} $$.
There exist unique scalars $$ a_{ij} $$ and $$ b_{ij} $$ ($$ 1 \le i \le m $$, $$ 1 \le j \le n $$) such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} a_{ij} w_i && \text{and} && \mathsf{U}(v_j) = \sum_{i = 1}^{m} b_{ij} w_i && \text{for } 1 \le j \le n
\end{align*}
$$

Hence

$$
\begin{align*}
    (\mathsf{T} + \mathsf{U})(v_j) = \sum_{i = 1}^{m} (a_{ij} + b_{ij}) w_i
\end{align*}
$$

Thus

$$
\begin{align*}
    {({[\mathsf{T} + \mathsf{U}]}_{\beta}^{\gamma})}_{ij} = a_{ij} + b_{ij} = {({[\mathsf{T}]}_{\beta}^{\gamma} + {[\mathsf{U}]}_{\beta}^{\gamma})}_{ij}
\end{align*}
$$

So (a) is proved, and the proof of (b) is similar. $$ \blacksquare $$

## 2.3 Composition of linear transformations and matrix multiplication

### Theorem 2.9

Let $$ \mathsf{V} $$, $$ \mathsf{W} $$, and $$ \mathsf{Z} $$ be vector spaces over the same field $$ F $$, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ and $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{Z} $$ be linear.
Then $$ \mathsf{U} \mathsf{T} : \mathsf{V} \rightarrow \mathsf{Z} $$ is linear.

**Proof**  
Let $$ x, \ y \in \mathsf{V} $$ and $$ a \in F $$.
Then

$$
\begin{align*}
    \mathsf{U} \mathsf{T} (ax + y) &= \mathsf{U}(\mathsf{T}(ax + y)) = \mathsf{U}(a \mathsf{T}(x) + \mathsf{T}(y)) \\
    &= a \mathsf{U}(\mathsf{T}(x)) + \mathsf{U}(\mathsf{T}(y)) = a(\mathsf{U} \mathsf{T})(x) + \mathsf{U} \mathsf{T}(y)
\end{align*}
$$

### Theorem 2.10

Let $$ \mathsf{V} $$ be a vector space.
Let $$ \mathsf{T}, \ \mathsf{U}_1, \ \mathsf{U}_2 \in \mathcal{L}(\mathsf{V}) $$.
Then  
(a) $$ \mathsf{T}(\mathsf{U}_1 + \mathsf{U}_2) = \mathsf{T} \mathsf{U}_1 + \mathsf{T} \mathsf{U}_2 $$ and $$ (\mathsf{U}_1 + \mathsf{U}_2) \mathsf{T} = \mathsf{U}_1 \mathsf{T} + \mathsf{U}_2 \mathsf{T} $$  
(b) $$ \mathsf{T} (\mathsf{U}_1 \mathsf{U}_2) = (\mathsf{T} \mathsf{U}_1) \mathsf{U}_2 $$  
(c) $$ \mathsf{T} \mathsf{I} = \mathsf{I} \mathsf{T} = \mathsf{T} $$  
(d) $$ a (\mathsf{U}_1 \mathsf{U}_2) = (a \mathsf{U}_1) \mathsf{U}_2 = \mathsf{U}_1 (a \mathsf{U}_2) $$ for all scalars $$ a $$.

### Definition

Let $$ A $$ be an $$ m \times n $$ matrix and $$ B $$ be an $$ n \times p $$ matrix.
We define the product of $$ A $$ and $$ B $$, denoted $$ AB $$, to be the $$ m \times p $$ matrix such that

$$
\begin{align*}
    {(AB)}_{ij} = \sum_{k = 1}^{n} A_{ik} B_{kj} && \text{for } 1 \le i \le m, \ 1 \le j \le p
\end{align*}
$$

### Theorem 2.11

Let $$ \mathsf{V} $$, $$ \mathsf{W} $$, and $$ \mathsf{Z} $$ be finite-dimensional vector spaces with ordered basis $$ \alpha $$, $$ \beta $$, and $$ \gamma $$, respectively.
Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ and $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{Z} $$ be linear transformations.
Then

$$
\begin{align*}
    {[\mathsf{U} \mathsf{T}]}_{\alpha}^{\gamma} = {[\mathsf{U}]}_{\beta}^{\gamma} {[\mathsf{T}]}_{\alpha}^{\beta}
\end{align*}
$$

### Corollary

Let $$ \mathsf{V} $$ be a finite-dimensional vector space with an ordered basis $$ \beta $$.
Let $$ \mathsf{T}, \ \mathsf{U} \in \mathcal{L}(\mathsf{V}) $$.
Then $$ {[\mathsf{U} \mathsf{T}]}_{\beta} = {[\mathsf{U}]}_{\beta} {[\mathsf{T}]}_{\beta} $$.

### Theorem 2.12

Let $$ A $$ be an $$ m \times n $$ matrix, $$ B $$ and $$ C $$ be $$ n \times p $$ matrices, and $$ D $$ and $$ E $$ be $$ q \times m $$ matrices.
Then  
(a) $$ A(B + C) = AB + AC $$ and $$ (D + E)A = DA + EA $$.  
(b) $$ a(AB) = (aA)B = A(aB) $$ for any scalar $$ a $$.  
(c) $$ I_m A = A = A I_n $$.

**Proof**  
(a) We have

$$
\begin{align*}
    {[A(B + C)]}_{ij} &= \sum_{k = 1}^{n} A_{ik} {(B + C)}_{kj} = \sum_{k = 1}^{n} A_{ik} (B_{kj} + C_{kj}) \\
    &= \sum_{k = 1}^{n} (A_{ik} B_{kj} + A_{ik} C_{kj}) = \sum_{k = 1}^{n} A_{ik} B_{kj} + \sum_{k = 1}^{n} A_{ik} C_{kj} \\
    &= {(AB)}_{ij} + {(AC)}_{ij} = {[AB + AC]}_{ij}
\end{align*}
$$

So $$ A(B + C) = AB + AC $$.  
(c) We have

$$
\begin{align*}
    {(I_m A)}_{ij} = \sum_{k = 1}^{m} {(I_m)}_{ik} A_{kj} = \sum_{k = 1}^{m} \delta_{ik} A_{kj} = A_{ij}
\end{align*}
$$

The remainders of proofs are left as an exercise. $$ \blacksquare $$

### Corollary

Let $$ A $$ be an $$ m \times n $$ matrix, $$ B_1, \ B_2, \dots, B_k $$ be $$ n \times p $$ matrices, $$ C_1, \ C_2, \dots, C_k $$ be $$ q \times m $$ matrices, and $$ a_1, \ a_2, \dots, a_k $$ be scalars.
Then

$$
\begin{align*}
    A \left( \sum_{i = 1}^{k} a_i B_i \right) = \sum_{i = 1}^{k} a_i A B_i
\end{align*}
$$

and

$$
\begin{align*}
    \left( \sum_{i = 1}^{k} a_i C_i \right) A = \sum_{i = 1}^{k} a_i C_i A
\end{align*}
$$

### Theorem 2.13

Let $$ A $$ be an $$ m \times n $$ matrix and $$ B $$ an $$ n \times p $$ matrix.
For each $$ j $$, $$ j = 1, \ 2, \dots, p $$, let $$ u_j $$ and $$ v_j $$ denote the $$ j $$th columns of $$ AB $$ and $$ B $$, respectively.
Then  
(a) $$ u_j = A v_j $$  
(b) $$ v_j = B e_j $$, where $$ e_j $$ is the $$ j $$th standard vector of $$ {\mathsf{F}}^p $$.

**Proof**  
(a) We have

$$
\begin{align*}
    u_j =
    \begin{pmatrix}
        (AB)_{1j} \\
        (AB)_{2j} \\
        \vdots \\
        (AB)_{mj}
    \end{pmatrix}
    =
    \begin{pmatrix}
        \displaystyle \sum_{k = 1}^{n} A_{1k} B_{kj} \\
        \displaystyle \sum_{k = 1}^{n} A_{2k} B_{kj} \\
        \vdots \\
        \displaystyle \sum_{k = 1}^{n} A_{mk} B_{kj}
    \end{pmatrix}
    = A
    \begin{pmatrix}
        B_{1j} \\
        B_{2j} \\
        \vdots \\
        B_{nj}
    \end{pmatrix}
    = A v_j
\end{align*}
$$

The proof of (b) is similar to (a). $$ \blacksquare $$

### Theorem 2.14

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces having ordered bases $$ \beta $$ and $$ \gamma $$, respectively, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then, for each $$ u \in \mathsf{V} $$, we have

$$
\begin{align*}
    {[\mathsf{T}(u)]}_{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} {[u]}_{\beta}
\end{align*}
$$

**Proof**  
Fix $$ u \in \mathsf{V} $$, and define the linear transformations $$ f : F \rightarrow \mathsf{V} $$ by $$ f(a) = au $$ and $$ g : F \rightarrow \mathsf{W} $$ by $$ g(a) = a \mathsf{T}(u) $$ for all $$ a \in F $$.
Let $$ \alpha = \{ 1 \} $$ be the standard ordered basis for $$ F $$.
Notice that $$ g = \mathsf{T} f $$.
Identifying column vectors as matrices and using Theorem 2.11, we obtain

$$
\begin{align*}
    {[\mathsf{T}(u)]}_{\gamma} = {[g(1)]}_{\gamma} = {[g]}_{\alpha}^{\gamma} = {[\mathsf{T} f]}_{\alpha}^{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} {[f]}_{\alpha}^{\beta} = {[\mathsf{T}]}_{\beta}^{\gamma} {[f(1)]}_{\beta} = {[\mathsf{T}]}_{\beta}^{\gamma} {[u]}_{\beta}
\end{align*}
$$

### Definition

Let $$ A $$ be an $$ m \times n $$ matrix with entries from a field $$ F $$.
We denote by $$ {\mathsf{L}}_A $$ the mapping $$ {\mathsf{L}}_A : \mathsf{F}^n \rightarrow \mathsf{F}^m $$ defined by $$ \mathsf{L}_A(x) = Ax $$ (the matrix product of $$ A $$ and $$ x $$) for each column vector $$ x \in \mathsf{F}^n $$.
We call $$ \mathsf{L}_A $$ a left-multiplication transformation.

### Theorem 2.15

Let $$ A $$ be an $$ m \times n $$ matrix with entries from $$ F $$.
Then the left-multiplication transformation $$ \mathsf{L}_A : \mathsf{F}^n \rightarrow \mathsf{F}^m $$ is linear.
Furthermore, if $$ B $$ is any other $$ m \times n $$ matrix (with entries from $$ F $$) and $$ \beta $$ and $$ \gamma $$ are the standard ordered bases for $$ \mathsf{F}^n $$ and $$ \mathsf{F}^m $$, respectively, then we have the following properties.  
(a) $$ {[\mathsf{L}_A]}_{\beta}^{\gamma} = A $$.  
(b) $$ \mathsf{L}_A = \mathsf{L}_B $$ if and only if $$ A = B $$.  
(c) $$ \mathsf{L}_{A + B} = \mathsf{L}_A + \mathsf{L}_B $$ and $$ \mathsf{L}_{aA} = a \mathsf{L}_A $$ for all $$ a \in F $$.  
(d) If $$ \mathsf{T} : \mathsf{F}^n \rightarrow \mathsf{F}^m $$ is linear, then there exists a unique $$ m \times n $$ matrix $$ C $$ such that $$ \mathsf{T} = \mathsf{L}_C $$.
In fact, $$ C = {[\mathsf{T}]}_{\beta}^{\gamma} $$.  
(e) If $$ E $$ is an $$ n \times p $$ matrix, then $$ \mathsf{L}_{AE} = \mathsf{L}_A \mathsf{L}_E $$.  
(f) If $$ m = n $$, then $$ \mathsf{L}_{I_n} = \mathsf{I}_{\mathsf{F}^n} $$.

**Proof**  
The fact that $$ \mathsf{L}_A $$ is linear follows immediately from Theorem 2.12.  
(a) The $$ j $$th column of $$ {[\mathsf{L}_A]}_{\beta}^{\gamma} $$ is equal to $$ \mathsf{L}_A(e_j) $$.
However, $$ \mathsf{L}_A(e_j) = A e_j $$, which is also the $$ j $$th column of $$ A $$ by Theorem 2.13(b).
So $$ {[\mathsf{L}_A]}_{\beta}^{\gamma} = A $$.  
(b) If $$ \mathsf{L}_A = \mathsf{L}_B $$, then we may use (a) to write $$ A = {[\mathsf{L}_A]}_{\beta}^{\gamma} = {[\mathsf{L}_B]}_{\beta}^{\gamma} = B $$.
Hence $$ A = B $$.
The proof of the converse is trivial.  
(c) Exercise.  
(d) Let $$ C = {[\mathsf{T}]}_{\beta}^{\gamma} $$.
By Theorem 2.14, we have $$ {[\mathsf{T}(x)]}_{\gamma} = {[\mathsf{T}]}_{\beta}^{\gamma} {[x]}_{\beta} $$, or $$ \mathsf{T}(x) = Cx = \mathsf{L}_C(x) $$ for all $$ x \in F^n $$.
So $$ \mathsf{T} = \mathsf{L}_C $$.
The uniqueness of $$ C $$ follows from $$ (b) $$.  
(e) For any $$ j $$ ($$ 1 \le j \le p $$), we may apply Theorem 2.13 several times to note that $$ (AE)e_j $$ is the $$ j $$th column of $$ AE $$ and that the $$ j $$th column of $$ AE $$ is also equal to $$ A(E e_j) $$.
So $$ (AE)e_j = A(E e_j) $$.
Thus

$$
\begin{align*}
    \mathsf{L}_{AE}(e_j) = (AE)e_j = A(E e_j) = \mathsf{L}_A (E e_j) = \mathsf{L}_A (\mathsf{L}_E(e_j))
\end{align*}
$$

Hence $$ \mathsf{L}_{AE} = \mathsf{L}_A \mathsf{L}_E $$ by the corollary to Theorem 2.6.  
(f) Exercise. $$ \blacksquare $$

### Theorem 2.16

Let $$ A $$, $$ B $$, and $$ C $$ be matrices such that $$ A(BC) $$ is defined.
Then $$ (AB)C $$ is also defined and $$ A(BC) = (AB)C $$; that is, matrix multiplication is associative.

**Proof**  
Using (e) of Theorem 2.15 and the associativity of functional composition, we have

$$
\begin{align*}
    \mathsf{L}_{A(BC)} = \mathsf{L}_A \mathsf{L}_{BC} = \mathsf{L}_A (\mathsf{L}_B \mathsf{L}_C) = (\mathsf{L}_A \mathsf{L}_B) \mathsf{L}_C = \mathsf{L}_{AB} \mathsf{L}_C = \mathsf{L}_{(AB)C}
\end{align*}
$$

So from (b) of Theorem 2.15, it follows that $$ A(BC) = (AB)C $$. $$ \blacksquare $$

## 2.4 Invertibility and isomorphisms

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
A function $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{V} $$ is said to be an inverse of $$ \mathsf{T} $$ if $$ \mathsf{T} \mathsf{U} = \mathsf{I}_{\mathsf{W}} $$ and $$ \mathsf{U} \mathsf{T} = \mathsf{I}_{\mathsf{V}} $$.
If $$ \mathsf{T} $$ has an inverse, then $$ \mathsf{T} $$ is said to be invertible.
If $$ \mathsf{T} $$ is invertible, then the inverse of $$ \mathsf{T} $$ is unique and is denoted by $$ \mathsf{T}^{-1} $$.

The following facts hold for invertible functions $$ \mathsf{T} $$ and $$ \mathsf{U} $$.

1. $$ {(\mathsf{T} \mathsf{U})}^{-1} = \mathsf{U}^{-1} \mathsf{T}^{-1} $$.
2. $$ (\mathsf{T}^{-1})^{-1} = \mathsf{T} $$; in particular, $$ \mathsf{T}^{-1} $$ is invertible.
3. Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be a linear transformation, where $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional spaces of equal dimension.
Then $$ \mathsf{T} $$ is invertible if and only if rank($$ \mathsf{T} $$) $$ = \dim(\mathsf{V}) $$.

### Theorem 2.17

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear and invertible.
Then $$ \mathsf{T}^{-1} : \mathsf{W} \rightarrow \mathsf{V} $$ is linear.

**Proof**  
Let $$ y_1, \ y_2 \in \mathsf{W} $$ and $$ c \in F $$.
Since $$ \mathsf{T} $$ is onto and one-to-one, there exist unique vectors $$ x_1 $$ and $$ x_2 $$ such that $$ \mathsf{T}(x_1) = y_1 $$ and $$ \mathsf{T}(x_2) = y_2 $$.
Thus $$ x_1 = \mathsf{T}^{-1}(y_1) $$ and $$ x_2 = \mathsf{T}^{-1}(y_2) $$; so

$$
\begin{align*}
    \mathsf{T}^{-1}(cy_1 + y_2) &= \mathsf{T}^{-1}[c \mathsf{T}(x_1) + \mathsf{T}(x_2)] = \mathsf{T}^{-1} [\mathsf{T}(cx_1 + x_2)] \\
    &= cx_1 + x_2 = c \mathsf{T}^{-1}(y_1) + \mathsf{T}^{-1}(y_2)
\end{align*}
$$

### Corollary

Let $$ \mathsf{T} $$ be an invertible linear transformation from $$ \mathsf{V} $$ to $$ \mathsf{W} $$.
Then $$ \mathsf{V} $$ is finite-dimensional if and only if $$ \mathsf{W} $$ is finite-dimensional.
In this case, $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.

**Proof**  
Suppose that $$ \mathsf{V} $$ is finite-dimensional.
Let $$ \beta = \{ x_1, \ x_2, \dots, x_n \} $$ be a basis for $$ \mathsf{V} $$.
By Theorem 2.2, $$ \mathsf{T}(\beta) $$ spans $$ \mathsf{R}(\mathsf{T}) = \mathsf{W} $$; hence $$ \mathsf{W} $$ is finite-dimensional by Theorem 1.9.
Conversely, if $$ \mathsf{W} $$ is finite-dimensional, then so is $$ \mathsf{V} $$ by a similar argument, using $$ \mathsf{T}^{-1} $$.  
Now suppose that $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional.
Because $$ \mathsf{T} $$ is one-to-one and onto, we have

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) = 0 && \text{and} && \text{rank}(\mathsf{T}) = \dim(\mathsf{R}(\mathsf{T})) = \dim(\mathsf{W})
\end{align*}
$$

So by the dimension Theorem, it follows that $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$. $$ \blacksquare $$

### Definition

Let $$ A $$ be an $$ n \times n $$ matrix.
Then $$ A $$ is invertible if there exists an $$ n \times n $$ matrix $$ B $$ such that $$ AB = BA = I $$.

### Theorem 2.18

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces with ordered bases $$ \beta $$ and $$ \gamma $$, respectively.
Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{T} $$ is invertible if and only if $$ {[\mathsf{T}]}_{\beta}^{\gamma} $$ is invertible.
Furthermore, $$ {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} = {({[\mathsf{T}]}_{\beta}^{\gamma})}^{-1} $$.

**Proof**  
Suppose that $$ \mathsf{T} $$ is invertible.
By the corollary to Theorem 2.17, we have $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.
Let $$ n = \dim(\mathsf{V}) $$.
So $$ {[\mathsf{T}]}_{\beta}^{\gamma} $$ is an $$ n \times n $$ matrix.
Now $$ \mathsf{T}^{-1} : \mathsf{W} \rightarrow \mathsf{V} $$ satisfies $$ \mathsf{T} \mathsf{T}^{-1} = \mathsf{I}_{\mathsf{W}} $$ and $$ \mathsf{T}^{-1} \mathsf{T} = \mathsf{I}_{\mathsf{V}} $$.
Thus

$$
\begin{align*}
    I_n = {[\mathsf{I}_{\mathsf{V}}]}_{\beta} = {[\mathsf{T}^{-1} \mathsf{T}]}_{\beta} = {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} {[\mathsf{T}]}_{\beta}^{\gamma}
\end{align*}
$$

Similarly, $$ {[\mathsf{T}]}_{\beta}^{\gamma} {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} = I_n $$.
So $$ {[\mathsf{T}]}_{\beta}^{\gamma} $$ is invertible and $$ {\left( {[\mathsf{T}]}_{\beta}^{\gamma} \right)}^{-1} = {[\mathsf{T}^{-1}]}_{\gamma}^{\beta} $$.  
Now suppose that $$ A = {[\mathsf{T}]}_{\beta}^{\gamma} $$ is invertible.
Then there exists an $$ n \times n $$ matrix $$ B $$ such that $$ AB = BA = I_n $$.
By Theorem 2.6, there exists $$ U \in \mathcal{L}(\mathsf{W}, \ \mathsf{V}) $$ such that

$$
\begin{align*}
    \mathsf{U}(w_j) = \sum_{i = 1}^{n} B_{ij} v_i && \text{for } j = 1, \ 2, \dots, n
\end{align*}
$$

where $$ \gamma = \{ w_1, \ w_2, \dots, w_n \} $$ and $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$.
It follows that $$ {[\mathsf{U}]}_{\gamma}^{\beta} = B $$.
To show that $$ \mathsf{U} = \mathsf{T}^{-1} $$, observe that

$$
\begin{align*}
    {[\mathsf{U} \mathsf{T}]}_{\beta} = {[\mathsf{U}]}_{\gamma}^{\beta} {[\mathsf{T}]}_{\beta}^{\gamma} = BA = I_n = {[\mathsf{I}_{\mathsf{V}}]}_{\beta}
\end{align*}
$$

by Theorem 2.11.
So $$ \mathsf{U} \mathsf{T} = \mathsf{I}_{\mathsf{V}} $$, and similarly, $$ \mathsf{T} \mathsf{U} = \mathsf{I}_{\mathsf{W}} $$. $$ \blacksquare $$

### Corollary 1

Let $$ \mathsf{V} $$ be a finite-dimensional vector space with an ordered basis $$ \beta $$, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{V} $$ be linear.
Then $$ \mathsf{T} $$ is invertible if and only if $$ {[\mathsf{T}]}_{\beta} $$ is invertible.
Furthermore, $$ {[\mathsf{T}^{-1}]}_{\beta} = {\left( {[\mathsf{T}]}_{\beta} \right)}^{-1} $$.

### Corollary 2

Let $$ A $$ be an $$ n \times n $$ matrix.
Then $$ A $$ is invertible if and only if $$ \mathsf{L}_A $$ is invertible.
Furthermore, $$ {(\mathsf{L}_A)}^{-1} = \mathsf{L}_{A^{-1}} $$.

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces.
We say that $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{W} $$ if there exists a linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ that is invertible.
Such a linear transformation is called an isomorphism from $$ \mathsf{V} $$ onto $$ \mathsf{W} $$.

### Theorem 2.19

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces (over the same field).
Then $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{W} $$ if and only if $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.

**Proof**  
Suppose that $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{W} $$ and that $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ is an isomorphism from $$ \mathsf{V} $$ to $$ \mathsf{W} $$.
By the lemma preceding Theorem 2.18, we have that $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$.  
Now suppose that $$ \dim(\mathsf{V}) = \dim(\mathsf{W}) $$, and let $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ and $$ \gamma = \{ w_1, \ w_2, \dots, w_n \} $$ be bases for $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
By Theorem 2.6, there exists $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \mathsf{T} $$ is linear and $$ \mathsf{T}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.
Using Theorem 2.2, we have

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) = \text{span}(\mathsf{T}(\beta)) = \text{span}(\gamma) = \mathsf{W}
\end{align*}
$$

So $$ \mathsf{T} $$ is onto.
From Theorem 2.5, we have that $$ \mathsf{T} $$ is also one-to-one.
Hence $$ \mathsf{T} $$ is an isomorphism. $$ \blacksquare $$

By the lemma to Theorem 2.18, if $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are isomorphic, then either both of $$ \mathsf{V} $$ and $$ \mathsf{W} $$ are finite-dimensional or both are infinite-dimensional.

### Corollary

Let $$ \mathsf{V} $$ be a vector space over $$ F $$.
Then $$ \mathsf{V} $$ is isomorphic to $$ \mathsf{F}^n $$ if and only if $$ \dim(\mathsf{V}) = n $$.

### Theorem 2.20

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces over $$ F $$ of dimensions $$ n $$ and $$ m $$, respectively, and let $$ \beta $$ and $$ \gamma $$ be ordered bases for $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
Then the function $$ \Phi_{\beta}^{\gamma} : \mathcal{L}(\mathsf{V}, \ \mathsf{W}) \rightarrow \mathsf{M}_{m \times n}(F) $$, defined by $$ \Phi_{\beta}^{\gamma}(\mathsf{T}) = {[\mathsf{T}]}_{\beta}^{\gamma} $$ for $$ \mathsf{T} \in \mathcal{L}(\mathsf{V}, \ \mathsf{W}) $$, is an isomorphism.

**Proof**  
By Theorem 2.8, $$ \Phi_{\beta}^{\gamma} $$ is linear.
Hence we must show that $$ \Phi_{\beta}^{\gamma} $$ is one-to-one and onto.
This is accomplished if we show that for every $$ m \times n $$ matrix $$ A $$, there exists a unique linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \Phi_{\beta}^{\gamma}(\mathsf{T}) = A $$.
Let $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$, $$ \gamma = \{ w_1, \ w_2, \dots, w_m \} $$, and let $$ A $$ be a given $$ m \times n $$ matrix.
By Theorem 2.6, there exists a unique linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^{m} A_{ij} w_i && \text{for } 1 \le j \le n
\end{align*}
$$

But this means that $$ {[\mathsf{T}]}_{\beta}^{\gamma} = A $$, or $$ \Phi_{\beta}^{\gamma}(\mathsf{T}) = A $$.
So $$ \Phi_{\beta}^{\gamma} $$ is an isomorphism. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces of dimensions $$ n $$ and $$ m $$, respectively.
Then $$ \mathcal{L}(\mathsf{V}, \ \mathsf{W}) $$ is finite-dimensional of dimension $$ mn $$.

**Proof**  
The proof follows from Theorems 2.20 and 2.19 and the fact that $$ \dim(\mathsf{M}_{m \times n}(F)) = mn $$. $$ \blacksquare $$

### Definition

Let $$ \beta $$ be an ordered basis for an $$ n $$-dimensional vector space $$ \mathsf{V} $$ over the field $$ F $$.
The standard representation of $$ \mathsf{V} $$ with respect to $$ \beta $$ is the function $$ \Phi_{\beta} : \mathsf{V} \rightarrow \mathsf{F}^n $$ defined by $$ \phi_{\beta}(x) = {[x]}_{\beta} $$ for each $$ x \in \mathsf{V} $$.

### Theorem 2.21

For any finite-dimensional vector space $$ \mathsf{V} $$ with ordered basis $$ \beta $$, $$ \phi_{\beta} $$ is an isomorphism.

![Desktop View](/assets/img/Linear Algebra/2.-Linear-Transformations-and-Matrices/Figure 2.1.png){: width="700" }
_**Figure 2.1**_

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces of dimension $$ n $$ and $$ m $$, respectively, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be a linear transformation.
Define $$ A = {[\mathsf{T}]}_{\beta}^{\gamma} $$, where $$ \beta $$ and $$ \gamma $$ are arbitrary ordered bases of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
To study the relationship between the linear transformations $$ \mathsf{T} $$ and $$ \mathsf{L}_A : \mathsf{F}^n \rightarrow \mathsf{F}^m $$, first consider Figure 2.2.
Notice that there are two composites of linear transformations that map $$ \mathsf{V} $$ info $$ \mathsf{F}^m $$:

1. Map $$ \mathsf{V} $$ into $$ \mathsf{F}^n $$ with $$ \phi_{\beta} $$ and follow this transformation with $$ \mathsf{L}_A $$; this yields the composite $$ \mathsf{L}_A \phi_{\beta} $$.
2. Map $$ \mathsf{V} $$ into $$ \mathsf{W} $$ with $$ \mathsf{T} $$ and follow it by $$ \phi_{\gamma} $$ to obtain the composite $$ \phi_{\gamma} \mathsf{T} $$.

By a reformulation of Theorem 2.14, we may conclude that

$$
\begin{align*}
    \mathsf{L}_A \phi_{\beta} = \phi_{\gamma} \mathsf{T}
\end{align*}
$$

## 2.5 The change of coordinate matrix

### Theorem 2.22

Let $$ \beta $$ and $$ \beta' $$ be two ordered bases for a finite-dimensional vector space $$ \mathsf{V} $$, and let $$ Q = {[\mathsf{I}_{\mathsf{V}}]}_{\beta'}^{\beta} $$.
Then  
(a) $$ Q $$ invertible.  
(b) For any $$ v \in \mathsf{V} $$, $$ {[v]}_{\beta} = Q {[v]}_{\beta'} $$.

**Proof**  
(a) Since $$ \mathsf{I}_{\mathsf{V}} $$ is invertible, $$ Q $$ is invertible by Theorem 2.18.  
(b) For any $$ v \in \mathsf{V} $$,

$$
\begin{align*}
    {[v]}_{\beta} = {[\mathsf{I}_{\mathsf{V}}]}_{\beta} = {[\mathsf{I}_{\mathsf{V}}]}_{\beta'}^{\beta} {[v]}_{\beta'} = Q {[v]}_{\beta'}
\end{align*}
$$

by Theorem 2.14. $$ \blacksquare $$

Observe that if $$ \beta = \{ x_1, \ x_2, \dots, x_n \} $$ and $$ \beta' = \{ x_1', \ x_2', \dots, x_n' \} $$, then

$$
\begin{align*}
    x_j' = \sum_{i = 1}^{n} Q_{ij} x_i
\end{align*}
$$

for $$ j = 1, \ 2, \dots, n $$; that is, the $$ j $$th column of $$ Q $$ is $$ {[x_j']}_{\beta} $$.

### Theorem 2.23

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$, and let $$ \beta $$ and $$ \beta' $$ be ordered bases for $$ \mathsf{V} $$.
Suppose that $$ Q $$ is the change of coordinate matrix that changes $$ \beta' $$-coordinates into $$ \beta $$-coordinates.
Then

$$
\begin{align*}
    {[\mathsf{T}]}_{\beta'} = Q^{-1} {[\mathsf{T}]}_{\beta} Q
\end{align*}
$$

**Proof**  
Let $$ \mathsf{I} $$ be the identity transformation on $$ \mathsf{V} $$.
Then $$ \mathsf{T} = \mathsf{I} \mathsf{T} = \mathsf{T} \mathsf{I} $$; hence, by Theorem 2.11,

$$
\begin{align*}
    Q {[\mathsf{T}]}_{\beta'} = {[\mathsf{I}]}_{\beta'}^{\beta} {[\mathsf{T}]}_{\beta'}^{\beta'} = {[\mathsf{I} \mathsf{T}]}_{\beta'}^{\beta} = {[\mathsf{T} \mathsf{I}]}_{\beta'}^{\beta} = {[\mathsf{T}]}_{\beta}^{\beta} {[\mathsf{I}]}_{\beta'}^{\beta} = {[\mathsf{T}]}_{\beta} Q
\end{align*}
$$

Therefore $$ {[\mathsf{T}]}_{\beta'} = Q^{-1} {[\mathsf{T}]}_{\beta} Q $$. $$ \blacksquare $$

### Corollary

Let $$ A \in \mathsf{M}_{n \times n}(F) $$, and let $$ \gamma $$ be an ordered basis for $$ \mathsf{F}^n $$.
Then $$ {[\mathsf{L}_A]}_{\gamma} = Q^{-1} AQ $$, where $$ Q $$ is the $$ n \times n $$ matrix whose $$ j $$th column is the $$ j $$th vector of $$ \gamma $$.

### Definition

Let $$ A $$ and $$ B $$ be matrices in $$ \mathsf{M}_{n \times n}(F) $$.
We say that $$ B $$ is similar to $$ A $$ if there exists an invertible matrix $$ Q $$ such that $$ B = Q^{-1} AQ $$.

## 2.6 Dual spaces

Linear transformations from a vector space $$ \mathsf{V} $$ into its field of scalars $$ F $$, which is itself a vector space of dimension 1 over $$ F $$, are called linear functionals on $$ \mathsf{V} $$.

### Definition

For a vector space $$ \mathsf{V} $$ over $$ F $$, we define the dual space of $$ \mathsf{V} $$ to be the vector space $$ \mathcal{L}(\mathsf{V}, \ F) $$, denoted by $$ \mathsf{V}^* $$.

Thus $$ \mathsf{V}^* $$ is the vector space consisting of all linear functionals on $$ \mathsf{V} $$ with the operations of addition and scalar multiplication as defined in Section 2.2.
Note that if $$ \mathsf{V} $$ is finite-dimensional, then by the corollary to Theorem 2.20

$$
\begin{align*}
    \dim(\mathsf{V}^*) = \dim(\mathcal{L}(\mathsf{V}, \ F)) = \dim(\mathsf{V}) \cdot \dim(F) = \dim(V)
\end{align*}
$$

Hence by Theorem 2.19, $$ \mathsf{V} $$ and $$ \mathsf{V}^* $$ are isomorphic.

### Theorem 2.24

Suppose that $$ \mathsf{V} $$ is a finite-dimensional vector space with the ordered basis $$ \beta = \{ x_1, \ x_2, \dots, x_n \} $$.
Let $$ \mathsf{f}_i $$ ($$ 1 \le i \le n $$) be the $$ i $$th coordinate function with respect to $$ \beta $$ as just defined, and let $$ \beta^* = \{ \mathsf{f}_1, \ \mathsf{f}_2, \dots, \mathsf{f}_n \} $$.
Then $$ \beta^* $$ is an ordered basis for $$ \mathsf{V}^* $$, and, for any $$ \mathsf{f} \in \mathsf{V}^* $$, we have

$$
\begin{align*}
    \mathsf{f} = \sum_{i = 1}^{n} \mathsf{f}(x_1) \mathsf{f}_i
\end{align*}
$$

**Proof**  
Let $$ \mathsf{f} \in \mathsf{V}^* $$.
Since $$ \dim(\mathsf{V}^*) = n $$, we need only show that

$$
\begin{align*}
    \mathsf{f} = \sum_{i = 1}^{n} \mathsf{f}(x_i) \mathsf{f}_i
\end{align*}
$$

from which it follows that $$ \beta^* $$ generates $$ \mathsf{V}^* $$, and hence is a basis by corollary 2(a) to the replacement theorem.
Let

$$
\begin{align*}
    \mathsf{g} = \sum_{i = 1}^{n} \mathsf{f}(x_i) \mathsf{f}_i
\end{align*}
$$

For $$ 1 \le j \le n $$, we have

$$
\begin{align*}
    \mathsf{g}(x_j) &= \left( \sum_{i = 1}^{n} \mathsf{f}(x_i) \mathsf{f}_i \right)(x_j) = \sum_{i = 1}^{n} \mathsf{f}(x_i) \mathsf{f}_i(x_j) \\
    &= \sum_{i = 1}^{n} \mathsf{f}(x_i) \delta_{ij} = \mathsf{f}(x_j)
\end{align*}
$$

Therefore $$ \mathsf{f} = \mathsf{g} $$ by the corollary to Theorem 2.6. $$ \blacksquare $$

### Definition

Using the notation of Theorem 2.24, we call the ordered basis $$ \beta^* = \{ \mathsf{f}_1, \ \mathsf{f}_2, \dots, \mathsf{f}_n \} $$ of $$ \mathsf{V}^* $$ that satisfies $$ \mathsf{f}_i(x_j) = \delta_{ij} $$ ($$ 1 \le i, \ j \le n $$) the dual basis of $$ \beta $$.

### Theorem 2.25

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces over $$ F $$ with ordered bases $$ \beta $$ and $$ \gamma $$, respectively.
For any linear transformaion $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$, the mapping $$ \mathsf{T}^t : \mathsf{W}^* \rightarrow \mathsf{V}^* $$ defined by $$ \mathsf{T}^t(\mathsf{g}) = \mathsf{g} \mathsf{T} $$ for all $$ \mathsf{g} \in \mathsf{W}^* $$ is a linear transformation with the property that $$ {[\mathsf{T}^t]}_{\gamma^*}^{\beta^*} = {({[\mathsf{T}]}_{\beta}^{\gamma})}^t $$.

**Proof**  
For $$ \mathsf{g} \in \mathsf{W}^* $$, it is clear that $$ \mathsf{T}^t(\mathsf{g}) = \mathsf{g} \mathsf{T} $$ is a linear functional on $$ \mathsf{V} $$ and hence is in $$ \mathsf{V}^* $$.
Thus $$ \mathsf{T}^t $$ maps $$ \mathsf{W}^* $$ into $$ \mathsf{V}^* $$.
We leave the proof that $$ \mathsf{T}^t $$ is linear as an exercise.  
To complete the proof, let $$ \beta = \{ x_1, \ x_2, \dots, x_n \} $$ and $$ \gamma = \{ y_1, \ y_2, \dots, y_m \} $$ with dual bases $$ \beta^* = \{ \mathsf{f}_1, \ \mathsf{f}_2, \dots, \mathsf{f}_n \} $$ and $$ \gamma^* = \{ \mathsf{g}_1, \ \mathsf{g}_2, \dots, \mathsf{g}_m \} $$, respectively.
Let $$ A = {[\mathsf{T}]}_{\beta}^{\gamma} $$.
To find the $$ j $$th column of $$ {[\mathsf{T}^t]}_{\gamma^*}^{\beta^*} $$, we begin by expressing $$ \mathsf{T}^t(\mathsf{g}_j) $$ as a linear combination of the vectors of $$ \beta^* $$.
By Theorem 2.24, we have

$$
\begin{align*}
    \mathsf{T}^t (\mathsf{g}_j) = \mathsf{g}_j \mathsf{T} = \sum_{s = 1}^{n} (\mathsf{g}_j \mathsf{T}) (x_s) \mathsf{f}_s
\end{align*}
$$

So the row $$ i $$, column $$ j $$ entry of $$ {[\mathsf{T}^t]}_{\gamma^*}^{\beta^*} $$ is

$$
\begin{align*}
    (\mathsf{g}_j \mathsf{T}) (x_i) = \mathsf{g}_j (\mathsf{T}(x_i)) &= \mathsf{g}_j \left( \sum_{k = 1}^{m} A_{ki} y_k \right) \\
    &= \sum_{k = 1}^{m} A_{ki} \mathsf{g}_j (y_k) = \sum_{k = 1}^{m} A_{ki} \delta_{jk} = A_{jk}
\end{align*}
$$

Hence $$ {[\mathsf{T}^t]}_{\gamma^*}^{\beta^*} = A^t $$. $$ \blacksquare $$

Any finite-dimensional vector space $$ \mathsf{V} $$ can be identified in a natural way with its double dual $$ \mathsf{V}^{**} $$.
There is, in fact, an isomorphism between $$ \mathsf{V} $$ and $$ \mathsf{V}^{**} $$ that does not depend on any choice of bases for the two vector spaces.  
For a vector $$ x \in \mathsf{V} $$, we define $$ \widehat{x} : \mathsf{V}^* \rightarrow F $$ by $$ \widehat{x}(\mathsf{f}) = \mathsf{f}(x) $$ for every $$ \mathsf{f} \in \mathsf{V}^* $$.
It is easy to verify that $$ \widehat{x} $$ is a linear functional on $$ \mathsf{V}^* $$, so $$ \widehat{x} \in \mathsf{V}^{**} $$.
The correspondence $$ x \leftrightarrow \widehat{x} $$ allows us to define the desired isomorphism between $$ \mathsf{V} $$ and $$ \mathsf{V}^{**} $$.

### Lemma

Let $$ \mathsf{V} $$ be a finite-dimensional vector space, and let $$ x \in \mathsf{V} $$.
If $$ \widehat{x}(\mathsf{f}) = 0 $$ for all $$ \mathsf{f} \in \mathsf{V}^* $$, then $$ x = \mathit{0} $$.

**Proof**  
Let $$ x \neq \mathit{0} $$.
We show that there exists $$ \mathsf{f} \in \mathsf{V}^* $$ such that $$ \widehat{x}(\mathsf{f}) \neq 0 $$.
Choose an ordered basis $$ \beta = \{ x_1, \ x_2, \dots, x_n \} $$ for $$ \mathsf{V} $$ such that $$ x_1 = x $$.
Let $$ \{ \mathsf{f}_1, \ \mathsf{f}_2, \dots, \mathsf{f}_n \} $$ be the dual basis of $$ \beta $$.
Then $$ \mathsf{f}_1(x_1) = 1 \neq 0 $$.
Let $$ \mathsf{f} = \mathsf{f}_1 $$. $$ \blacksquare $$

### Theorem 2.26

Let $$ \mathsf{V} $$ be a finite-dimensional vector space, and define $$ \psi : \mathsf{V} \rightarrow \mathsf{V}^{**} $$ by $$ \psi(x) = \widehat{x} $$.
Then $$ \psi $$ is an isomorphism.

**Proof**  
(a) $$ \psi $$ is linear: Let $$ x, y \in \mathsf{V} $$ and $$ c \in F $$.
For $$ \mathsf{f} \in \mathsf{V}^* $$, we have

$$
\begin{align*}
    \psi(cx + y)(\mathsf{f}) &= \mathsf{f}(cx + y) = c \mathsf{f}(x) + \mathsf{f}(y) = c \widehat{x}(\mathsf{f}) + \widehat{y}(\mathsf{f}) \\
    &= (c \widehat{x} + \widehat{y})(\mathsf{f})
\end{align*}
$$

Therefore

$$
\begin{align*}
    \psi(cx + y) = c \widehat{x} + \widehat{y} = c \psi(x) + \psi(y)
\end{align*}
$$

(b) $$ \psi $$ is one-to-one: Suppose that $$ \psi(x) $$ is the zero functional on $$ \mathsf{V}^* $$ for some $$ x \in \mathsf{V} $$.
Then $$ \widehat{x}(\mathsf{f}) = 0 $$ for every $$ \mathsf{f} \in \mathsf{V}^* $$.
By the previous lemma, we conclude that $$ x = \mathit{0} $$.  
(c) $$ \psi $$ is an isomorphism: This follows from (b) and the fact that $$ \dim(\mathsf{V}) = \dim(\mathsf{V}^{**}) $$. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{V} $$ be a finite-dimensional vector space with dual space $$ \mathsf{V}^{**} $$.
Then every ordered basis for $$ \mathsf{V}^* $$ is the dual basis for some basis for $$ \mathsf{V} $$.

**Proof**  
Let $$ \{ \mathsf{f}_1, \ \mathsf{f}_2, \dots, \mathsf{f}_n \} $$ be an ordered basis for $$ \mathsf{V}^* $$.
We may combine Theorems 2.24 and 2.26 to conclude that for this basis for $$ \mathsf{V}^* $$ there exists a dual basis $$ \{ \widehat{x}_1, \ \widehat{x}_2, \dots, \widehat{x}_n \} $$ in $$ \mathsf{V}^{**} $$, that is, $$ \delta_{ij} = \widehat{x}_j (\mathsf{f}_j) = \mathsf{f}_j (x_i) $$ for all $$ i $$ and $$ j $$.
Thus $$ \{ \mathsf{f}_1, \ \mathsf{f}_2, \dots, \mathsf{f}_n \} $$ is the dual basis of $$ \{ x_1, \ x_2, \dots, x_n \} $$. $$ \blacksquare $$

## 2.7 Homogeneous linear differential equations with constant coefficients

### Definition

Given a function $$ x \in \mathcal{F}(R, \ C) $$ with real part $$ x_1 $$ and imaginary part $$ x_2 $$, we say that $$ x $$ is differentiable if $$ x_1 $$ and $$ x_2 $$ are differentiable.
If $$ x $$ is differentiable, we define the derivative $$ x' $$ of $$ x $$ by

$$
\begin{align*}
    x' = x'_1 + i x'_2
\end{align*}
$$

### Theorem 2.27

Any solution to a homogeneous linear differential equation with constant coefficients has derivatives of all orders; that is, if $$ x $$ is a solution to such an equation, then $$ x^{(k)} $$ exists for every positive integer $$ k $$.

### Definition

We use $$ \mathsf{C}^{\infty} $$ to denote the set of all functions in $$ \mathcal{F}(R, \ C) $$ that have derivatives of all orders.

### Definition

For any polynomial $$ p(t) $$ over $$ C $$ of positive degree, we call $$ p(\mathsf{D}) $$ a differential operator (with constant coefficients).
The order of the differential operator $$ p(\mathsf{D}) $$ is the degree of the polynomial $$ p(t) $$.

### Definition

Given the differential equation above, the complex polynomial

$$
\begin{align*}
    p(t) = t^n + a_{n - 1} t^{n - 1} + \cdots + a_1 t + a_0
\end{align*}
$$

is called the auxiliary polynomial associated with the equation.

### Theorem 2.28

The set of all solutions to a homogeneous linear differential equation with constant coefficients coincides with the null space of $$ p(\mathsf{D}) $$, where $$ p(t) $$ is the auxiliary polynomial associated with the equation.

### Corollary

The set of all solutions to a homogeneous linear differential equation with constant coefficients is a subspace of $$ \mathsf{C}^{\infty} $$.

### Definition

Let $$ c = a + ib $$ be a complex number with real part $$ a $$ and imaginary part $$ b $$.
Define

$$
\begin{align*}
    e^c = e^a(\cos{b} + i \sin{b})
\end{align*}
$$

The special case

$$
\begin{align*}
    e^{ib} = \cos{b} + i \sin{b}
\end{align*}
$$

is called Euler's formula.

### Definition

A function $$ f : R \rightarrow C $$ defined by $$ f(t) = e^{ct} $$ for a fixed complex number $$ c $$ is called an exponential function.

### Theorem 2.29

For any exponential function $$ f(t) = e^{ct} $$, $$ f'(t) = ce^{ct} $$.

We can use exponential functions to describe all solutions to a homogeneous linear differential equation of order 1.
An equation of order 1 is of the form

$$
\begin{align*}
    y' = a_0 y = \mathit{0}
\end{align*}
\label{eq:1}
\tag{2.1}
$$

### Theorem 2.30

The solution space for \eqref{eq:1} is of dimension 1 and has $$ \{ e^{-a_0 t} \} $$ as a basis.

**Proof**  
Clearly \eqref{eq:1} has $$ e^{-a_0 t} $$ as a solution.
Suppose that $$ x(t) $$ is any solution to \eqref{eq:1}.
Then

$$
\begin{align*}
    x'(t) = -a_0 x(t) && \text{for all } t \in R
\end{align*}
$$

Define

$$
\begin{align*}
    z(t) = e^{a_0 t} x(t)
\end{align*}
$$

Differentiating $$ z $$ yields

$$
\begin{align*}
    z'(t) = (e^{a_0 t})' x(t) + e^{a_0 t} x'(t) = a_0 e^{a_0 t} x(t) - a_0 e^{a_0 t} x(t) = \mathit{0}
\end{align*}
$$

Since $$ z' $$ is identically zero, $$ z $$ is a constant function.
Thus there exists a complex number $$ k $$ such that

$$
\begin{align*}
    z(t) = e^{a_0 t} x(t) = k && \text{for all } t \in R
\end{align*}
$$

So

$$
\begin{align*}
    x(t) = ke^{-a_0 t}
\end{align*}
$$

We conclude that any solution to \eqref{eq:1} is a scalar multiple of $$ e^{-a_0 t} $$. $$ \blacksquare $$

### Corollary

For any complex number $$ c $$, the null space of the differential operator $$ \mathsf{D} - c \mathsf{I} $$ has $$ \{ e^{ct} \} $$ as a basis.

### Theorem 2.31

Let $$ P(t) $$ be the auxiliary polynomial for a homogeneous linear differential equation with constant coefficients.
For any complex number $$ c $$, if $$ c $$ is a zero of $$ p(t) $$, then $$ e^{ct} $$ is a solution to the differential equation.

### Theorem 2.32

For any differential operator $$ p(\mathsf{D}) $$ of order $$ n $$, the null space of $$ p(\mathsf{D}) $$ is an $$ n $$-dimensional subspace of $$ \mathsf{C}^{\infty} $$.

### Lemma 1

The differential operator $$ \mathsf{D} - c \mathsf{I} : \mathsf{C}^{\infty} \rightarrow \mathsf{C}^{\infty} $$ is onto for any complex number $$ c $$.

**Proof**  
Let $$ v \in \mathsf{C}^{\infty} $$.
We wish to find a $$ u \in \mathsf{C}^{\infty} $$ such that $$ (\mathsf{D} - c \mathsf{I}) u = v $$.
Let $$ w(t) = v(t) e^{-ct} $$ for $$ t \in R $$.
Cleary, $$ w \in \mathsf{C}^{\infty} $$ because both $$ v $$ and $$ e^{-ct} $$ lie in $$ \mathsf{C}^{\infty} $$.
Let $$ w_1 $$ and $$ w_2 $$ be the real and imaginary parts of $$ w $$.
Then $$ w_1 $$ and $$ w_2 $$ are continuous because they are differentiable.
Hence they have antiderivatives, say, $$ W_1 $$ and $$ W_2 $$, respectively.
Let $$ W : R \rightarrow C $$ be defined by

$$
\begin{align*}
    W(t) = W_1(t) + iW_2(t) && \text{for } t \in R
\end{align*}
$$

Then $$ W \in \mathsf{C}^{\infty} $$, and the real and imaginary parts of $$ W $$ are $$ W_1 $$ and $$ W_2 $$, respectively.
Furthermore, $$ W' = w $$.
Finally, let $$ u : R \rightarrow C $$ be defined by $$ u(t) = W(t) e^{ct} $$ for $$ t \in R $$.
Cleary $$ u \in \mathsf{C}^{\infty} $$, and since

$$
\begin{align*}
    (\mathsf{D} - c \mathsf{I}) &= u'(t) - cu(t) \\
    &= W'(t) e^{ct} + W(t) ce^{ct} - cW(t) e^{ct} \\
    &= w(t) e^{ct} \\
    &= v(t) e^{-ct} e^{ct} \\
    &= v(t)
\end{align*}
$$

We have $$ (\mathsf{D} - c \mathsf{I}) u = v $$. $$ \blacksquare $$

### Lemma 2

Let $$ \mathsf{V} $$ be a vector space, and suppose that $$ \mathsf{T} $$ and $$ \mathsf{U} $$ are linear operators on $$ \mathsf{V} $$ such that $$ \mathsf{U} $$ is onto and the null spaces of $$ \mathsf{T} $$ and $$ \mathsf{U} $$ are finite-dimensional.
Then the null space of $$ \mathsf{T} \mathsf{U} $$ is finite-dimensional, and

$$
\begin{align*}
    \dim(\mathsf{N}(\mathsf{T} \mathsf{U})) = \dim(\mathsf{N}(\mathsf{T})) + \dim(\mathsf{N}(\mathsf{U}))
\end{align*}
$$

**Proof**  
Let $$ p = \dim(\mathsf{N}(\mathsf{T})) $$, $$ q = \dim(\mathsf{N}(\mathsf{U})) $$, and $$ \{ u_1, \ u_2, \dots, u_p \} $$ and $$ \{ v_1, \ v_2, \dots, v_q \} $$ be bases for $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{N}(\mathsf{U}) $$, respectively.
Since $$ \mathsf{U} $$ is onto, we can choose for each $$ i $$ ($$ 1 \le i \le p $$) a vector $$ w_i \in \mathsf{V} $$ such that $$ \mathsf{U}(w_i) = u_i $$.
Note that the $$ w_i $$'s are distinct.
Furthermore, for any $$ i $$ and $$ j $$, $$ w_i \neq v_j $$, for otherwise $$ u_i = \mathsf{U}(w_i) = \mathsf{U}(v_j) = \mathit{0} $$—a contradiction.
Hence the set

$$
\begin{align*}
    \beta = \{ w_1, \ w_2, \dots, w_p, \ v_1, \ v_2, \dots, v_q \}
\end{align*}
$$

contains $$ p + q $$ distinct vectors.
To complete the proof of the lemma, it suffices to show that $$ \beta $$ is a basis for $$ \mathsf{N}(\mathsf{T} \mathsf{U}) $$.  
We first show that $$ \beta $$ generates $$ \mathsf{N}(\mathsf{T} \mathsf{U}) $$.
Since for any $$ w_i $$ and $$ v_j $$ in $$ \beta $$, $$ \mathsf{T} \mathsf{U}(w_i) = \mathsf{T}(u_i) = \mathit{0} $$ and $$ \mathsf{T} \mathsf{U}(v_j) = \mathsf{T}(\mathit{0}) = \mathit{0} $$, it follows that $$ \beta \subseteq \mathsf{N} (\mathsf{T} \mathsf{U}) $$.
Now suppose that $$ v \in \mathsf{N}(\mathsf{T} \mathsf{U}) $$.
Then $$ \mathit{0} = \mathsf{T} \mathsf{U}(v) = \mathsf{T}(\mathsf{U}(v)) $$.
Thus $$ \mathsf{U}(v) \in \mathsf{N}(\mathsf{T}) $$.
So there exist scalars $$ a_1, \ a_2, \dots, a_p $$ such that

$$
\begin{align*}
    \mathsf{U}(v) &= a_1 u_1 + a_2 u_2 + \cdots + a_p u_p \\
    &= a_1 \mathsf{U}(w_1) + a_2 \mathsf{U}(w_2) + \cdots + a_p \mathsf{U}(w_p) \\
    &= \mathsf{U}(a_1 w_1 + a_2 w_2 + \cdots + a_p w_p)
\end{align*}
$$

Hence

$$
\begin{align*}
    \mathsf{U}(v - (a_1 w_1 + a_2 w_2 + \cdots + a_p w_p)) = \mathit{0}
\end{align*}
$$

Consequently, $$ v - (a_1 w_1 + a_2 w_2 + \cdots + a_p w_p) $$ lies in $$ \mathsf{N}(\mathsf{U}) $$.
It follows that there exist scalars $$ b_1, \ b_2, \dots, b_q $$ such that

$$
\begin{align*}
    
\end{align*}
$$