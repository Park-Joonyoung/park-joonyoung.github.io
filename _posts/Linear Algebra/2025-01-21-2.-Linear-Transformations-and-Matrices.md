---
title: 2. Linear Transformations and Matrices
# description: Short summary of the post
date: 2025-01-02 13:20
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, linear-transformation]     # TAG names should always be lowercase
math: true
pin: false
---

## 2.1 Linear transformations, null spaces, and ranges

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over the same field $$ F $$.
We call a function $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ a linear transformation from $$ \mathsf{V} $$ to $$ \mathsf{W} $$ if, for all $$ x, \ y \in \mathsf{V} $$ and $$ c \in F $$, we have  
(a) $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) $$ and  
(b) $$ \mathsf{T}(cx) = c \mathsf{T}(x) $$.

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
We define the null space (or kernel) $$ \mathsf{N}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the set of all vectors $$ x $$ in $$ \mathsf{V} $$ such that $$\mathsf{T}(x) = \mathit{0} $$; that is, $$ \mathsf{N}(\mathsf{T}) = \{ x \in \mathsf{V} : \mathsf{T}(x) = \mathit{0} \} $$.
We define the range (or image) $$ \mathsf{R}(\mathsf{T}) $$ of $$ \mathsf{T} $$ to be the subset of $$ \mathsf{W} $$ consisting of all images (under $$ \mathsf{T} $$) of vectors in $$ \mathsf{V} $$; that is, $$ \mathsf{R}(\mathsf{T}) = \{ \mathsf{T}(x) : x \in \mathsf{V} \} $$.

### Theorem 2.1

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are subspaces of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.

**Proof**  
We use the symbols $$ \mathit{0}_{\mathsf{V}} $$ and $$ \mathit{0}_{\mathsf{W}} $$ to denote the zero vectors of $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
Since $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_\mathsf{W} $$, we have that $$ \mathit{0}_{\mathsf{V}} \in \mathsf{N}(\mathsf{T}) $$.
Let $$ x, \ y \in \mathsf{N}(\mathsf{T}) $$ and $$ c \in F $$.
Then $$ \mathsf{T}(x + y) = \mathsf{T}(x) + \mathsf{T}(y) = \mathit{0}_{\mathsf{W}} + \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$, and $$ \mathsf{T}(cx) = c \mathsf{T}(x) = c \mathit{0}_{\mathsf{W}} = \mathit{0}_{\mathsf{W}} $$.
Hence $$ x + y \in \mathsf{N}(\mathsf{T}) $$ and $$ cx \in \mathsf{N}(\mathsf{T}) $$, so that $$ \mathsf{N}(\mathsf{T}) $$ is a subspace of $$ \mathsf{V} $$.  
Because $$ \mathsf{T}(\mathit{0}_{\mathsf{V}}) = \mathit{0}_{\mathsf{W}} $$, we have that $$ \mathit{0}_{\mathsf{W}} \in \mathsf{R}(\mathsf{T}) $$.
Now let $$ x, \ y \in \mathsf{R}(\mathsf{T}) $$ and $$ c \in F $$.
Then there exists $$ v $$ and $$ w $$ in $$ \mathsf{V} $$ such that $$ \mathsf{T}(v) = x $$ and $$ \mathsf{T}(w) = y $$.
So $$ \mathsf{T}(v + w) = \mathsf{T}(v) + \mathsf{T}(w) = x + y $$, and $$ \mathsf{T}(cv) = c \mathsf{T}(v) = cx $$.
Thus $$ x + y \in \mathsf{R}(\mathsf{T}) $$ and $$ cx \in \mathsf{R}(\mathsf{T}) $$, so $$ \mathsf{R}(\mathsf{T}) $$ is a subspace of $$ \mathsf{W} $$. $$ \blacksquare $$

### Theorem 2.2

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$, then

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) = \text{span}(\mathsf{T}(\beta)) = \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \})
\end{align*}
$$

**Proof**  
Clearly $$ \mathsf{T}(v_i) \in \mathsf{R}(\mathsf{T}) $$.
Then $$ w = \mathsf{T}(v) $$ for some $$ v \in \mathsf{V} $$.
Because $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have

$$
\begin{align*}
    v = \sum_{i = 1}^{n} a_i v_i && \text{for some } a_1, \ a_2, \dots, a_n \in F
\end{align*}
$$

Since $$ \mathsf{T} $$ is linear, it follows that

$$
\begin{align*}
    w = \mathsf{T}(v) = \sum_{i = 1}^{n} a_i \mathsf{T}(v_i) \in \text{span}(\mathsf{T}(\beta))
\end{align*}
$$

so $$ \mathsf{R}(\mathsf{T}) $$ is contained in span($$ \mathsf{T}(\beta) $$). $$ \blacksquare $$

### Definition

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$ are finite-dimensional, then we define the nullity of $$ \mathsf{T} $$ and the rank of $$ \mathsf{T} $$ to be the dimensions of $$ \mathsf{N}(\mathsf{T}) $$ and $$ \mathsf{R}(\mathsf{T}) $$, respectively.

### Theorem 2.3 (Dimension Theorem)

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
If $$ \mathsf{V} $$ is finite-dimensional, then

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

**Proof**  
Suppose that $$ \dim(\mathsf{V}) = n $$, $$ \dim(\mathsf{N}(\mathsf{T})) = k $$, and $$ \{ v_1, \ v_2, \dots, v_k \} $$ is a basis for $$ \mathsf{N}(\mathsf{T}) $$.
By the corollary to Theorem 1.11, we may extend $$ \{ v_1, \ v_2, \dots, v_k \} $$ to a basis $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ for $$ \mathsf{V} $$.
We claim that $$ S = \{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \} $$ is a basis for $$ \mathsf{R}(\mathsf{T}) $$.  
First we prove that $$ S $$ generates $$ \mathsf{R}(\mathsf{T}) $$.
Using Theorem 2.2 and the fact that $$ \mathsf{T}(v_i) = \mathit{0} $$ for $$ 1 \le i \le k $$, we have

$$
\begin{align*}
    \mathsf{R}(\mathsf{T}) &= \text{span}(\{ \mathsf{T}(v_1), \ \mathsf{T}(v_2), \dots, \mathsf{T}(v_n) \}) \\
    &= \text{span}(\{ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) \}) = \text{span}(S)
\end{align*}
$$

Now we prove that $$ S $$ is linearly independent.
Suppose that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i \mathsf{T}(v_i) = \mathit{0} && \text{for } b_{k + 1}, \ b_{k + 2}, \dots, b_n \in F
\end{align*}
$$

Using the fact that $$ \mathsf{T} $$ is linear, we have

$$
\begin{align*}
    \mathsf{T} \left( \sum_{i = k + 1}^{n} b_i v_i \right) = \mathit{0}
\end{align*}
$$

So

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i \in \mathsf{N}(\mathsf{T})
\end{align*}
$$

Hence there exist $$ c_1, \ c_2, \dots, c_k \in F $$ such that

$$
\begin{align*}
    \sum_{i = k + 1}^{n} b_i v_i = \sum_{i = 1}^{k} c_i v_i
\end{align*}
$$

or

$$
\begin{align*}
    \sum_{i = 1}^{k} (-c_i) v_i + \sum_{i = k + 1}^{n} b_i v_i = \mathit{0}
\end{align*}
$$

Since $$ \beta $$ is a basis for $$ \mathsf{V} $$, we have $$ b_i = 0 $$ for all $$ i $$.
Hence $$ S $$ is linearly independent.
This argument also shows that $$ \mathsf{T}(v_{k + 1}), \ \mathsf{T}(v_{k + 2}), \dots, \mathsf{T}(v_n) $$ are distinct; therefore rank($$ \mathsf{T} $$) $$ = n - k $$. $$ \blacksquare $$

### Theorem 2.4

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.

**Proof**  
Suppose that $$ \mathsf{T} $$ is one-to-one and $$ x \in \mathsf{N}(\mathsf{T}) $$.
Then $$ \mathsf{T}(x) = \mathit{0} = \mathsf{T}(\mathit{0}) $$.
Since $$ \mathsf{T} $$ is one-to-one, we have $$ x = \mathit{0} $$.
Hence $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.  
Now assume that $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, and suppose that $$ \mathsf{T}(x) = \mathsf{T}(y) $$.
Then $$ \mathit{0} = \mathsf{T}(x) - \mathsf{T}(y) = \mathsf{T}(x - y) $$.
Therefore $$ x - y \in \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$.
So $$ x - y = \mathit{0} $$, or $$ x = y $$.
This means $$ \mathsf{T} $$ is one-to-one. $$ \blacksquare $$

### Theorem 2.5

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be finite-dimensional vector spaces of equal dimension, and let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be linear.
Then the following are equivalent.  
(a) $$ \mathsf{T} $$ is one-to-one.  
(b) $$ \mathsf{T} $$ is onto.  
(c) rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{V}) $$

**Proof**  
From the dimension theorem, we have

$$
\begin{align*}
    \text{nullity}(\mathsf{T}) + \text{rank}(\mathsf{T}) = \dim(\mathsf{V})
\end{align*}
$$

Now, with the use of Theorem 2.4, we have that $$ \mathsf{T} $$ is one-to-one if and only if $$ \mathsf{N}(\mathsf{T}) = \{ \mathit{0} \} $$, if and only if nullity($$ \mathsf{T} $$) $$ = 0 $$, if and only if rank($$ \mathsf{T} $$) $$ = \dim(\mathsf{V}) $$, if and only if rank($$ \mathsf{T} $$) = $$ \dim(\mathsf{W}) $$, and if and only if $$ \dim(\mathsf{R}(\mathsf{T})) = \dim(\mathsf{W}) $$.
By Theorem 1.11, this equality is equivalent to $$ \mathsf{R}(\mathsf{T}) = \mathsf{W} $$, the definition of $$ \mathsf{T} $$ being onto. $$ \blacksquare $$

We note that if $$ \mathsf{V} $$ is not finite-dimensional and $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{V} $$ is linear, then it does not follow that one-to-one and onto are equivalent.

### Theorem 2.6

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces over $$ F $$, and suppose that $$ \{ v_1, \ v_2, \dots, v_n \} $$ is a basis for $$ \mathsf{V} $$.
For $$ w_1, \ w_2, \dots, w_n $$ in $$ \mathsf{W} $$, there exists exactly one linear transformation $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ such that $$ \mathsf{T}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.

**Proof**  
Let $$ x \in \mathsf{V} $$.
Then

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

where $$ a_1, \ a_2, \dots, a_n $$ are unique scalars.
Define $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ by

$$
\begin{align*}
    \mathsf{T}(x) = \sum_{i = 1}^{n} a_i w_i
\end{align*}
$$

(a) $$ \mathsf{T} $$ is linear: Suppose that $$ u, \ v \in \mathsf{V} $$ and $$ d \in F $$.
Then we may write

$$
\begin{align*}
    u = \sum_{i = 1}^{n} b_i v_i
\end{align*}
$$

and

$$
\begin{align*}
    v = \sum_{i = 1}^{n} c_i v_i
\end{align*}
$$

for some scalars $$ b_1, \ b_2, \dots, b_n, \ c_1, \ c_2, \dots, c_n $$.
Thus

$$
\begin{align*}
    du + v = \sum_{i = 1}^{n} (d b_i + c_i) v_i
\end{align*}
$$

So

$$
\begin{align*}
    \mathsf{T}(du + v) = \sum_{i = 1}^{n} (d b_i + c_i) w_i = d \sum_{i = 1}^{n} b_i w_i + \sum_{i = 1}^{n} c_i w_i = d \mathsf{T}(u) + \mathsf{T}(v)
\end{align*}
$$

(b) Clearly

$$
\begin{align*}
    \mathsf{T}(v_i) = w_i && \text{for } i = 1, \ 2, \dots, n
\end{align*}
$$

(c) $$ \mathsf{T} $$ is unique: Suppose that $$ \mathsf{U} : \mathsf{V} \rightarrow \mathsf{W} $$ is linear and $$ \mathsf{U}(v_i) = w_i $$ for $$ i = 1, \ 2, \dots, n $$.
Then for $$ x \in \mathsf{V} $$ with

$$
\begin{align*}
    x = \sum_{i = 1}^{n} a_i v_i
\end{align*}
$$

we have

$$
\begin{align*}
    \mathsf{U}(x) = \sum_{i = 1}^{n} a_i \mathsf{U}(v_i) = \sum_{i = 1}^{n} a_i w_i = \mathsf{T}(x)
\end{align*}
$$

Hence $$ \mathsf{U} = \mathsf{T} $$. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{V} $$ and $$ \mathsf{W} $$ be vector spaces, and suppose that $$ \mathsf{V} $$ has a finite basis $$ \{ v_1, \ v_2, \dots, v_n \} $$.
If $$ \mathsf{U}, \ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ are linear and $$ \mathsf{U}(v_i) = \mathsf{T}(v_i) $$ for $$ i = 1, \ 2, \dots, n $$, then $$ \mathsf{U} = \mathsf{T} $$.

## 2.2 The matrix representation of a linear transformation