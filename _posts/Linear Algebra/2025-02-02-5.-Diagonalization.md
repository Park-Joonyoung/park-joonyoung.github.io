---
title: 5. Diagonalization
# description: Short summary of the post
date: 2025-01-05 18:14
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, diagonalization, eigenvalue, eigenvector, markov-chain, cayley-hamilton-theorem]     # TAG names should always be lowercase
math: true
pin: false
---

## 5.1 Eigenvalues and eigenvectors

### Definition

A linear operator $$ \mathsf{T} $$ on a finite-dimensional vector space $$ \mathsf{V} $$ is called diagonalizable if there is an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ such that $$ {[\mathsf{T}]}_{\beta} $$ is a diagonal matrix.
A square matrix $$ A $$ is called diagonalizable if $$ \mathsf{L}_A $$ is diagonalizable.

Note that, if $$ D = {[\mathsf{T}]}_\beta $$ is a diagonal matrix, then for each vector $$ v_j \in \beta $$, we have

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^n D_{ij} v_i = D_{jj} v_j = \lambda_j v_j
\end{align*}
$$

where $$ \lambda_j = D_{jj} $$.  
Conversely, if $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis for $$ \mathsf{V} $$ such that $$ \mathsf{T}(v_j) = \lambda_j v_j $$ for some scalars $$ \lambda_1, \ \lambda_2, \dots, \lambda_n $$, then clearly

$$
\begin{align*}
    {[\mathsf{T}]}_\beta =
    \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix}
\end{align*}
$$

Each vector $$ v $$ in the basis $$ \beta $$ satisfies the condition that $$ \mathsf{T}(v) = \lambda v $$ for some scalar $$ \lambda $$.
Moreover, because $$ v $$ lies in a basis, $$ v $$ is nonzero.
These computations motivate the following definitions.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a vector space $$ \mathsf{V} $$.
A nonzero vector $$ v \in \mathsf{V} $$ is called an eigenvector of $$ \mathsf{T} $$ if there exists a scalar $$ \lambda $$ such that $$ \mathsf{T}(v) = \lambda v $$.
The scalar $$ \lambda $$ is called the eigenvalue corresponding to the eigenvector $$ v $$.  
Let $$ A $$ be in $$ \mathsf{M}_{n \times n}(F) $$.
A nonzero vector $$ v \in \mathsf{F}^n $$ is called an eigenvector of $$ A $$ if $$ v $$ is an eigenvector of $$ \mathsf{L}_A $$; that is, if $$ Av = \lambda v $$ for some scalar $$ \lambda $$.
The scalar $$ \lambda $$ is called the eigenvalue of $$ A $$ corresponding to the eigenvector $$ v $$.

### Theorem 5.1

A linear operator $$ \mathsf{T} $$ on a finite-dimensional vector space $$ \mathsf{V} $$ is diagonalizable if and only if there exists an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$.
Furthermore, if $$ \mathsf{T} $$ is diagonalizable, $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis of eigenvectors of $$ \mathsf{T} $$, and $$ D = {[\mathsf{T}]}_\beta $$, then $$ D $$ is a diagonal matrix and $$ D_{jj} $$ is the eigenvalue corresponding to $$ v_j $$ for $$ 1 \le j \le n $$.

### Corollary

A matrix $$ A \in \mathsf{M}_{n \times n}(F) $$ is diagonalizable if and only if there exists an ordered basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$.
Furthermore, if $$ \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$ and $$ Q $$ is the $$ n \times n $$ matrix whose $$ j $$th column is $$ v_j $$ for $$ j = 1, \ 2, \dots, n $$, then $$ D = Q^{-1} AQ $$ is a diagonal matrix such that $$ D_{jj} $$ is the eigenvalue of $$ A $$ corresponding to $$ v_j $$.
Hence $$ A $$ is diagonalizable if and only if it is similar to a diagonal matrix.

### Theorem 5.2

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.
Then a scalar $$ \lambda $$ is an eigenvalue of $$ A $$ if and only if $$ \det(A - \lambda I_n) = 0 $$.

**Proof**  
A scalar $$ \lambda $$ is an eigenvalue of $$ A $$ if and only if there exists a nonzero vector $$ v \in \mathsf{F}^n $$ such that $$ Av = \lambda v $$, that is, $$ (A - \lambda I_n) = \mathit{0} $$.
By Theorem 2.5, this is true if and only if $$ A - \lambda I_n $$ is not invertible.
However, this result is equivalent to the statement that $$ \det(A - \lambda I_n) = 0 $$. $$ \blacksquare $$

### Definition

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.
The polynomial $$ f(t) = \det(A - t I_n) $$ is called the characteristic polynomial of $$ A $$.

It is easily shown that similar matrices have the same determinant and the same characteristic polynomial.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$.
Choose any ordered basis $$ \beta $$ for $$ \mathsf{V} $$.
We define the determinant of $$ \mathsf{T} $$, denoted $$ \det(\mathsf{T}) $$, to be the determinant of $$ A = {[\mathsf{T}]}_\beta $$, and the characteristic polynomial $$ f(t) $$ of $$ \mathsf{T} $$ to be the characteristic polynomial of $$ A $$.
That is,

$$
\begin{align*}
    f(t) = \det(A - t I_n)
\end{align*}
$$

The remark preceding these definitions shows that they are independent of the choice of ordered basis $$ \beta $$.
Thus if $$ \mathsf{T} $$ is  a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$ and $$ \beta $$ is an ordered basis for $$ \mathsf{V} $$, then $$ \lambda $$ is an eigenvalue of $$ \mathsf{T} $$ if and only if $$ \lambda $$ is an eigenvalue of $$ {[\mathsf{T}]}_\beta $$.

### Theorem 5.3

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.  
(a) The characteristic polynomial of $$ A $$ is a polynomial of degree $$ n $$ with leading coefficient $$ {(-1)}^n $$.  
(b) $$ A $$ has at most $$ n $$ distinct eigenvalues.

### Theorem 5.4

Let $$ A \in \mathsf{M}_{n \times n}(F) $$, and let $$ \lambda $$ be an eigenvalue of $$ A $$.
Vector $$ v \in \mathsf{F}^n $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$ if and only if $$ v \neq \mathit{0} $$ and $$ (A - \lambda I)v = \mathit{0} $$.

Suppose that $$ \beta $$ is a basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$.
The corollary to Theorem 2.23, assures us that if $$ Q $$ is the $$ n \times n $$ matrix whose columns are the vectors in $$ \beta $$, then $$ Q^{-1} AQ $$ is a diagonal matrix.  

![Desktop View](/assets/img/Linear Algebra/5.-Diagonalization/Figure 5.1.png){: width="400" }
_**Figure 5.1**_

To find the eigenvectors of a linear operator $$ \mathsf{T} $$ on an $$ n $$-dimensional vector space, select an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ and let $$ A = {[\mathsf{T}]}_\beta $$.
Figure 5.1 is the special case of Figure 2.1 in Section 2.4 in which $$ \mathsf{V} = \mathsf{W} $$ and $$ \beta = \gamma $$.
Recall that for $$ v \in \mathsf{V} $$, $$ \phi_B(v) = {[v]}_\beta $$, the coordinate vector of $$ v $$ relative to $$ \beta $$.
We show that $$ v \in \mathsf{V} $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$ if and only if $$ \phi_B(v) $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$.
Suppose that $$ v $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.
Then $$ \mathsf{T}(v) = \lambda v $$.
Hence

$$
\begin{align*}
    A \phi_B(v) = {\mathsf{L}}_A \phi_B(v) = \phi_B(\lambda v) = \lambda \phi_B(v)
\end{align*}
$$

Now $$ \phi_B(v) \neq \mathit{0} $$, since $$ \phi_B $$ is an isomorphism; hence $$ \phi_B(v) $$ is an eigenvector of $$ A $$.
This argument is reversible, and so we can establish that if $$ \phi_B(v) $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$, then $$ v $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.  
An equivalent formulation of the result is that for an eigenvalue $$ \lambda $$ of $$ A $$ (and hence of $$ \mathsf{T} $$), a vector $$ y \in \mathsf{F}^n $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$ if and only if $$ {\phi_B}^{-1}(y) $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.

## 5.2 Diagonalizability

### Theorem 5.5

Let $$ \mathsf{T} $$ be a linear operator on a vector space, and let $$ \lambda_1, \lambda_2, \dots, \lambda_k $$ be distinct eigenvalues of $$ \mathsf{T} $$.
For each $$ i = 1, \ 2, \dots, k $$, let $$ S_i $$ be a finite set of eigenvectors of $$ \mathsf{T} $$ corresponding to $$ \lambda_i $$.
If each $$ S_i $$ $$ (i = 1, \ 2, \dots, k) $$ is linearly independent, then $$ S_1 \cup S_2 \cup \cdots \cup S_k $$ is linearly independent.

**Proof**  
The proof is by mathematical induction on $$ k $$.
If $$ k = 1 $$, there is nothing to prove.
So assume that the theorem holds for $$ k - 1 $$ distinct eigenvalues, where $$ k - 1 \ge 1 $$, and that we have $$ k $$ distinct eigenvalues $$ \lambda_1, \lambda_2, \dots, \lambda_k $$ of $$ \mathsf{T} $$.
For each $$ i = 1, \ 2, \dots, k $$, let $$ S_i = \{ v_{i1}, \ v_{i2}, \dots, v_{i n_i} \} $$ be a linearly independent set of eigenvectors of $$ \mathsf{T} $$ corresponding to $$ \lambda_i $$.  
Consider any scalars $$ \{ a_{ij} \} $$, where $$ i = 1, \ 2, \dots, k $$ and $$ j = 1, \ 2, \dots, n_i $$, such that

$$
\begin{align*}
    \sum_{i = 1}^k \sum_{j = 1}^{n_i} a_{ij} v_{ij} = \mathit{0}
\end{align*}
\label{eq:1}
\tag{5.1}
$$

Because $$ v_{ij} $$ is an eigenvector of $$ \mathsf{T} $$ of corresponding to $$ \lambda_i $$, applying $$ \mathsf{T} - \lambda_k \mathsf{I} $$ to both sides of \eqref{eq:1} yields

$$
\begin{align*}
    \sum_{i = 1}^{k - 1} \sum_{j = 1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} = \mathit{0}
\end{align*}
\label{eq:2}
\tag{5.2}
$$

But $$ S_1 \cup S_2 \cup \cdots \cup S_{k - 1} $$ is linearly independent by the induction hypothesis, so that \eqref{eq:2} implies $$ a_{ij} (\lambda_i - \lambda_k) = 0 $$ for $$ i = 1, \ 2, \dots, k - 1 $$ and $$ j = 1, \ 2, \dots, n_i $$.
Since $$ \lambda_1, \ \lambda_2, \dots, \lambda_k $$ are distinct, it follows that $$ \lambda_i - \lambda_k \neq 0 $$ for $$ 1 \le i \le k - 1 $$.
Hence $$ a_{ij} = 0 $$ for $$ i = 1, \ 2, \dots, k - 1 $$ and $$ j = 1, \ 2, \dots, n_i $$, and therefore \eqref{eq:1} reduces to $$ \sum_{j = 1}^{n_k} a_{kj} v_{kj} = \mathit{0} $$.
But $$ S_k $$ is also linearly independent, and so $$ a_{kj} = 0 $$ for $$ j = 1, \ 2, \dots, n_k $$.
Consequently $$ a_{ij} = 0 $$ for $$ i = 1, \ 2, \dots, k $$ and $$ j = 1, \ 2, \dots, n_i $$, proving that $$ S $$ is linearly independent. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{T} $$ be a linear operator on an $$ n $$-dimensional vector space $$ \mathsf{V} $$.
If $$ \mathsf{T} $$ has $$ n $$ distinct eigenvalues, then $$ \mathsf{T} $$ is diagonalizable.

**Proof**  
Suppose that $$ \mathsf{T} $$ has $$ n $$ eigenvalues $$ \lambda_1, \dots, \lambda_n $$.
For each $$ i $$ choose an eigenvector $$ v_i $$ corresponding to $$ \lambda_i $$.
By Theorem 5.5, $$ \{ v_1, \dots, v_n \} $$ is linearly independent, and since $$ \dim(\mathsf{V}) = n $$, this set is a basis for $$ \mathsf{V} $$.
Thus, by Theorem 5.1, $$ \mathsf{T} $$ is diagonalizable. $$ \blacksquare $$

The converse of the corollary to Theorem 5.5 is false.
That is, it is not true that if $$ \mathsf{T} $$ is diagonalizable, then it has $$ n $$ distinct eigenvalues.

### Definition

A polynomial $$ f(t) $$ in $$ \mathsf{P}(F) $$ splits over $$ F $$ if there are scalars $$ c, \ a_1, \dots, a_n $$ (not necessarily distinct) in $$ F $$ such that

$$
\begin{align*}
    f(t) = c(t - a_1)(t - a_2) \cdots (t - a_n)
\end{align*}
$$

### Theorem 5.6

The characteristic polynomial of any diagonalizable linear operator on a vector space $$ \mathsf{V} $$ over a field $$ F $$ splits over $$ F $$.

**Proof**  
Let $$ \mathsf{T} $$ be a diagonalizable linear operator on the $$ n $$-dimensional vector space $$ \mathsf{V} $$, and let $$ \beta $$ be an ordered basis for $$ \mathsf{V} $$ such that $$ {[\mathsf{T}]}_{\beta} = D $$ is a diagonal matrix.
Suppose that

$$
\begin{align*}
    D =
    \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix}
\end{align*}
$$

and let $$ f(t) $$ be the characteristic polynomial of $$ \mathsf{T} $$.
Then

$$
\begin{align*}
    f(t) &= \det(D - tI) = \det
    \begin{pmatrix}
        \lambda_1 - t & 0 & \cdots & 0 \\
        0 & \lambda_2 - t & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n - t
    \end{pmatrix} \\
    &= (\lambda_1 - t)(\lambda_2 - t) \cdots (\lambda_n - t) = {(-1)}^n (t - \lambda_1)(t - \lambda_2) \cdots (t - \lambda_n)
\end{align*}
$$

### Definition

Let $$ \lambda $$ be an eigenvalue of a linear operator or matrix with characteristic polynomial $$ f(t) $$.
The multiplicity (or algebraic multiplicity) of $$ \lambda $$ is the largest positive integer $$ k $$ for which $$ {(t - \lambda)}^k $$ is a factor of $$ f(t) $$.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a vector space $$ \mathsf{V} $$, and let $$ \lambda $$ be an eigenvalue of $$ \mathsf{T} $$.
Define $$ {\mathsf{E}}_\lambda = \{ x \in \mathsf{V} : \mathsf{T}(x) = \lambda x \} = \mathsf{N}(\mathsf{T} - \lambda {\mathsf{I}}_{\mathsf{V}}) $$.
The set $$ \mathsf{E}_\lambda $$ is called the eigenspace of $$ \mathsf{T} $$ corresponding to the eigenvalue $$ \lambda $$.
Analogously, we define the eigenspace of a square matrix $$ A $$ corresponding to the eigenvalue $$ \lambda $$ to be the eigenspace of $$ \mathsf{L}_A $$ corresponding to $$ \lambda $$.

### Theorem 5.7

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$, and let $$ \lambda $$ be an eigenvalue of $$ \mathsf{T} $$ having multiplicity $$ m $$.
Then $$ 1 \le \dim(\mathsf{E}_\lambda) \le m $$.

**Proof**  
Choose an ordered basis $$ \{ v_1, \ v_2, \dots, v_p \} $$ for $$ \mathsf{E}_\lambda $$, extend it to an ordered basis $$ \beta =\{ v_1, \ v_2, \dots, v_p, \ v_{p + 1}, \dots, v_n \} $$ for $$ \mathsf{V} $$, and let $$ A = {[\mathsf{T}]}_{\beta} $$.
Observe that $$ v_i $$ $$ (1 \le i \le p) $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$, and therefore

$$
\begin{align*}
    A =
    \begin{pmatrix}
        \lambda I_p & B \\
        O & C
    \end{pmatrix}
\end{align*}
$$

The characteristic polynomial of $$ \mathsf{T} $$ is

$$
\begin{align*}
    f(t) &= \det(A - t I_n) = \det
    \begin{pmatrix}
        (\lambda - t) I_p & B \\
        O & C - t I_{n - p}
    \end{pmatrix} \\
    &= \det((\lambda - t) I_p) \cdot \det(C - t I_{n - p}) \\
    &= (\lambda - t)^p g(t)
\end{align*}
$$

where $$ g(t) $$ is a polynomial.
Thus $$ {(\lambda - t)}^p $$ is a factor of $$ f(t) $$, and hence the multiplicity of $$ \lambda $$ is at least $$ p $$.
But $$ \dim(\mathsf{E}_\lambda) = p $$, and so $$ \dim(\mathsf{E}_\lambda) \le m $$. $$ \blacksquare $$

### Theorem 5.8

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$ such that the characteristic polynomial of $$ \mathsf{T} $$ splits.
Let $$ \lambda_1, \ \lambda_2, \dots, \lambda_k $$ be the distinct eigenvalues of $$ \mathsf{T} $$.
Then  
(a) $$ \mathsf{T} $$ is diagonalizable if and only if the multiplicity of $$ \lambda_i $$ is equal to $$ \dim(\mathsf{E}_\lambda) $$ for all $$ i $$.  
(b) If $$ \mathsf{T} $$ is diagonalizable and $$ \beta_i $$ is an ordered basis for $$ \mathsf{E}_\lambda $$, for each $$ i $$, then $$ \beta = \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k $$ is an ordered basis for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$. (We regard $$ \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k $$ as an ordered basis in the natural way—the vectors in $$ \beta_1 $$ are listed first (in the same order as in $$ \beta_1 $$), then the vectors in $$ \beta_2 $$ (in the same order as in $$ \beta_2 $$), etc.)

**Proof**  
For each $$ i $$, let $$ m_i $$ denote the multiplicity of $$ \lambda_i $$, $$ d_i = \dim(\mathsf{E}_{\lambda_i}) $$, and $$ n = \dim(\mathsf{V}) $$.
First, suppose that $$ \mathsf{T} $$ is diagonalizable.
Let $$ \beta $$ be a basis for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$.
For each $$ i $$, let $$ \beta_i = \beta \cap \mathsf{E}_{\lambda_i} $$, the set of vectors in $$ \beta $$ that are eigenvectors corresponding to $$ \lambda_i $$, and let $$ n_i $$ denote the number of vectors in $$ \beta_i $$.
Then $$ n_i \le d_i $$ for each $$ i $$ because $$ \beta_i $$ is a linearly independent subset of a subspace of dimension $$ d_i $$, and $$ d_i \le m_i $$ by Theorem 5.7.
The $$ n_i $$'s sum to $$ n $$ because $$ \beta $$ contains $$ n $$ vectors.
The $$ m_i $$'s also sum to $$ n $$ because the degree of the characteristic polynomial of $$ \mathsf{T} $$ is equal to the sum of the multiplicities of the eigenvalues.
Thus

$$
\begin{align*}
    n = \sum_{i = 1}^k n_i \le \sum_{i = 1}^k d_i \le \sum_{i = 1}^k m_i = n
\end{align*}
$$

It follows that

$$
\begin{align*}
    \sum_{i = 1}^k (m_i - d_i) = 0
\end{align*}
$$

Since $$ (m_i - d_i) \ge 0 $$ for all $$ i $$, we conclude that $$ m_i = d_i $$ for all $$ i $$.  
Conversely, suppose that $$ m_i = d_i $$ for all $$ i $$.
We simultaneously show that $$ \mathsf{T} $$ is diagonalizable and prove (b).
For each $$ i $$, let $$ \beta_i $$ be an ordered basis for $$ \mathsf{E}_\lambda $$, and let $$ \beta = \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k $$.
By Theorem 5.5, $$ \beta $$ is linearly independent.
Furthermore, since $$ d_i = m_i $$ for all $$ i $$, $$ \beta $$ contains

$$
\begin{align*}
    \sum_{i = 1}^k d_i = \sum_{i = 1}^k m_i = n
\end{align*}
$$

vectors.
Therefore $$ \beta $$ is an ordered basis for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{V} $$, and we conclude that $$ \mathsf{T} $$ is diagonalizable. $$ \blacksquare $$

### Test for diagonalizability

Let $$ \mathsf{T} $$ be a linear operator on an $$ n $$-dimensional vector space $$ \mathsf{V} $$.
Then $$ \mathsf{T} $$ is diagonalizable if and only if both of the following conditions hold.

1. The characteristic polynomial of $$ \mathsf{T} $$ splits.
2. For each eigenvalue $$ \lambda $$ of $$ \mathsf{T} $$, the multiplicity of $$ \lambda $$ equals $$ \text{nullity}(\mathsf{T} - \lambda \mathsf{I}) $$, that is, the multiplicity of $$ \lambda $$ equals $$ n - \text{rank}(\mathsf{T} - \lambda \mathsf{I}) $$.

These conditions can be used to test if a square matrix $$ A $$ is diagonalizable because diagonalizability of $$ A $$ is equivalent to diagonalizability of the operator $$ \mathsf{L}_A $$.  
If $$ \mathsf{T} $$ is a diagonalizable operator and $$ \beta_1, \beta_2, \dots, \beta_k $$ is an ordered bases for the eigenspaces of $$ \mathsf{T} $$, then the union $$ \beta = \beta_1 \cup \beta_2, \cup \cdots \cup \beta_k $$ is an ordered for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$, and hence $$ {[\mathsf{T}]}_\beta $$ is a diagonal matrix.  
When testing $$ \mathsf{T} $$ for diagonalizability, it is usually easiest to choose a convenient basis $$ \alpha $$ for $$ \mathsf{V} $$ and work with $$ B = {[\mathsf{T}]}_\alpha $$.
If the characteristic polynomial of $$ B $$ splits, then use condition 2 above to check if the multiplicity of each repeated eigenvalue of $$ B $$ equals $$ n - \text{rank}(B - \lambda I) $$.
(By Theorem 5.7, condition 2 is automatically satisfied for eigenvalues of multiplicity 1.)
If so, then $$ B $$, and hence $$ \mathsf{T} $$, is diagonalizable.  
If $$ \mathsf{T} $$ is diagonalizable and a basis $$ \beta $$ for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$ is desired, then we first find a basis for each eigenspace of $$ B $$.
The union of these bases is a basis $$ \gamma $$ for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ B $$.
Each vector in $$ \gamma $$ is the coordinate vector relative to $$ \alpha $$ of an eigenvector of $$ \mathsf{T} $$.
The set consisting of these $$ n $$ eigenvectors of $$ \mathsf{T} $$ is the desired basis $$ \beta $$.  
Furthermore, if $$ A $$ is an $$ n \times n $$ diagonalizable matrix, we can use the corollary to Theorem 2.23 to find an invertible $$ n \times n $$ matrix $$ Q $$ and a diagonal $$ n \times n $$ matrix $$ D $$ such that $$ Q^{-1} AQ = D $$.
The matrix $$ Q $$ has as its columns the vectors in a basis of eigenvectors of $$ A $$, and $$ D $$ has as its $$ j $$th diagonal entry the eigenvalue of $$ A $$ corresponding to the $$ j $$th column of $$ Q $$.

### Systems of differential equations

Consider the system of differential equations

$$
\begin{align*}
    {x_1}' &= 3x_1 + x_2 + x_3 \\
    {x_2}' &= 2x_1 + 4x_2 + 2x_3 \\
    {x_3}' &= -x_1 - x_2 + x_3 \\
\end{align*}
$$

where, for each $$ i $$, $$ x_i = x_i(t) $$ is a differentiable real-valued function of the real variable $$ t $$. Let $$ x : R \rightarrow \mathsf{R}^3 $$ be the function defined by


$$
\begin{align*}
    x'(t) =
    \begin{pmatrix}
        {x_1}'(t) \\
        {x_2}'(t) \\
        {x_3}'(t)
    \end{pmatrix}
\end{align*}
$$

Let

$$
\begin{align*}
    A =
    \begin{pmatrix}
        3 & 1 & 1 \\
        2 & 4 & 2 \\
        -1 & -1 & 1
    \end{pmatrix}
\end{align*}
$$

be the coefficient matrix of the given system, so that we can rewrite the system as the matrix equation $$ x' = Ax $$.
It can be verified that for

$$
\begin{align*}
    Q =
    \begin{pmatrix}
        -1 & 0 & -1 \\
        0 & -1 & -2 \\
        1 & 1 & 1
    \end{pmatrix}
    && \text{and} && D =
    \begin{pmatrix}
        2 & 0 & 0 \\
        0 & 2 & 0 \\
        0 & 0 & 4
    \end{pmatrix}
\end{align*}
$$

we have $$ Q^{-1} AQ = D $$.
Substitute $$ A = QDQ^{-1} $$ into $$ x' = Ax $$ to obtain $$ x' = QDQ^{-1} x $$ or, equivalently, $$ Q^{-1} x' = DQ^{-1} x $$.
The function $$ y : R \rightarrow \mathsf{R}^3 $$ defined by $$ y(t) = Q^{-1} x(t) $$ can be shown to be differentiable, and $$ y' = Q^{-1} x' $$.
Hence the original system can be written as $$ y' = Dy $$.  
Since $$ D $$ is a diagonal matrix, the system $$ y' = Dy $$ is easy to solve.
Setting

$$
\begin{align*}
    y(t) =
    \begin{pmatrix}
        y_1(t) \\
        y_2(t) \\
        y_3(t)
    \end{pmatrix}
\end{align*}
$$

we can rewrite $$ y' = Dy $$ as

$$
\begin{align*}
    \begin{pmatrix}
        {y_1}'(t) \\
        {y_2}'(t) \\
        {y_3}'(t)
    \end{pmatrix}
    =
    \begin{pmatrix}
        2 & 0 & 0 \\
        0 & 2 & 0 \\
        0 & 0 & 4 \\
    \end{pmatrix}
    \begin{pmatrix}
        y_1(t) \\
        y_2(t) \\
        y_3(t)
    \end{pmatrix}
    =
    \begin{pmatrix}
        2 y_1(t) \\
        2 y_2(t) \\
        4 y_3(t)
    \end{pmatrix}
\end{align*}
$$

The three equations

$$
\begin{align*}
    {y_1}' = 2 y_1 \\
    {y_2}' = 2 y_2 \\
    {y_3}' = 4 y_3
\end{align*}
$$

are independent of each other, and thus can be solved individually.
It is easily seen that the general solution to these equations is $$ y_1(t) = c_1 e^{2t} $$, $$ y_2(t) = c_2 e^{2t} $$, and $$ y_3(t) = c_3 e^{4t} $$, where $$ c_1 $$, $$ c_2 $$, and $$ c_3 $$ are arbitrary constants.
Finally,

$$
\begin{align*}
    \begin{pmatrix}
        x_1(t) \\
        x_2(t) \\
        x_3(t)
    \end{pmatrix}
    &= x(t) = Qy(t) =
    \begin{pmatrix}
        -1 & 0 & -1 \\
        0 & -1 & -2 \\
        1 & 1 & 1
    \end{pmatrix}
    \begin{pmatrix}
        c_1 e^{2t} \\
        c_2 e^{2t} \\
        c_3 e^{4t}
    \end{pmatrix} \\
    &=
    \begin{pmatrix}
        -c_1 e^{2t} - c_3 e^{4t} \\
        -c_2 e^{2t} - 2 c_3 e^{4t} \\
        c_1 e^{2t} + c_2 e^{2t} + c_3 e^{4t}
    \end{pmatrix}
\end{align*}
$$

yields the general solution of the original system.
Note that this solution can be written as

$$
\begin{align*}
    x(t) = e^{2t} \left[ c_1
    \begin{pmatrix}
        -1 \\
        0 \\
        1
    \end{pmatrix}
    + c_2
    \begin{pmatrix}
        0 \\
        -1 \\
        1
    \end{pmatrix}
    \right] + e^{4t} \left[ c_3
    \begin{pmatrix}
        -1 \\
        -2 \\
        1
    \end{pmatrix}
    \right]
\end{align*}
$$

The expressions in brackets are arbitrary vectors in $$ \mathsf{E}_{\lambda_1} $$ and $$ \mathsf{E}_{\lambda_2} $$, respectively, where $$ \lambda_1 = 2 $$ and $$ \lambda_2 = 4 $$.
Thus the general solution of the original system is $$ x(t) = e^{2t} z_1 + e^{4t} z_2 $$, where $$ z_1 \in \mathsf{E}_{\lambda_1} $$ and $$ z_2 \in \mathsf{E}_{\lambda_2} $$.