---
title: 5. Diagonalization
# description: Short summary of the post
date: 2025-01-05 18:14
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, diagonalization, eigenvalue, eigenvector, markov-chain, cayley-hamilton-theorem]     # TAG names should always be lowercase
math: true
pin: false
---

## 5.1 Eigenvalues and eigenvectors

### Definition

A linear operator $$ \mathsf{T} $$ on a finite-dimensional vector space $$ \mathsf{V} $$ is called diagonalizable if there is an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ such that $$ {[\mathsf{T}]}_{\beta} $$ is a diagonal matrix.
A square matrix $$ A $$ is called diagonalizable if $$ \mathsf{L}_A $$ is diagonalizable.

Note that, if $$ D = {[\mathsf{T}]}_\beta $$ is a diagonal matrix, then for each vector $$ v_j \in \beta $$, we have

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^n D_{ij} v_i = D_{jj} v_j = \lambda_j v_j
\end{align*}
$$

where $$ \lambda_j = D_{jj} $$.  
Conversely, if $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis for $$ \mathsf{V} $$ such that $$ \mathsf{T}(v_j) = \lambda_j v_j $$ for some scalars $$ \lambda_1, \ \lambda_2, \dots, \lambda_n $$, then clearly

$$
\begin{align*}
    {[\mathsf{T}]}_\beta =
    \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix}
\end{align*}
$$

Each vector $$ v $$ in the basis $$ \beta $$ satisfies the condition that $$ \mathsf{T}(v) = \lambda v $$ for some scalar $$ \lambda $$.
Moreover, because $$ v $$ lies in a basis, $$ v $$ is nonzero.
These computations motivate the following definitions.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a vector space $$ \mathsf{V} $$.
A nonzero vector $$ v \in \mathsf{V} $$ is called an eigenvector of $$ \mathsf{T} $$ if there exists a scalar $$ \lambda $$ such that $$ \mathsf{T}(v) = \lambda v $$.
The scalar $$ \lambda $$ is called the eigenvalue corresponding to the eigenvector $$ v $$.  
Let $$ A $$ be in $$ \mathsf{M}_{n \times n}(F) $$.
A nonzero vector $$ v \in \mathsf{F}^n $$ is called an eigenvector of $$ A $$ if $$ v $$ is an eigenvector of $$ \mathsf{L}_A $$; that is, if $$ Av = \lambda v $$ for some scalar $$ \lambda $$.
The scalar $$ \lambda $$ is called the eigenvalue of $$ A $$ corresponding to the eigenvector $$ v $$.

### Theorem 5.1

A linear operator $$ \mathsf{T} $$ on a finite-dimensional vector space $$ \mathsf{V} $$ is diagonalizable if and only if there exists an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$.
Furthermore, if $$ \mathsf{T} $$ is diagonalizable, $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis of eigenvectors of $$ \mathsf{T} $$, and $$ D = {[\mathsf{T}]}_\beta $$, then $$ D $$ is a diagonal matrix and $$ D_{jj} $$ is the eigenvalue corresponding to $$ v_j $$ for $$ 1 \le j \le n $$.

### Corollary

A matrix $$ A \in \mathsf{M}_{n \times n}(F) $$ is diagonalizable if and only if there exists an ordered basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$.
Furthermore, if $$ \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$ and $$ Q $$ is the $$ n \times n $$ matrix whose $$ j $$th column is $$ v_j $$ for $$ j = 1, \ 2, \dots, n $$, then $$ D = Q^{-1} AQ $$ is a diagonal matrix such that $$ D_{jj} $$ is the eigenvalue of $$ A $$ corresponding to $$ v_j $$.
Hence $$ A $$ is diagonalizable if and only if it is similar to a diagonal matrix.

### Theorem 5.2

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.
Then a scalar $$ \lambda $$ is an eigenvalue of $$ A $$ if and only if $$ \det(A - \lambda I_n) = 0 $$.

**Proof**  
A scalar $$ \lambda $$ is an eigenvalue of $$ A $$ if and only if there exists a nonzero vector $$ v \in \mathsf{F}^n $$ such that $$ Av = \lambda v $$, that is, $$ (A - \lambda I_n) = \mathit{0} $$.
By Theorem 2.5, this is true if and only if $$ A - \lambda I_n $$ is not invertible.
However, this result is equivalent to the statement that $$ \det(A - \lambda I_n) = 0 $$. $$ \blacksquare $$

### Definition

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.
The polynomial $$ f(t) = \det(A - t I_n) $$ is called the characteristic polynomial of $$ A $$.

It is easily shown that similar matrices have the same determinant and the same characteristic polynomial.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$.
Choose any ordered basis $$ \beta $$ for $$ \mathsf{V} $$.
We define the determinant of $$ \mathsf{T} $$, denoted $$ \det(\mathsf{T}) $$, to be the determinant of $$ A = {[\mathsf{T}]}_\beta $$, and the characteristic polynomial $$ f(t) $$ of $$ \mathsf{T} $$ to be the characteristic polynomial of $$ A $$.
That is,

$$
\begin{align*}
    f(t) = \det(A - t I_n)
\end{align*}
$$

The remark preceding these definitions shows that they are independent of the choice of ordered basis $$ \beta $$.
Thus if $$ \mathsf{T} $$ is  a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$ and $$ \beta $$ is an ordered basis for $$ \mathsf{V} $$, then $$ \lambda $$ is an eigenvalue of $$ \mathsf{T} $$ if and only if $$ \lambda $$ is an eigenvalue of $$ {[\mathsf{T}]}_\beta $$.

### Theorem 5.3

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.  
(a) The characteristic polynomial of $$ A $$ is a polynomial of degree $$ n $$ with leading coefficient $$ {(-1)}^n $$.  
(b) $$ A $$ has at most $$ n $$ distinct eigenvalues.

### Theorem 5.4

Let $$ A \in \mathsf{M}_{n \times n}(F) $$, and let $$ \lambda $$ be an eigenvalue of $$ A $$.
Vector $$ v \in \mathsf{F}^n $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$ if and only if $$ v \neq \mathit{0} $$ and $$ (A - \lambda I)v = \mathit{0} $$.

Suppose that $$ \beta $$ is a basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$.
The corollary to Theorem 2.23, assures us that if $$ Q $$ is the $$ n \times n $$ matrix whose columns are the vectors in $$ \beta $$, then $$ Q^{-1} AQ $$ is a diagonal matrix.  

![Desktop View](/assets/img/Linear Algebra/5.-Diagonalization/Figure 5.1.png){: width="400" }
_**Figure 5.1**_

To find the eigenvectors of a linear operator $$ \mathsf{T} $$ on an $$ n $$-dimensional vector space, select an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ and let $$ A = {[\mathsf{T}]}_\beta $$.
Figure 5.1 is the special case of Figure 2.1 in Section 2.4 in which $$ \mathsf{V} = \mathsf{W} $$ and $$ \beta = \gamma $$.
Recall that for $$ v \in \mathsf{V} $$, $$ \phi_B(v) = {[v]}_\beta $$, the coordinate vector of $$ v $$ relative to $$ \beta $$.
We show that $$ v \in \mathsf{V} $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$ if and only if $$ \phi_B(v) $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$.
Suppose that $$ v $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.
Then $$ \mathsf{T}(v) = \lambda v $$.
Hence

$$
\begin{align*}
    A \phi_B(v) = {\mathsf{L}}_A \phi_B(v) = \phi_B(\lambda v) = \lambda \phi_B(v)
\end{align*}
$$

Now $$ \phi_B(v) \neq \mathit{0} $$, since $$ \phi_B $$ is an isomorphism; hence $$ \phi_B(v) $$ is an eigenvector of $$ A $$.
This argument is reversible, and so we can establish that if $$ \phi_B(v) $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$, then $$ v $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.  
An equivalent formulation of the result is that for an eigenvalue $$ \lambda $$ of $$ A $$ (and hence of $$ \mathsf{T} $$), a vector $$ y \in \mathsf{F}^n $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$ if and only if $$ {\phi_B}^{-1}(y) $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.

## 5.2 Diagonalizability

### Theorem 5.5

Let $$ \mathsf{T} $$ be a linear operator on a vector space, and let $$ \lambda_1, \lambda_2, \dots, \lambda_k $$ be distinct eigenvalues of $$ \mathsf{T} $$.
For each $$ i = 1, \ 2, \dots, k $$, let $$ S_i $$ be a finite set of eigenvectors of $$ \mathsf{T} $$ corresponding to $$ \lambda_i $$.
If each $$ S_i $$ $$ (i = 1, \ 2, \dots, k) $$ is linearly independent, then $$ S_1 \cup S_2 \cup \cdots \cup S_k $$ is linearly independent.

**Proof**  
The proof is by mathematical induction on $$ k $$.
If $$ k = 1 $$, there is nothing to prove.
So assume that the theorem holds for $$ k - 1 $$ distinct eigenvalues, where $$ k - 1 \ge 1 $$, and that we have $$ k $$ distinct eigenvalues $$ \lambda_1, \lambda_2, \dots, \lambda_k $$ of $$ \mathsf{T} $$.
For each $$ i = 1, \ 2, \dots, k $$, let $$ S_i = \{ v_{i1}, \ v_{i2}, \dots, v_{i n_i} \} $$ be a linearly independent set of eigenvectors of $$ \mathsf{T} $$ corresponding to $$ \lambda_i $$.  
Consider any scalars $$ \{ a_{ij} \} $$, where $$ i = 1, \ 2, \dots, k $$ and $$ j = 1, \ 2, \dots, n_i $$, such that

$$
\begin{align*}
    \sum_{i = 1}^k \sum_{j = 1}^{n_i} a_{ij} v_{ij} = \mathit{0}
\end{align*}
\label{eq:1}
\tag{5.1}
$$

