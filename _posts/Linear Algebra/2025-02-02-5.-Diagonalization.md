---
title: 5. Diagonalization
# description: Short summary of the post
date: 2025-01-05 18:14
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, diagonalization, eigenvalue, eigenvector, markov-chain, cayley-hamilton-theorem]     # TAG names should always be lowercase
math: true
pin: false
---

## 5.1 Eigenvalues and eigenvectors

### Definition

A linear operator $$ \mathsf{T} $$ on a finite-dimensional vector space $$ \mathsf{V} $$ is called diagonalizable if there is an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ such that $$ {[\mathsf{T}]}_{\beta} $$ is a diagonal matrix.
A square matrix $$ A $$ is called diagonalizable if $$ \mathsf{L}_A $$ is diagonalizable.

Note that, if $$ D = {[\mathsf{T}]}_\beta $$ is a diagonal matrix, then for each vector $$ v_j \in \beta $$, we have

$$
\begin{align*}
    \mathsf{T}(v_j) = \sum_{i = 1}^n D_{ij} v_i = D_{jj} v_j = \lambda_j v_j
\end{align*}
$$

where $$ \lambda_j = D_{jj} $$.  
Conversely, if $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis for $$ \mathsf{V} $$ such that $$ \mathsf{T}(v_j) = \lambda_j v_j $$ for some scalars $$ \lambda_1, \ \lambda_2, \dots, \lambda_n $$, then clearly

$$
\begin{align*}
    {[\mathsf{T}]}_\beta =
    \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix}
\end{align*}
$$

Each vector $$ v $$ in the basis $$ \beta $$ satisfies the condition that $$ \mathsf{T}(v) = \lambda v $$ for some scalar $$ \lambda $$.
Moreover, because $$ v $$ lies in a basis, $$ v $$ is nonzero.
These computations motivate the following definitions.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a vector space $$ \mathsf{V} $$.
A nonzero vector $$ v \in \mathsf{V} $$ is called an eigenvector of $$ \mathsf{T} $$ if there exists a scalar $$ \lambda $$ such that $$ \mathsf{T}(v) = \lambda v $$.
The scalar $$ \lambda $$ is called the eigenvalue corresponding to the eigenvector $$ v $$.  
Let $$ A $$ be in $$ \mathsf{M}_{n \times n}(F) $$.
A nonzero vector $$ v \in \mathsf{F}^n $$ is called an eigenvector of $$ A $$ if $$ v $$ is an eigenvector of $$ \mathsf{L}_A $$; that is, if $$ Av = \lambda v $$ for some scalar $$ \lambda $$.
The scalar $$ \lambda $$ is called the eigenvalue of $$ A $$ corresponding to the eigenvector $$ v $$.

### Theorem 5.1

A linear operator $$ \mathsf{T} $$ on a finite-dimensional vector space $$ \mathsf{V} $$ is diagonalizable if and only if there exists an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$.
Furthermore, if $$ \mathsf{T} $$ is diagonalizable, $$ \beta = \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis of eigenvectors of $$ \mathsf{T} $$, and $$ D = {[\mathsf{T}]}_\beta $$, then $$ D $$ is a diagonal matrix and $$ D_{jj} $$ is the eigenvalue corresponding to $$ v_j $$ for $$ 1 \le j \le n $$.

### Corollary

A matrix $$ A \in \mathsf{M}_{n \times n}(F) $$ is diagonalizable if and only if there exists an ordered basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$.
Furthermore, if $$ \{ v_1, \ v_2, \dots, v_n \} $$ is an ordered basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$ and $$ Q $$ is the $$ n \times n $$ matrix whose $$ j $$th column is $$ v_j $$ for $$ j = 1, \ 2, \dots, n $$, then $$ D = Q^{-1} AQ $$ is a diagonal matrix such that $$ D_{jj} $$ is the eigenvalue of $$ A $$ corresponding to $$ v_j $$.
Hence $$ A $$ is diagonalizable if and only if it is similar to a diagonal matrix.

### Theorem 5.2

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.
Then a scalar $$ \lambda $$ is an eigenvalue of $$ A $$ if and only if $$ \det(A - \lambda I_n) = 0 $$.

**Proof**  
A scalar $$ \lambda $$ is an eigenvalue of $$ A $$ if and only if there exists a nonzero vector $$ v \in \mathsf{F}^n $$ such that $$ Av = \lambda v $$, that is, $$ (A - \lambda I_n) = \mathit{0} $$.
By Theorem 2.5, this is true if and only if $$ A - \lambda I_n $$ is not invertible.
However, this result is equivalent to the statement that $$ \det(A - \lambda I_n) = 0 $$. $$ \blacksquare $$

### Definition

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.
The polynomial $$ f(t) = \det(A - t I_n) $$ is called the characteristic polynomial of $$ A $$.

It is easily shown that similar matrices have the same determinant and the same characteristic polynomial.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$.
Choose any ordered basis $$ \beta $$ for $$ \mathsf{V} $$.
We define the determinant of $$ \mathsf{T} $$, denoted $$ \det(\mathsf{T}) $$, to be the determinant of $$ A = {[\mathsf{T}]}_\beta $$, and the characteristic polynomial $$ f(t) $$ of $$ \mathsf{T} $$ to be the characteristic polynomial of $$ A $$.
That is,

$$
\begin{align*}
    f(t) = \det(A - t I_n)
\end{align*}
$$

The remark preceding these definitions shows that they are independent of the choice of ordered basis $$ \beta $$.
Thus if $$ \mathsf{T} $$ is  a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$ and $$ \beta $$ is an ordered basis for $$ \mathsf{V} $$, then $$ \lambda $$ is an eigenvalue of $$ \mathsf{T} $$ if and only if $$ \lambda $$ is an eigenvalue of $$ {[\mathsf{T}]}_\beta $$.

### Theorem 5.3

Let $$ A \in \mathsf{M}_{n \times n}(F) $$.  
(a) The characteristic polynomial of $$ A $$ is a polynomial of degree $$ n $$ with leading coefficient $$ {(-1)}^n $$.  
(b) $$ A $$ has at most $$ n $$ distinct eigenvalues.

### Theorem 5.4

Let $$ A \in \mathsf{M}_{n \times n}(F) $$, and let $$ \lambda $$ be an eigenvalue of $$ A $$.
Vector $$ v \in \mathsf{F}^n $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$ if and only if $$ v \neq \mathit{0} $$ and $$ (A - \lambda I)v = \mathit{0} $$.

Suppose that $$ \beta $$ is a basis for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ A $$.
The corollary to Theorem 2.23, assures us that if $$ Q $$ is the $$ n \times n $$ matrix whose columns are the vectors in $$ \beta $$, then $$ Q^{-1} AQ $$ is a diagonal matrix.  

![Desktop View](/assets/img/Linear Algebra/5.-Diagonalization/Figure 5.1.png){: width="400" }
_**Figure 5.1**_

To find the eigenvectors of a linear operator $$ \mathsf{T} $$ on an $$ n $$-dimensional vector space, select an ordered basis $$ \beta $$ for $$ \mathsf{V} $$ and let $$ A = {[\mathsf{T}]}_\beta $$.
Figure 5.1 is the special case of Figure 2.1 in Section 2.4 in which $$ \mathsf{V} = \mathsf{W} $$ and $$ \beta = \gamma $$.
Recall that for $$ v \in \mathsf{V} $$, $$ \phi_B(v) = {[v]}_\beta $$, the coordinate vector of $$ v $$ relative to $$ \beta $$.
We show that $$ v \in \mathsf{V} $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$ if and only if $$ \phi_B(v) $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$.
Suppose that $$ v $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.
Then $$ \mathsf{T}(v) = \lambda v $$.
Hence

$$
\begin{align*}
    A \phi_B(v) = {\mathsf{L}}_A \phi_B(v) = \phi_B(\lambda v) = \lambda \phi_B(v)
\end{align*}
$$

Now $$ \phi_B(v) \neq \mathit{0} $$, since $$ \phi_B $$ is an isomorphism; hence $$ \phi_B(v) $$ is an eigenvector of $$ A $$.
This argument is reversible, and so we can establish that if $$ \phi_B(v) $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$, then $$ v $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.  
An equivalent formulation of the result is that for an eigenvalue $$ \lambda $$ of $$ A $$ (and hence of $$ \mathsf{T} $$), a vector $$ y \in \mathsf{F}^n $$ is an eigenvector of $$ A $$ corresponding to $$ \lambda $$ if and only if $$ {\phi_B}^{-1}(y) $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$.

## 5.2 Diagonalizability

### Theorem 5.5

Let $$ \mathsf{T} $$ be a linear operator on a vector space, and let $$ \lambda_1, \lambda_2, \dots, \lambda_k $$ be distinct eigenvalues of $$ \mathsf{T} $$.
For each $$ i = 1, \ 2, \dots, k $$, let $$ S_i $$ be a finite set of eigenvectors of $$ \mathsf{T} $$ corresponding to $$ \lambda_i $$.
If each $$ S_i $$ $$ (i = 1, \ 2, \dots, k) $$ is linearly independent, then $$ S_1 \cup S_2 \cup \cdots \cup S_k $$ is linearly independent.

**Proof**  
The proof is by mathematical induction on $$ k $$.
If $$ k = 1 $$, there is nothing to prove.
So assume that the theorem holds for $$ k - 1 $$ distinct eigenvalues, where $$ k - 1 \ge 1 $$, and that we have $$ k $$ distinct eigenvalues $$ \lambda_1, \lambda_2, \dots, \lambda_k $$ of $$ \mathsf{T} $$.
For each $$ i = 1, \ 2, \dots, k $$, let $$ S_i = \{ v_{i1}, \ v_{i2}, \dots, v_{i n_i} \} $$ be a linearly independent set of eigenvectors of $$ \mathsf{T} $$ corresponding to $$ \lambda_i $$.  
Consider any scalars $$ \{ a_{ij} \} $$, where $$ i = 1, \ 2, \dots, k $$ and $$ j = 1, \ 2, \dots, n_i $$, such that

$$
\begin{align*}
    \sum_{i = 1}^k \sum_{j = 1}^{n_i} a_{ij} v_{ij} = \mathit{0}
\end{align*}
\label{eq:1}
\tag{5.1}
$$

Because $$ v_{ij} $$ is an eigenvector of $$ \mathsf{T} $$ of corresponding to $$ \lambda_i $$, applying $$ \mathsf{T} - \lambda_k \mathsf{I} $$ to both sides of \eqref{eq:1} yields

$$
\begin{align*}
    \sum_{i = 1}^{k - 1} \sum_{j = 1}^{n_i} a_{ij} (\lambda_i - \lambda_k) v_{ij} = \mathit{0}
\end{align*}
\label{eq:2}
\tag{5.2}
$$

But $$ S_1 \cup S_2 \cup \cdots \cup S_{k - 1} $$ is linearly independent by the induction hypothesis, so that \eqref{eq:2} implies $$ a_{ij} (\lambda_i - \lambda_k) = 0 $$ for $$ i = 1, \ 2, \dots, k - 1 $$ and $$ j = 1, \ 2, \dots, n_i $$.
Since $$ \lambda_1, \ \lambda_2, \dots, \lambda_k $$ are distinct, it follows that $$ \lambda_i - \lambda_k \neq 0 $$ for $$ 1 \le i \le k - 1 $$.
Hence $$ a_{ij} = 0 $$ for $$ i = 1, \ 2, \dots, k - 1 $$ and $$ j = 1, \ 2, \dots, n_i $$, and therefore \eqref{eq:1} reduces to $$ \sum_{j = 1}^{n_k} a_{kj} v_{kj} = \mathit{0} $$.
But $$ S_k $$ is also linearly independent, and so $$ a_{kj} = 0 $$ for $$ j = 1, \ 2, \dots, n_k $$.
Consequently $$ a_{ij} = 0 $$ for $$ i = 1, \ 2, \dots, k $$ and $$ j = 1, \ 2, \dots, n_i $$, proving that $$ S $$ is linearly independent. $$ \blacksquare $$

### Corollary

Let $$ \mathsf{T} $$ be a linear operator on an $$ n $$-dimensional vector space $$ \mathsf{V} $$.
If $$ \mathsf{T} $$ has $$ n $$ distinct eigenvalues, then $$ \mathsf{T} $$ is diagonalizable.

**Proof**  
Suppose that $$ \mathsf{T} $$ has $$ n $$ eigenvalues $$ \lambda_1, \dots, \lambda_n $$.
For each $$ i $$ choose an eigenvector $$ v_i $$ corresponding to $$ \lambda_i $$.
By Theorem 5.5, $$ \{ v_1, \dots, v_n \} $$ is linearly independent, and since $$ \dim(\mathsf{V}) = n $$, this set is a basis for $$ \mathsf{V} $$.
Thus, by Theorem 5.1, $$ \mathsf{T} $$ is diagonalizable. $$ \blacksquare $$

The converse of the corollary to Theorem 5.5 is false.
That is, it is not true that if $$ \mathsf{T} $$ is diagonalizable, then it has $$ n $$ distinct eigenvalues.

### Definition

A polynomial $$ f(t) $$ in $$ \mathsf{P}(F) $$ splits over $$ F $$ if there are scalars $$ c, \ a_1, \dots, a_n $$ (not necessarily distinct) in $$ F $$ such that

$$
\begin{align*}
    f(t) = c(t - a_1)(t - a_2) \cdots (t - a_n)
\end{align*}
$$

### Theorem 5.6

The characteristic polynomial of any diagonalizable linear operator on a vector space $$ \mathsf{V} $$ over a field $$ F $$ splits over $$ F $$.

**Proof**  
Let $$ \mathsf{T} $$ be a diagonalizable linear operator on the $$ n $$-dimensional vector space $$ \mathsf{V} $$, and let $$ \beta $$ be an ordered basis for $$ \mathsf{V} $$ such that $$ {[\mathsf{T}]}_{\beta} = D $$ is a diagonal matrix.
Suppose that

$$
\begin{align*}
    D =
    \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix}
\end{align*}
$$

and let $$ f(t) $$ be the characteristic polynomial of $$ \mathsf{T} $$.
Then

$$
\begin{align*}
    f(t) &= \det(D - tI) = \det
    \begin{pmatrix}
        \lambda_1 - t & 0 & \cdots & 0 \\
        0 & \lambda_2 - t & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n - t
    \end{pmatrix} \\
    &= (\lambda_1 - t)(\lambda_2 - t) \cdots (\lambda_n - t) = {(-1)}^n (t - \lambda_1)(t - \lambda_2) \cdots (t - \lambda_n)
\end{align*}
$$

### Definition

Let $$ \lambda $$ be an eigenvalue of a linear operator or matrix with characteristic polynomial $$ f(t) $$.
The multiplicity (or algebraic multiplicity) of $$ \lambda $$ is the largest positive integer $$ k $$ for which $$ {(t - \lambda)}^k $$ is a factor of $$ f(t) $$.

### Definition

Let $$ \mathsf{T} $$ be a linear operator on a vector space $$ \mathsf{V} $$, and let $$ \lambda $$ be an eigenvalue of $$ \mathsf{T} $$.
Define $$ {\mathsf{E}}_\lambda = \{ x \in \mathsf{V} : \mathsf{T}(x) = \lambda x \} = \mathsf{N}(\mathsf{T} - \lambda {\mathsf{I}}_{\mathsf{V}}) $$.
The set $$ \mathsf{E}_\lambda $$ is called the eigenspace of $$ \mathsf{T} $$ corresponding to the eigenvalue $$ \lambda $$.
Analogously, we define the eigenspace of a square matrix $$ A $$ corresponding to the eigenvalue $$ \lambda $$ to be the eigenspace of $$ \mathsf{L}_A $$ corresponding to $$ \lambda $$.

### Theorem 5.7

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$, and let $$ \lambda $$ be an eigenvalue of $$ \mathsf{T} $$ having multiplicity $$ m $$.
Then $$ 1 \le \dim(\mathsf{E}_\lambda) \le m $$.

**Proof**  
Choose an ordered basis $$ \{ v_1, \ v_2, \dots, v_p \} $$ for $$ \mathsf{E}_\lambda $$, extend it to an ordered basis $$ \beta =\{ v_1, \ v_2, \dots, v_p, \ v_{p + 1}, \dots, v_n \} $$ for $$ \mathsf{V} $$, and let $$ A = {[\mathsf{T}]}_{\beta} $$.
Observe that $$ v_i $$ $$ (1 \le i \le p) $$ is an eigenvector of $$ \mathsf{T} $$ corresponding to $$ \lambda $$, and therefore

$$
\begin{align*}
    A =
    \begin{pmatrix}
        \lambda I_p & B \\
        O & C
    \end{pmatrix}
\end{align*}
$$

The characteristic polynomial of $$ \mathsf{T} $$ is

$$
\begin{align*}
    f(t) &= \det(A - t I_n) = \det
    \begin{pmatrix}
        (\lambda - t) I_p & B \\
        O & C - t I_{n - p}
    \end{pmatrix} \\
    &= \det((\lambda - t) I_p) \cdot \det(C - t I_{n - p}) \\
    &= (\lambda - t)^p g(t)
\end{align*}
$$

where $$ g(t) $$ is a polynomial.
Thus $$ {(\lambda - t)}^p $$ is a factor of $$ f(t) $$, and hence the multiplicity of $$ \lambda $$ is at least $$ p $$.
But $$ \dim(\mathsf{E}_\lambda) = p $$, and so $$ \dim(\mathsf{E}_\lambda) \le m $$. $$ \blacksquare $$

### Theorem 5.8

Let $$ \mathsf{T} $$ be a linear operator on a finite-dimensional vector space $$ \mathsf{V} $$ such that the characteristic polynomial of $$ \mathsf{T} $$ splits.
Let $$ \lambda_1, \ \lambda_2, \dots, \lambda_k $$ be the distinct eigenvalues of $$ \mathsf{T} $$.
Then  
(a) $$ \mathsf{T} $$ is diagonalizable if and only if the multiplicity of $$ \lambda_i $$ is equal to $$ \dim(\mathsf{E}_\lambda) $$ for all $$ i $$.  
(b) If $$ \mathsf{T} $$ is diagonalizable and $$ \beta_i $$ is an ordered basis for $$ \mathsf{E}_\lambda $$, for each $$ i $$, then $$ \beta = \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k $$ is an ordered basis for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$. (We regard $$ \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k $$ as an ordered basis in the natural way—the vectors in $$ \beta_1 $$ are listed first (in the same order as in $$ \beta_1 $$), then the vectors in $$ \beta_2 $$ (in the same order as in $$ \beta_2 $$), etc.)

**Proof**  
For each $$ i $$, let $$ m_i $$ denote the multiplicity of $$ \lambda_i $$, $$ d_i = \dim(\mathsf{E}_{\lambda_i}) $$, and $$ n = \dim(\mathsf{V}) $$.
First, suppose that $$ \mathsf{T} $$ is diagonalizable.
Let $$ \beta $$ be a basis for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$.
For each $$ i $$, let $$ \beta_i = \beta \cap \mathsf{E}_{\lambda_i} $$, the set of vectors in $$ \beta $$ that are eigenvectors corresponding to $$ \lambda_i $$, and let $$ n_i $$ denote the number of vectors in $$ \beta_i $$.
Then $$ n_i \le d_i $$ for each $$ i $$ because $$ \beta_i $$ is a linearly independent subset of a subspace of dimension $$ d_i $$, and $$ d_i \le m_i $$ by Theorem 5.7.
The $$ n_i $$'s sum to $$ n $$ because $$ \beta $$ contains $$ n $$ vectors.
The $$ m_i $$'s also sum to $$ n $$ because the degree of the characteristic polynomial of $$ \mathsf{T} $$ is equal to the sum of the multiplicities of the eigenvalues.
Thus

$$
\begin{align*}
    n = \sum_{i = 1}^k n_i \le \sum_{i = 1}^k d_i \le \sum_{i = 1}^k m_i = n
\end{align*}
$$

It follows that

$$
\begin{align*}
    \sum_{i = 1}^k (m_i - d_i) = 0
\end{align*}
$$

Since $$ (m_i - d_i) \ge 0 $$ for all $$ i $$, we conclude that $$ m_i = d_i $$ for all $$ i $$.  
Conversely, suppose that $$ m_i = d_i $$ for all $$ i $$.
We simultaneously show that $$ \mathsf{T} $$ is diagonalizable and prove (b).
For each $$ i $$, let $$ \beta_i $$ be an ordered basis for $$ \mathsf{E}_\lambda $$, and let $$ \beta = \beta_1 \cup \beta_2 \cup \cdots \cup \beta_k $$.
By Theorem 5.5, $$ \beta $$ is linearly independent.
Furthermore, since $$ d_i = m_i $$ for all $$ i $$, $$ \beta $$ contains

$$
\begin{align*}
    \sum_{i = 1}^k d_i = \sum_{i = 1}^k m_i = n
\end{align*}
$$

vectors.
Therefore $$ \beta $$ is an ordered basis for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{V} $$, and we conclude that $$ \mathsf{T} $$ is diagonalizable. $$ \blacksquare $$

### Test for diagonalizability

Let $$ \mathsf{T} $$ be a linear operator on an $$ n $$-dimensional vector space $$ \mathsf{V} $$.
Then $$ \mathsf{T} $$ is diagonalizable if and only if both of the following conditions hold.

1. The characteristic polynomial of $$ \mathsf{T} $$ splits.
2. For each eigenvalue $$ \lambda $$ of $$ \mathsf{T} $$, the multiplicity of $$ \lambda $$ equals $$ \text{nullity}(\mathsf{T} - \lambda \mathsf{I}) $$, that is, the multiplicity of $$ \lambda $$ equals $$ n - \text{rank}(\mathsf{T} - \lambda \mathsf{I}) $$.

These conditions can be used to test if a square matrix $$ A $$ is diagonalizable because diagonalizability of $$ A $$ is equivalent to diagonalizability of the operator $$ \mathsf{L}_A $$.  
If $$ \mathsf{T} $$ is a diagonalizable operator and $$ \beta_1, \beta_2, \dots, \beta_k $$ is an ordered bases for the eigenspaces of $$ \mathsf{T} $$, then the union $$ \beta = \beta_1 \cup \beta_2, \cup \cdots \cup \beta_k $$ is an ordered for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$, and hence $$ {[\mathsf{T}]}_\beta $$ is a diagonal matrix.  
When testing $$ \mathsf{T} $$ for diagonalizability, it is usually easiest to choose a convenient basis $$ \alpha $$ for $$ \mathsf{V} $$ and work with $$ B = {[\mathsf{T}]}_\alpha $$.
If the characteristic polynomial of $$ B $$ splits, then use condition 2 above to check if the multiplicity of each repeated eigenvalue of $$ B $$ equals $$ n - \text{rank}(B - \lambda I) $$.
(By Theorem 5.7, condition 2 is automatically satisfied for eigenvalues of multiplicity 1.)
If so, then $$ B $$, and hence $$ \mathsf{T} $$, is diagonalizable.  
If $$ \mathsf{T} $$ is diagonalizable and a basis $$ \beta $$ for $$ \mathsf{V} $$ consisting of eigenvectors of $$ \mathsf{T} $$ is desired, then we first find a basis for each eigenspace of $$ B $$.
The union of these bases is a basis $$ \gamma $$ for $$ \mathsf{F}^n $$ consisting of eigenvectors of $$ B $$.
Each vector in $$ \gamma $$ is the coordinate vector relative to $$ \alpha $$ of an eigenvector of $$ \mathsf{T} $$.
The set consisting of these $$ n $$ eigenvectors of $$ \mathsf{T} $$ is the desired basis $$ \beta $$.  
Furthermore, if $$ A $$ is an $$ n \times n $$ diagonalizable matrix, we can use the corollary to Theorem 2.23 to find an invertible $$ n \times n $$ matrix $$ Q $$ and a diagonal $$ n \times n $$ matrix $$ D $$ such that $$ Q^{-1} AQ = D $$.
The matrix $$ Q $$ has as its columns the vectors in a basis of eigenvectors of $$ A $$, and $$ D $$ has as its $$ j $$th diagonal entry the eigenvalue of $$ A $$ corresponding to the $$ j $$th column of $$ Q $$.

### Systems of differential equations

Consider the system of differential equations

$$
\begin{align*}
    {x_1}' &= 3x_1 + x_2 + x_3 \\
    {x_2}' &= 2x_1 + 4x_2 + 2x_3 \\
    {x_3}' &= -x_1 - x_2 + x_3 \\
\end{align*}
$$

where, for each $$ i $$, $$ x_i = x_i(t) $$ is a differentiable real-valued function of the real variable $$ t $$. Let $$ x : R \rightarrow \mathsf{R}^3 $$ be the function defined by


$$
\begin{align*}
    x'(t) =
    \begin{pmatrix}
        {x_1}'(t) \\
        {x_2}'(t) \\
        {x_3}'(t)
    \end{pmatrix}
\end{align*}
$$

Let

$$
\begin{align*}
    A =
    \begin{pmatrix}
        3 & 1 & 1 \\
        2 & 4 & 2 \\
        -1 & -1 & 1
    \end{pmatrix}
\end{align*}
$$

be the coefficient matrix of the given system, so that we can rewrite the system as the matrix equation $$ x' = Ax $$.
It can be verified that for

$$
\begin{align*}
    Q =
    \begin{pmatrix}
        -1 & 0 & -1 \\
        0 & -1 & -2 \\
        1 & 1 & 1
    \end{pmatrix}
    && \text{and} && D =
    \begin{pmatrix}
        2 & 0 & 0 \\
        0 & 2 & 0 \\
        0 & 0 & 4
    \end{pmatrix}
\end{align*}
$$

we have $$ Q^{-1} AQ = D $$.
Substitute $$ A = QDQ^{-1} $$ into $$ x' = Ax $$ to obtain $$ x' = QDQ^{-1} x $$ or, equivalently, $$ Q^{-1} x' = DQ^{-1} x $$.
The function $$ y : R \rightarrow \mathsf{R}^3 $$ defined by $$ y(t) = Q^{-1} x(t) $$ can be shown to be differentiable, and $$ y' = Q^{-1} x' $$.
Hence the original system can be written as $$ y' = Dy $$.  
Since $$ D $$ is a diagonal matrix, the system $$ y' = Dy $$ is easy to solve.
Setting

$$
\begin{align*}
    y(t) =
    \begin{pmatrix}
        y_1(t) \\
        y_2(t) \\
        y_3(t)
    \end{pmatrix}
\end{align*}
$$

we can rewrite $$ y' = Dy $$ as

$$
\begin{align*}
    \begin{pmatrix}
        {y_1}'(t) \\
        {y_2}'(t) \\
        {y_3}'(t)
    \end{pmatrix}
    =
    \begin{pmatrix}
        2 & 0 & 0 \\
        0 & 2 & 0 \\
        0 & 0 & 4 \\
    \end{pmatrix}
    \begin{pmatrix}
        y_1(t) \\
        y_2(t) \\
        y_3(t)
    \end{pmatrix}
    =
    \begin{pmatrix}
        2 y_1(t) \\
        2 y_2(t) \\
        4 y_3(t)
    \end{pmatrix}
\end{align*}
$$

The three equations

$$
\begin{align*}
    {y_1}' = 2 y_1 \\
    {y_2}' = 2 y_2 \\
    {y_3}' = 4 y_3
\end{align*}
$$

are independent of each other, and thus can be solved individually.
It is easily seen that the general solution to these equations is $$ y_1(t) = c_1 e^{2t} $$, $$ y_2(t) = c_2 e^{2t} $$, and $$ y_3(t) = c_3 e^{4t} $$, where $$ c_1 $$, $$ c_2 $$, and $$ c_3 $$ are arbitrary constants.
Finally,

$$
\begin{align*}
    \begin{pmatrix}
        x_1(t) \\
        x_2(t) \\
        x_3(t)
    \end{pmatrix}
    &= x(t) = Qy(t) =
    \begin{pmatrix}
        -1 & 0 & -1 \\
        0 & -1 & -2 \\
        1 & 1 & 1
    \end{pmatrix}
    \begin{pmatrix}
        c_1 e^{2t} \\
        c_2 e^{2t} \\
        c_3 e^{4t}
    \end{pmatrix} \\
    &=
    \begin{pmatrix}
        -c_1 e^{2t} - c_3 e^{4t} \\
        -c_2 e^{2t} - 2 c_3 e^{4t} \\
        c_1 e^{2t} + c_2 e^{2t} + c_3 e^{4t}
    \end{pmatrix}
\end{align*}
$$

yields the general solution of the original system.
Note that this solution can be written as

$$
\begin{align*}
    x(t) = e^{2t} \left[ c_1
    \begin{pmatrix}
        -1 \\
        0 \\
        1
    \end{pmatrix}
    + c_2
    \begin{pmatrix}
        0 \\
        -1 \\
        1
    \end{pmatrix}
    \right] + e^{4t} \left[ c_3
    \begin{pmatrix}
        -1 \\
        -2 \\
        1
    \end{pmatrix}
    \right]
\end{align*}
$$

The expressions in brackets are arbitrary vectors in $$ \mathsf{E}_{\lambda_1} $$ and $$ \mathsf{E}_{\lambda_2} $$, respectively, where $$ \lambda_1 = 2 $$ and $$ \lambda_2 = 4 $$.
Thus the general solution of the original system is $$ x(t) = e^{2t} z_1 + e^{4t} z_2 $$, where $$ z_1 \in \mathsf{E}_{\lambda_1} $$ and $$ z_2 \in \mathsf{E}_{\lambda_2} $$.

### Direct sums

### Definition

Let $$ \mathsf{W}_1, \ \mathsf{W}_2, \dots, \mathsf{W}_k $$ be subspaces of a vector space $$ \mathsf{V} $$.
We define the sum of these subspaces to be the set

$$
\begin{align*}
    \{ v_1 + v_2 + \cdots + v_k : v_i \in \mathsf{W}_i \text{ for } 1 \le i \le k \}
\end{align*}
$$

which we denote by $$ \mathsf{W}_1 + \mathsf{W}_2 + \cdots + \mathsf{W}_k $$ or $$ \displaystyle \sum_{i = 1}^{k} \mathsf{W}_i $$.

### Definition

Let $$ \mathsf{W}, \ \mathsf{W}_1, \ \mathsf{W}_2, \dots, \mathsf{W}_k $$ be subspaces of a vector space $$ \mathsf{V} $$ such that $$ \mathsf{W}_i \subseteq \mathsf{W} $$ for $$ i = 1, \ 2, \dots, k $$.
We call $$ \mathsf{W} $$ the direct sum of the subspaces $$ \mathsf{W}_1, \ \mathsf{W}_2, \dots, \mathsf{W}_k $$, and write $$ \mathsf{V} = \mathsf{W}_1 \oplus \mathsf{W}_2 \oplus \cdots \oplus \mathsf{W}_k $$, if

$$
\begin{align*}
    \mathsf{V} = \sum_{i = 1}^k \mathsf{W}_k && \text{and} && \mathsf{W}_j \cap \sum_{i \neq j} \mathsf{W}_i = \{ \mathit{0} \} && \text{for each } j \ (1 \le j \le k)
\end{align*}
$$

### Theorem 5.9

Let $$ \mathsf{W}_1, \ \mathsf{W}_2, \dots, \mathsf{W}_k $$ be subspaces of a finite-dimensional vector space $$ \mathsf{V} $$.
The following conditions are equivalent.  
(a) $$ \mathsf{V} = \mathsf{W}_1 \oplus \mathsf{W}_2 \oplus \cdots \oplus \mathsf{W}_k $$  
(b) $$ \displaystyle \mathsf{V} = \sum_{i = 1}^k \mathsf{W}_i $$ and, for any vectors $$ v_1, \ v_2, \dots, v_k $$ such that $$ v_i \in \mathsf{W}_i $$ $$ (1 \le i \le k) $$, if $$ v_1 + v_2 + \cdots + v_k = \mathit{0} $$, then $$ v_i = \mathit{0} $$ for all $$ i $$.  
(c) Each vector $$ v $$ in $$ \mathsf{V} $$ can be uniquely written as $$ v = v_1 + v_2 + \cdots + v_k $$, where $$ v_i \in \mathsf{W}_i $$.  
(d) If $$ \gamma_i $$ is an ordered basis for $$ \mathsf{W}_i $$ $$ (1 \le i \le k) $$, then $$ \gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ is an ordered basis for $$ \mathsf{V} $$.  
(e) For each $$ i = 1, \ 2, \dots, k $$, there exists an ordered basis $$ \gamma_i $$ for $$ \mathsf{W}_i $$ such that $$ \gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ is an ordered basis for $$ \mathsf{V} $$.

**Proof**  
Assume (a).
We prove (b).
Clearly

$$
\begin{align*}
    \mathsf{V} = \sum_{i = 1}^k \mathsf{W}_i
\end{align*}
$$

Now suppose that $$ v_1, \ v_2, \dots, v_k $$ are vectors such that $$ v_i \in \mathsf{W}_i $$ for all $$ i $$ and $$ v_1 + v_2 + \cdots + v_k = \mathit{0} $$.
Then for any $$ j $$

$$
\begin{align*}
    -v_j = \sum_{i \neq j} v_i \in \sum_{i \neq j} \mathsf{W}_i
\end{align*}
$$

But $$ -v_j \in \mathsf{W}_j $$ and hence

$$
\begin{align*}
    -v_j \in \mathsf{W}_j \cap \sum_{i \neq j} \mathsf{W}_i = \{ \mathit{0} \}
\end{align*}
$$

So $$ v_j = \mathit{0} $$, proving (b).  
Now assume (b).
We prove (c).
Let $$ v \in \mathsf{V} $$.
By (b), there exist vectors $$ v_1, \ v_2, \dots, v_k $$ such that $$ v_i \in \mathsf{W}_i $$ and $$ v = v_1 + v_2 + \cdots + v_k $$.
We must show that this representation is unique.
Suppose also that $$ v = w_1 + w_2 + \cdots + w_k $$, where $$ w_i \in \mathsf{W}_i $$ for all $$ i $$.
Then

$$
\begin{align*}
    (v_1 - w_1) + (v_2 - w_2) + \cdots + (v_k - w_k) = \mathit{0}
\end{align*}
$$

But $$ v_i - w_i \in \mathsf{W}_i $$ for all $$ i $$, and therefore $$ v_i - w_i = \mathit{0} $$ for all $$ i $$ by (b).
Thus $$ v_i = w_i $$ for all $$ i $$, proving the uniqueness of the representation.  
Now assume (c).
We prove (d).
For each $$ i $$, let $$ \gamma_i $$ be an ordered basis for $$ \mathsf{W}_i $$.
Since

$$
\begin{align*}
    \mathsf{V} = \sum_{i = i}^k \mathsf{W}_i
\end{align*}
$$

by (c), it follows that $$\gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ generates $$ \mathsf{V} $$.
To show that this set is linearly independent, consider vectors $$ v_{ij} \in \gamma_i $$ $$ (j = 1, \ 2, \dots, m_i \text{ and } i = 1, \ 2, \dots, k) $$ and scalars $$ a_{ij} $$ such that

$$
\begin{align*}
    \sum_{i, \ j} a_{ij} v_{ij} = \mathit{0}
\end{align*}
$$

For each $$ i $$, set

$$
\begin{align*}
    w_i = \sum_{j = 1}^{m_i} a_{ij} v_{ij}
\end{align*}
$$

Then for each $$ i $$, $$ w_i \in \text{span}(\gamma_i) = \mathsf{W}_i $$ and

$$
\begin{align*}
    w_1 + w_2 + \cdots + w_k = \sum_{i, \ j} a_{ij} v_{ij} = \mathit{0}
\end{align*}
$$

Since $$ \mathit{0} \in \mathsf{W}_i $$ for each $$ i $$ and $$ \mathit{0} + \mathit{0} + \cdots + \mathit{0} = w_1 + w_2 + \cdots + w_k $$, (c) implies that $$ w_i = \mathit{0} $$ for all $$ i $$.
Thus

$$
\begin{align*}
    \mathit{0} = w_i = \sum_{j = 1}^{m_i} a_{ij} v_{ij}
\end{align*}
$$

for each $$ i $$.
But each $$ \gamma_i $$ is linearly independent, and hence $$ a_{ij} = 0 $$ for all $$ i $$ and $$ j $$.
Consequently $$ \gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ is linearly independent and therefore is a basis for $$ \mathsf{V} $$.  
Clearly (e) follows immediately from (d).  
Finally, we assume (e) and prove (a).
For each $$ i $$, let $$ \gamma_i $$ be an ordered basis for $$ \mathsf{W}_i $$ such that $$ \gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ is an ordered basis for $$ \mathsf{V} $$.
Then

$$
\begin{align*}
    \mathsf{V} &= \text{span}(\gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k) \\
    &= \text{span}(\gamma_1) + \text{span}(\gamma_2) + \cdots + \text{span}(\gamma_k) = \sum_{i = 1}^k \mathsf{W}_i
\end{align*}
$$

Fix $$ j $$ $$ (1 \le j \le k) $$, and suppose that, for some nonzero vector $$ v \in \mathsf{V} $$

$$
\begin{align*}
    v \in \mathsf{W}_j \cap \sum_{i \neq j} \mathsf{W}_i
\end{align*}
$$

Then

$$
\begin{align*}
    v \in \mathsf{W}_j = \text{span}(\gamma_j) && \text{and} && v \in \sum_{i \neq j} \mathsf{W}_i = \text{span} \left( \bigcup_{i \neq j} \gamma_i \right)
\end{align*}
$$

Hence $$ v $$ is a nontrivial linear combination of both $$ \gamma_j $$ and $$ \displaystyle \bigcup_{i \neq j} \gamma_i $$, so that $$ v $$ can be expressed as a linear combination of $$ \gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ in more than one way.
But these representations contradict Theorem 1.8, and so we conclude that

$$
\begin{align*}
    \mathsf{W}_j \cap \sum_{i \neq j} \mathsf{W}_i = \{ \mathit{0} \}
\end{align*}
$$

proving (a). $$ \blacksquare $$

### Theorem 5.10

A linear operator $$ \mathsf{T} $$ on a finite-dimensional vector space $$ \mathsf{V} $$ is diagonalizable if and only if $$ \mathsf{V} $$ is the direct sum of the eigenspaces of $$ \mathsf{T} $$.

**Proof**  
Let $$ \lambda_1, \ \lambda_2, \dots, \lambda_k $$ be the distinct eigenvalues of $$ \mathsf{T} $$.
First suppose that $$ \mathsf{T} $$ is diagonalizable, and for each $$ i $$ choose an ordered basis $$ \gamma_i $$ for the eigenspace $$ \mathsf{E}_{\lambda_i} $$.
By Theorem 5.8, $$ \gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ is a basis for $$ \mathsf{V} $$, and hence $$ \mathsf{V} $$ is a direct sum of the $$ \mathsf{E}_{\lambda_i} $$'s by Theorem 5.9.  
Conversely, suppose that $$ \mathsf{V} $$ is a direct sum of the eigenspaces of $$ \mathsf{T} $$.
For each $$ i $$, choose an ordered basis $$ \gamma_i $$ of $$ \mathsf{E}_{\lambda_i} $$.
By Theorem 5.9, the union $$ \gamma_1 \cup \gamma_2 \cup \cdots \cup \gamma_k $$ is a basis for $$ \mathsf{V} $$.
Since this basis consists of eigenvectors of $$ \mathsf{T} $$, we conclude that $$ \mathsf{T} $$ is diagonalizable. $$ \blacksquare $$

## 5.3 Matrix limits and Markov chains

We assume familiarity with limits of sequences of real numbers.
The limit of a sequence of complex numbers $$ \{ z_m : m = 1, \ 2, \dots \} $$ can be defined in terms of the limits of the sequences of the real and imaginary parts: If $$ z_m = r_m + i s_m $$, where $$ r_m $$ and $$ s_m $$ are real numbers, and $$ i $$ is the imaginary number such that $$ i^2 = -1 $$, then

$$
\begin{align*}
    \lim \limits_{m \to \infty} z_m = \lim \limits_{m \to \infty} r_m + i \lim \limits_{m \to \infty} s_m
\end{align*}
$$

provided that $$ \lim \limits_{m \to \infty} r_m $$ and $$ \lim \limits_{m \to \infty} s_m $$ exist.

### Definition

Let $$ L, \ A_1, \ A_2, \dots $$ be $$ n \times p $$ matrices having complex entries.
The sequence $$ A_1, \ A_2, \dots $$ is said to converge to the $$ n \times p $$ matrix $$ L $$, called the limit of the sequence, if

$$
\begin{align*}
    \lim \limits_{m \to \infty} {(A_m)}_{ij} = L_{ij}
\end{align*}
$$

for all $$ 1 \le i \le n $$ and $$ 1 \le j \le p $$.
To denote that $$ L $$ is the limit of the sequence, we write

$$
\begin{align*}
    \lim \limits_{m \to \infty} A_m = L
\end{align*}
$$

### Theorem 5.11

Let $$ A_1, \ A_2, \dots $$ be a sequence of $$ n \times p $$ matrices with complex entries that converges to the matrix $$ L $$.
Then for any $$ P \in \mathsf{M}_{r \times n}(C) $$ and $$ Q \in \mathsf{M}_{p \times s}(C) $$,

$$
\begin{align*}
    \lim \limits_{m \to \infty} P A_m = PL && \text{and} && \lim \limits_{m \to \infty} A_m Q = LQ
\end{align*}
$$

**Proof**  
For any $$ i $$ $$ (1 \le i \le r) $$ and $$ j $$ $$ (1 \le j \le p) $$,

$$
\begin{align*}
    \lim \limits_{m \to \infty} {(P A_m)}_{ij} &= \lim \limits_{m \to \infty} \sum_{k = 1}^n P_{ik} {(A_m)}_{kj} \\
    &= \sum_{k = 1}^n P_{ik} \cdot \lim \limits_{m \to \infty} {(A_m)}_{kj} = \sum_{k = 1}^n P_{ik} L_{kj} = {(PL)}_{ij}
\end{align*}
$$

Hence $$ \lim \limits_{m \to \infty} P A_m = PL $$.
The proof that $$ \lim \limits_{m \to \infty} A_m Q = LQ $$ is similar. $$ \blacksquare $$

### Corollary

Let $$ A \in \mathsf{M}_{n \times n}(C) $$ be such that $$ \lim \limits_{m \to \infty} A^m = L $$.
Then for any invertible matrix $$ Q \in \mathsf{M}_{n \times n}(C) $$,

$$
\begin{align*}
    \lim \limits_{m \to \infty} {(QA Q^{-1})}^m = QL Q^{-1}
\end{align*}
$$

**Proof**  
Since

$$
\begin{align*}
    {(QA Q^{-1})}^m = (QA Q^{-1}) (QA Q^{-1}) \cdots (QA Q^{-1}) = Q A^m Q^{-1}
\end{align*}
$$

we have

$$
\begin{align*}
    \lim \limits_{m \to \infty} {(QA Q^{-1})}^m = \lim \limits_{m \to \infty} Q A^m Q^{-1} = Q \left( \lim \limits_{m \to \infty} A^m \right) Q^{-1} = QL Q^{-1}
\end{align*}
$$

by applying Theorem 5.11 twice. $$ \blacksquare $$

In the discussion that follows, we frequently encounter the set

$$
\begin{align*}
    S = \{ \lambda \in C : |\lambda| < 1 \text{ or } \lambda = 1 \}
\end{align*}
$$

Geometrically, this set consists of the complex number $$ 1 $$ and the interior of the unit disk (the disk of radius $$ 1 $$ centered at the origin).
If $$ \lambda $$ is a complex number, then $$ \lim \limits_{m \to \infty} \lambda^m $$ exists if and only if $$ \lambda \in S $$.

### Theorem 5.12

Let $$ A $$ be a square matrix with complex entries.
Then $$ \lim \limits_{m \to \infty} A^m $$ exists if and only if both of the following conditions hold.  
(a) Every eigenvalue of $$ A $$ is contained in $$ S $$.  
(b) If $$ 1 $$ is an eigenvalue of $$ A $$, then the dimension of the eigenspace corresponding to $$ 1 $$ equals the multiplicity of $$ 1 $$ as an eigenvalue of $$ A $$.

### Theorem 5.13

Let $$ A \in \mathsf{M}_{n \times n}(C) $$ satisfy the following two conditions.  
(i) Every eigenvalue of $$ A $$ is contained in $$ S $$.  
(ii) $$ A $$ is diagonalizable.  
Then $$ \lim \limits_{m \to \infty} A^m $$ exists.

**Proof**  
Since $$ A $$ is diagonalizable, there exists an invertible matrix $$ Q $$ such that $$ Q^{-1} AQ = D $$ is a diagonal matrix.
Suppose that

$$
\begin{align*}
    D =
    \begin{pmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix}
\end{align*}
$$

Because $$ \lambda_1, \ \lambda_2, \dots, \lambda_n $$ are eigenvalues of $$ A $$, condition (i) requires that for each $$ i $$, either $$ \lambda_i = 1 $$ or $$ |\lambda_i| < 1 $$.
Thus

$$
\begin{align*}
    \lim \limits_{m \to \infty} {\lambda_i}^m =
    \begin{cases}
        1 && \text{if } \lambda_i = 1 \\
        0 && \text{otherwise}
    \end{cases}
\end{align*}
$$

But since

$$
\begin{align*}
    D^m =
    \begin{pmatrix}
        {\lambda_1}^m & 0 & \cdots & 0 \\
        0 & {\lambda_2}^m & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\
        0 & 0 & \cdots & {\lambda_n}^m
    \end{pmatrix}
\end{align*}
$$

the sequence $$ D, \ D^2, \dots $$ converges to a limit $$ L $$.
Hence

$$
\begin{align*}
    \lim \limits_{m \to \infty} A^m = \lim \limits_{m \to \infty} {(QD Q^{-1})}^m = QL Q^{-1}
\end{align*}
$$

by the corollary to Theorem 5.11. $$ \blacksquare $$

If a square matrix $$ A $$ has nonnegative entries and the sum of the entries of each column of $$ A $$ is $$ 1 $$, $$ A $$ is called a transition matrix or a stochastic matrix.
For an arbitrary $$ n \times n $$ transition matrix $$ M $$, the rows and columns correspond to $$ n $$ states.
The entry $$ M_{ij} $$ represents the probability of moving from state $$ j $$ to state $$ i $$ in one stage, and $$ {(M^m)}_{ij} $$ represents the probability of moving from state $$ j $$ to state $$ i $$ in $$ m $$ stages. 
Likewise, if a vector $$ P $$ contains nonnegative entries that sum to $$ 1 $$, it is called a probability vector.

### Theorem 5.14

Let $$ M $$ be an $$ n \times n $$ matrix having real nonnegative entries, let $$ v $$ be a column vector in $$ \mathsf{R}^n $$ having nonnegative coordinates, and let $$ u \in \mathsf{R}^n $$ be the column vector in which each coordinate equals $$ 1 $$.
Then  
(a) $$ M $$ is a transition matrix if and only if $$ u^t M = u^t $$  
(b) $$ v $$ is a probability vector if and only if $$ u^t v = \begin{pmatrix} 1 \end{pmatrix} $$

### Corollary

(a) The product of two $$ n \times n $$ transition matrices is an $$ n \times n $$ transition matrix.
In particular, any power of a transition matrix is a transition matrix.  
(b) The product of a transition matrix and a probability vector is a probability vector.