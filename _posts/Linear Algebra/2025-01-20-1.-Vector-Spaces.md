---
title: 1. Vector Spaces
# description: Short summary of the post
date: 2025-01-01 10:11
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra, vector-space]     # TAG names should always be lowercase
math: true
pin: false
---

## 1.1 Vector spaces

### Definition

A vector space (or linear space) $$ \mathsf{V} $$ over a field F consists of a set on which two operations (addition and scalar multiplication) are defined so that for each pair of elements $$ x $$, $$ y $$, in $$ \mathsf{V} $$ there is a unique element $$ x + y $$ in $$ \mathsf{V} $$, and for each element $$ a $$ in $$ F $$ and each element $$ x $$ in $$ \mathsf{V} $$ there is a unique element $$ ax $$ in $$ \mathsf{V} $$, such that the following conditions hold.

- (VS 1) For all $$ x $$, $$ y $$ in $$ \mathsf{V} $$, $$ x + y = y + x $$ (commutativity of addition).
- (VS 2) For all $$ x $$, $$ y $$, $$ z $$ in $$ \mathsf{V} $$, $$ (x + y) + z = x + (y + z) $$ (associativity of addition).
- (VS 3) There exists an element in $$ \mathsf{V} $$ denoted by $$ \mathit{0} $$ such that $$ x + \mathit{0} = x $$ for each $$ x $$ in $$ \mathsf{V} $$.
- (VS 4) For each element $$ x $$ in $$ \mathsf{V} $$ there exists an element $$ y $$ in $$ \mathsf{V} $$ such that $$ x + y = \mathit{0} $$.
- (VS 5) For each element $$ x $$ in $$ \mathsf{V} $$, $$ 1x = x $$.
- (VS 6) For each pair of elements $$ a $$, $$ b $$ in $$ F $$ and each element $$ x $$ in $$ \mathsf{V} $$, $$ (ab)x = a(bx) $$.
- (VS 7) For each element $$ a $$ in $$ F $$ and each pair of elements $$ x $$, $$ y $$ in $$ \mathsf{V} $$, $$ a(x + y) = ax + ay $$.
- (VS 8) For each pair of elements $$ a $$, $$ b $$ in $$ F $$ and each element $$ x $$ in $$ \mathsf{V} $$, $$ (a + b)x = ax + bx $$.

### Theorem 1.1 (Cancellation Law for Vector Addition)

If $$ x $$, $$ y $$, and $$ z $$ are vectors in a vector space $$ \mathsf{V} $$ such that $$ x + z = y + z $$, then $$ x = y $$.

**Proof**  
There exists a vector $$ v $$ in $$ \mathsf{V} $$ such that $$ z + v = \mathit{0} $$ (VS 4).
Thus

$$
\begin{align*}
    x &= x + \mathit{0} = x + (z + v) = (x + z) + v \\
      &= (y + z) + v = y + (z + v) = y + \mathit{0} = y
\end{align*}
$$

by (VS 2) and (VS 3). $$ \blacksquare $$

### Corollary 1.

The vector $$ \mathit{0} $$ described in (VS 3) is unique.

### Corollary 2.

The vector $$ y $$ described in (VS 4) is unique.

### Theorem 1.2

In any vector space $$ \mathsf{V} $$, the following statements are true:  
(a) $$ 0x = \mathit{0} $$ for each $$ x \in \mathsf{V} $$.  
(b) $$ (-a)x = -(ax) = a(-x) $$ for each $$ a \in F $$ and each $$ x \in \mathsf{V} $$.  
(c) $$ a \mathit{0} = \mathit{0} $$ for each $$ a \in F $$.

**Proof**  
(a) By (VS 8), (VS 3), and (VS 1), it follows that

$$
\begin{align*}
    0x + 0x = (0 + 0)x = 0x = 0x + \mathit{0} = \mathit{0} + 0x
\end{align*}
$$

Hence $$ 0x = \mathit{0} $$ by Theorem 1.1.  
(b) The vector $$ -(ax) $$ is the unique element of $$ \mathsf{V} $$ such that $$ ax + [-(ax)] = \mathit{0} $$.
Thus if $$ ax + (-a)x = \mathit{0} $$, Corollary 2 to Theorem 1.1 implies that $$ (-a)x = -(ax) $$.
But by (VS 8),

$$
\begin{align*}
    ax + (-a)x = [a + (-a)]x = 0x = \mathit{0}
\end{align*}
$$

by (a).
Consequently $$ (-a)x = -(ax) $$.  
The proof of (c) is similar to the proof of (a). $$ \blacksquare $$

## 1.2 Subspaces

### Definition

A subset $$ \mathsf{W} $$ of a vector space $$ \mathsf{V} $$ over a field $$ F $$ is called a subspace of $$ \mathsf{V} $$ if $$ \mathsf{W} $$ is a vector space over $$ F $$ with the operations of addition and scalar multiplication defined on $$ \mathsf{V} $$.

A subset $$ \mathsf{W} $$ of a vector space $$ \mathsf{V} $$ is a subspace of $$ \mathsf{V} $$ if and only if the following four properties hold.

1. $$ x + y \in \mathsf{W} $$ whenever $$ x \in \mathsf{W} $$ and $$ y \in \mathsf{W} $$. ($$ \mathsf{W} $$ is closed under addition.)
2. $$ cx \in \mathsf{W} $$ whenever $$ c \in F $$ and $$ x \in \mathsf{W} $$. ($$ \mathsf{W} $$ is closed under scalar multiplication.)
3. $$ \mathsf{W} $$ has a zero vector.
4. Each vector in $$ \mathsf{W} $$ has an additive inverse in $$ \mathsf{W} $$.

### Theorem 1.3

Let $$ \mathsf{V} $$ be a vector space and $$ \mathsf{W} $$ a subset of $$ \mathsf{V} $$.
Then $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$ if and only if the following three conditions hold for the operations defined in $$ \mathsf{V} $$.  
(a) $$ \mathit{0} \in \mathsf{W} $$.  
(b) $$ x + y \in \mathsf{W} $$ whenever $$ x \in \mathsf{W} $$ and $$ y \in \mathsf{W} $$.  
(c) $$ cx \in \mathsf{W} $$ whenever $$ c \in F $$ and $$ x \in \mathsf{W} $$.

**Proof**  
If $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$, then $$ \mathsf{W} $$ is a vector space with the operations of addition and scalar multiplication defined on $$ \mathsf{V} $$.
Hence conditions (b) and (c) hold, and there exists a vector $$ \mathit{0}' \in \mathsf{W} $$ such that $$ x + \mathit{0}' = x $$ for each $$ x \in \mathsf{W} $$.
But also $$ x + \mathit{0} = x $$, and thus $$ \mathit{0}' = \mathit{0} $$ by Theorem 1.1.
So condition (a) holds.  
Conversely, if conditions (a), (b), and (c) hold, the discussion preceding this theorem shows that $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$ if the additive inverse of each vector in $$ \mathsf{W} $$ lies in $$ \mathsf{W} $$.
But if $$ x \in \mathsf{w} $$, then $$ (-1)x \in \mathsf{W} $$ by condition (c), and $$ -x = (-1)x $$ by Theorem 1.2.
Hence $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$. $$ \blacksquare $$

### Theorem 1.4

Any intersection of subspaces of a vector space $$ \mathsf{V} $$ is a subspace of $$ \mathsf{V} $$.

**Proof**  
Let $$ \mathcal{C} $$ be a collection of subspaces of $$ \mathsf{V} $$, and let $$ \mathsf{W} $$ denote the intersection of the subspaces in $$ \mathcal{C} $$.
Since every subspace contains the zero vector, $$ \mathit{0} \in \mathsf{W} $$.
Let $$ a \in F $$ and $$ x, \ y \in \mathsf{W} $$.
Then $$ x $$ and $$ y $$ are contained in each subspace in $$ \mathcal{C} $$.
Because each subspace in $$ \mathcal{C} $$ is closed under addition and scalar multiplication, it follows that $$ x + y $$ and and $$ ax $$ are contained in each subspace in $$ \mathcal{C} $$.
Hence $$ x + y $$ and $$ ax $$ are also contained in $$ \mathsf{W} $$, so that $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$ by Theorem 1.3. $$ \blacksquare $$

## 1.3 Linear combinations

### Definition

Let $$ \mathsf{V} $$ be a vector space and $$ S $$ a nonempty subset of $$ \mathsf{V} $$.
A vector $$ v \in \mathsf{V} $$ is called a linear combination of vectors of $$ S $$ if there exist a finite number of vectors $$ u_1, \ u_2, \dots, u_n $$ in $$ S $$ and scalars $$ a_1, \ a_2, \dots, a_n $$ in $$ F $$ such that $$ v = a_1 u_1 + a_2 u_2 + \cdots + a_n u_n $$.

Observe that in any vector space $$ \mathsf{V} $$, $$ 0v = \mathit{0} $$ for each $$ v \in \mathsf{V} $$.
Thus the zero vector is a linear combination of any nonempty subset of $$ \mathsf{V} $$.

### Definition

Let $$ S $$ be a nonempty subset of a vector space $$ \mathsf{V} $$.
The span of $$ S $$, denoted span($$ S $$), is the set consisting of all linear combinations of the vectors in $$ S $$.
We define span($$ \emptyset $$) $$ = \{ \mathit{0} \} $$.

### Theorem 1.5

The span of any subset $$ S $$ of a vector $$ \mathsf{V} $$ is a subspace of $$ \mathsf{V} $$ that contains $$ S $$.
Moreover, any subspace of $$ \mathsf{V} $$ that contains $$ S $$ must also contain the span of $$ S $$.

**Proof**  
This result is immediate if $$ S = \emptyset $$ because span($$ \emptyset $$) $$ = \{ \mathit{0} \} $$, which is a subspace that contains $$ S $$ and is contained in any subspace of $$ \mathsf{V} $$.
If $$ S \neq \emptyset $$, then $$ S $$ contains a vector $$ z $$.
So $$ 0z = \mathit{0} $$ is in span($$ S $$).
Let $$ x, \ y \in $$ span($$ S $$).
Then there exist vectors $$ u_1, \ u_2, \dots, u_m, \ v_1, \ v_2, \dots, v_n $$ in $$ S $$ and scalars $$ a_1, \ a_2, \dots, a_m, \ b_1, \ b_2, \dots, b_n $$ such that

$$
\begin{align*}
    x = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m
\end{align*}
$$

and

$$
\begin{align*}
    y = b_1 v_1 + b_2 v_2 + \cdots + b_n v_n
\end{align*}
$$

Then

$$
\begin{align*}
    x + y = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m + b_1 v_1 + b_2 v_2 + \cdots + b_n v_n
\end{align*}
$$

and, for any scalar $$ c $$,

$$
\begin{align*}
    cx = (c a_1) u_1 + (c a_2) u_2 + \cdots + (c a_m) u_m
\end{align*}
$$

are clearly linear combinations of the vectors in $$ S $$; so $$ x + y $$ and $$ cx $$ are in span($$ S $$).
Thus span($$ S $$) is a subspace of $$ \mathsf{V} $$.
Furthermore, if $$ v \in S $$, then $$ v = 1 \cdot v \in $$ span($$ S $$); so the span of $$ S $$ contains $$ S $$.  
Now let $$ \mathsf{W} $$ denote any subspace of $$ \mathsf{V} $$ that contains $$ S $$.
If $$ w \in $$ span($$ S $$), then $$ w $$ has the form $$ w = c_1 w_1 + c_2 w_2 + \cdots + c_k w_k $$ for some vectors $$ w_1, \ w_2, \dots, w_k $$ in $$ S $$ and some scalars $$ c_1, \ c_2, \dots, c_k $$.
Since $$ S \subseteq \mathsf{W} $$, we have $$ w_1, \ w_2, \dots, w_k \in \mathsf{W} $$.
Therefore $$ w = c_1 w_1 + c_2 w_2 + \cdots + c_k w_k $$ is in $$ \mathsf{W} $$.
Because $$ w $$, an arbitrary vector in span($$ S $$), belongs to $$ \mathsf{W} $$, it follows that span($$ S $$) $$ \subseteq \mathsf{W} $$. $$ \blacksquare $$

### Definition

A subset $$ S $$ of a vector space $$ \mathsf{V} $$ generates (or spans) $$ \mathsf{V} $$ if span($$ S $$) = $$ \mathsf{V} $$.

## 1.4 Linear dependence and linear independence

### Definition

A subset $$ S $$ of a vector space $$ \mathsf{V} $$ is called linearly dependent if there exist a finite number of distinct vectors $$ u_1, \ u_2, \dots, u_n $$ in $$ S $$ and scalars $$ a_1, \ a_2, \dots, a_n $$ not all zero, such that

$$
\begin{align*}
    a_1 u_1 + a_2 u_2 + \cdots + a_n u_n = \mathit{0}
\end{align*}
$$

In this case we also say that the vectors of $$ S $$ are linearly dependent.

### Definition

A subset $$ S $$ of a vector space that is not linearly dependent is called linearly independent.

The following facts about linearly independent sets are true in any vector space.

1. The empty set is linearly independent, for linearly dependent sets must be nonempty.
2. A set consisting of a single nonzero vector is linearly independent.
For if $$ \{ u \} $$ is linearly dependent, then $$ au = \mathit{0} $$ for some nonzero scalar $$ a $$.
Thus $$ u = a^{-1} (au) = a^{-1} \mathit{0} = \mathit{0} $$.
3. A set is linearly independent if and only if the only representations of $$ \mathit{0} $$ as linear combinations of its vectors are trivial representations.

### Theorem 1.6

Let $$ \mathsf{V} $$ be a vector space, and let $$ S_1 \subseteq S_2 \subseteq \mathsf{V} $$.
If $$ S_1 $$ is linearly dependent, then $$ S_2 $$ is linearly dependent.

### Corollary

Let $$ \mathsf{V} $$ be a vector space, and let $$ S_1 \subseteq S_2 \subseteq \mathsf{V} $$.
If $$ S_2 $$ is linearly independent, then $$ S_1 $$ is linearly independent.

### Theorem 1.7

Let $$ S $$ be a linearly independent subset of a vector space $$ \mathsf{V} $$, and let $$ v $$ be a vector in $$ \mathsf{V} $$ that is not in $$ S $$.
Then $$ S \cup \{ v \} $$ is linearly dependent if and only if $$ v \in $$ span($$ S $$).

**Proof**  
If $$ S \cup \{ v \} $$ is linearly dependent, then there are vectors $$ u_1, \ u_2, \dots, u_n $$ in $$ S \cup \{ v \} $$ such that $$ a_1 u_1 + a_2 u_2 + \cdots + a_n u_n = \mathit{0} $$ for some nonzero scalars $$ a_1, \ a_2, \dots, a_n $$.
Since $$ S $$ is linearly independent, one of the $$ u_i $$'s, say $$ u_1 $$, equals $$ v $$.
Thus $$ a_1 v + a_2 u_2 + \cdots + a_n u_n = \mathit{0} $$, and so

$$
\begin{align*}
    v = a^{-1} (-a_2 u_2 - \cdots - a_n u_n) = -({a_1}^{-1} a_2) u_2 - \cdots - ({a_1}^{-1} a_n) u_n
\end{align*}
$$

Because $$ v $$ is a linear combination of $$ u_2, \dots, u_n $$, which are in $$ S $$, we have $$ v \in $$ span($$ S $$).

Conversely, let $$ v \in $$ span($$ S $$).
Then there exist vectors $$ v_1, \ v_2, \dots, v_m $$ in $$ S $$ and scalars $$ b_1, \ b_2, \dots, b_m $$ such that $$ v = b_1 v_1 + b_2 v_2 + \cdots + b_m v_m $$.
Therefore

$$
\begin{align*}
    0 = b_1 v_1 + b_2 v_2 + \cdots + b_n v_m + (-1) v
\end{align*}
$$

Note that $$ v \neq v_i $$ for $$ i = 1, \ 2, \dots, m $$ because $$ v \notin S $$.
Hence the coefficient of $$ v $$ in this linear combination is nonzero, and so the set $$ \{ v_1, \ v_2, \dots, v_m, \ v \} $$ is linearly dependent.
Thus $$ S \cup \{ v \} $$ is linearly dependent by Theorem 1.6. $$ \blacksquare $$

## 1.5 Bases and dimension

### Definition

A basis $$ \beta $$ for a vector space $$ \mathsf{V} $$ is a linearly independent subset of $$ \mathsf{V} $$ that generates $$ \mathsf{V} $$.

### Theorem 1.8

Let $$ \mathsf{V} $$ be a vector space and $$ u_1, \ u_2, \dots, u_n $$ be distinct vectors in $$ \mathsf{V} $$.
Then $$ \beta = \{ u_1, \ u_2, \dots, u_n \} $$ is a basis for $$ \mathsf{V} $$ if and only if each $$ v \in \mathsf{V} $$ can be uniquely expressed as a linear combination of vectors of $$ \beta $$, that is, can be expressed in the form

$$
\begin{align*}
    v = a_1 u_1 + a_2 u_2 + \cdots + a_n u_n
\end{align*}
$$

for unique scalars $$ a_1, \ a_2, \dots, a_n $$.

**Proof**  
Let $$ \beta $$ be a basis for $$ \mathsf{V} $$.
If $$ v \in \mathsf{V} $$, then $$ v \in $$ span($$ \beta $$) because span($$ \beta $$) $$ = \mathsf{V} $$.
Thus $$ v $$ is a linear combination of the vectors of $$ \beta $$.
Suppose that

$$
\begin{align*}
    v = a_1 u_1 + a_2 u_2 + \cdots + a_n u_n
\end{align*}
$$

and

$$
\begin{align*}
    v = b_1 u_1 + b_2 u_2 + \cdots + b_n u_n
\end{align*}
$$

are two such representations of $$ v $$.
Subtracting the second equation from the first gives

$$
\begin{align*}
    \mathit{0} = (a_1 - b_1) u_1 + (a_2 - b_2) u_2 + \cdots + (a_n - b_n) u_n
\end{align*}
$$

Since $$ \beta $$ is linearly independent, it follows that $$ a_1 - b_1 = a_2 - b_2 = \cdots = a_n - b_n = 0 $$.
Hence $$ a_1 = b_1, \ a_2 = b_2, \dots, a_n = b_n $$, and so $$ v $$ is uniquely expressible as a linear combination of the vectors of $$ \beta $$.  
The proof of the converse is an exercise. $$ \blacksquare $$

### Theorem 1.9

If a vector space $$ \mathsf{V} $$ is generated by a finite set $$ S $$, then some subset of $$ S $$ is a basis for $$ \mathsf{V} $$.
Hence $$ \mathsf{V} $$ has a finite basis.

**Proof**  
If $$ S = \emptyset $$ or $$ S = \{ \mathit{0} \} $$, then $$ \mathsf{V} = \{ \mathit{0} \} $$ and $$ \emptyset $$ is a subset of $$ S $$ that is a basis for $$ \mathsf{V} $$.
Otherwise $$ S $$ contains a nonzero vector $$ u_1 $$, and $$ \{ u_1 \} $$ is a linearly independent set.
Continue, if possible, choosing vectors $$ u_2, \dots, u_k $$ in $$ S $$ such that $$ \{ u_1, \ u_2, \dots, u_k \} $$ is a linearly independent set of $$ k $$ vectors.
Since $$ S $$ is a finite set, this process must end with a linearly independent set $$ \beta = \{ u_1, \ u_2, \dots, u_n \} $$.
There are two ways this could happen.  
(i) The set $$ \beta = S $$.
In this case, $$ S $$ is both a linearly independent set and a generating set for $$ \mathsf{V} $$.
That is, $$ S $$ is itself a basis for $$ \mathsf{V} $$.  
(ii) The set $$ \beta $$ is a proper linearly independent subset of $$ S $$ such that adjoining to $$ \beta $$ any vector in $$ S $$ not in $$ \beta $$ produces a linearly independent set.
In this case, we claim that $$ \beta $$ is the desired subset of $$ S $$ that is a basis for $$ \mathsf{V} $$.
Because $$ \beta $$ is linearly independent by construction, it suffices to show that $$ \beta $$ spans $$ \mathsf{V} $$.
By Theorem 1.5, we need to show that $$ S \subseteq $$ span($$ \beta $$).
Let $$ v \in S $$.
If $$ v \in \beta $$, then clearly $$ v \in $$ span($$ \beta $$).
Otherwise, if $$ v \notin \beta $$, then the preceding construction shows that $$ \beta \cup \{ v \} $$ is linearly dependent.
So $$ v \in $$ span($$ \beta $$) by Theorem 1.7.
Thus $$ S \subseteq $$ span($$ \beta $$), completing the proof. $$ \blacksquare $$

### Theorem 1.10 (Replacement Theorem)

Let $$ \mathsf{V} $$ be a vector space that is generated by a set $$ G $$ containing exactly $$ n $$ vectors, and let $$ L $$ be a linearly independent subset of $$ \mathsf{V} $$ containing exactly $$ m $$ vectors.
Then $$ m \le n $$ and there exists a subset $$ H $$ of $$ G $$ containing exactly $$ n - m $$ vectors such that $$ L \cup H $$ generates $$ \mathsf{V} $$.

**Proof**  
The proof is by mathematical induction on $$ m $$.
The induction begins with $$ m = 0 $$; for in this case $$ L = \emptyset $$, and so taking $$ H = G $$ gives the desired result.  
Now suppose that the theorem is true for some integer $$ m \ge 0 $$.
Let $$ L = \{ v_1, \ v_2, \dots, v_{m + 1} \} $$ be a linearly independent subset of $$ \mathsf{V} $$ consisting of $$ m + 1 $$ vectors.
By the corollary to Theorem 1.6, $$ \{ v_1, \ v_2, \dots, v_m \} $$ is linearly independent, and so we may apply the induction hypothesis to conclude that $$ m \le n $$ and that there is a subset $$ \{ u_1, \ u_2, \dots, u_{n - m} \} $$ of $$ G $$ such that $$ \{ v_1, \ v_2, \dots, v_m \} \cup \{ u_1, \ u_2, \dots, u_{n - m} \} $$ generates $$ \mathsf{V} $$.
Thus there exist scalars $$ a_1, \ a_2, \dots, a_m, \ b_1, \ b_2, \dots, b_{n - m} $$ such that

$$
\begin{align*}
    a_1 v_1 &+ a_2 v_2 + \cdots + a_m v_m \\
    &+ b_1 u_1 + b_2 u_2 + \cdots + b_{n - m} u_{n - m} = v_{m + 1}
\end{align*}
\label{eq:1}
\tag{1.1}
$$

Note that $$ n - m > 0 $$, lest $$ v_{m + 1} $$ be a linear combination of $$ v_1, \ v_2, \dots, v_m $$, which by Theorem 1.7 contradicts the assumption that $$ L $$ is linearly independent.
Hence $$ n > m $$; that is, $$ n \le m + 1 $$.
Moreover, some $$ b_i $$, say $$ b_1 $$, is nonzero, for otherwise we obtain the same contradiction.
Solving \eqref{eq:1} for $$ u_1 $$ gives

$$
\begin{align*}
    u_1 &= ({-b_1}^{-1} a_1) v_1 + ({-b_1}^{-1} a_2) v_2 + \cdots + ({-b_1}^{-1} a_m) v_m \\
    &+ ({b_1}^{-1}) v_{m + 1} + ({-b_1}^{-1} b_2) u_2 + \cdots + ({-b_1}^{-1} b_{n - m}) u_{n - m}
\end{align*}
$$

Let $$ H = \{ u_2, \dots, u_{n - m} \} $$.
Then $$ u_1 \in $$ span($$ L \cup H $$), and because $$ v_1, \ v_2, \dots, v_m $$, $$ u_2, \dots, u_{n - m} $$ are clearly in span($$ L \cup H $$), it follows that

$$
\begin{align*}
    \{ v_1, v_2, \dots, v_m, \ u_1, \ u_2, \dots, u_{n - m} \} \subseteq \text{span}(L \cup H)
\end{align*}
$$

Because $$ \{ v_1, \ v_2, \dots, v_m, \ u_1, \ u_2, \dots, u_{n - m} \} $$ generates $$ \mathsf{V} $$, Theorem 1.5 implies that span($$ L \cup H $$) $$ = \mathsf{V} $$.
Since $$ H $$ is a subset of $$ G $$ that contains $$ (n - m) - 1 = n - (m + 1) $$ vectors, the theorem is true for $$ m + 1 $$.
This completes the induction. $$ \blacksquare $$

### Corollary 1.

Let $$ \mathsf{V} $$ be a vector space having a finite basis.
Then all bases for $$ \mathsf{V} $$ are finite, and every basis for $$ \mathsf{V} $$ contains the same number of vectors.

**Proof**  
Suppose that $$ \beta $$ is a finite basis for $$ \mathsf{V} $$ that contains exactly $$ n $$ vectors, and let $$ \gamma $$ be any other basis for $$ \mathsf{V} $$.
If $$ \gamma $$ contains more than $$ n $$ vectors, then we can select a subset $$ S $$ of $$ \gamma $$ containing exactly $$ n + 1 $$ vectors.
Since $$ S $$ is linearly independent and $$ \beta $$ generates $$ \mathsf{V} $$, the replacement theorem implies that $$ n + 1 \le n $$, a contradiction.
Therefore $$ \gamma $$ is finite, and the number $$ m $$ of vectors in $$ \gamma $$ satisfies $$ m \le n $$.
Reversing the roles of $$ \beta $$ and $$ \gamma $$ and arguing as above, we obtain $$ n \le m $$.
Hence $$ m = n $$. $$ \blacksquare $$

### Definition

A vector space is called finite-dimensional if it has a basis consisting of a finite number of vectors.
The unique integer $$ n $$ such that every basis for $$ \mathsf{V} $$ contains exactly $$ n $$ elements is called the dimension of $$ \mathsf{V} $$ and is denoted by $$ \dim(\mathsf{V}) $$.
A vector space that is not finite-dimensional is called infinite-dimensional.

In the terminology of dimension, the replacement theorem states that if $$ \mathsf{V} $$ is a finite-dimensional vector space, then no linearly independent subset of $$ \mathsf{V} $$ can contain more than $$ \dim(\mathsf{V}) $$ vectors.

### Corollary 2.

Let $$ \mathsf{V} $$ be a vector space with dimension $$ n $$.  
(a) Any finite generating set for $$ \mathsf{V} $$ contains at least $$ n $$ vectors, and a generating set for $$ \mathsf{V} $$ that contains exactly $$ n $$ vectors is a basis for $$ \mathsf{V} $$.  
(b) Any linearly independent subset of $$ \mathsf{V} $$ that contains exactly $$ n $$ vectors is a basis for $$ \mathsf{V} $$.  
(c) Every linearly independent subset of $$ \mathsf{V} $$ can be extended to a basis for $$ \mathsf{V} $$, that is, if $$ L $$ is a linearly independent subset of $$ \mathsf{V} $$, then there is a basis $$ \beta $$ of $$ \mathsf{V} $$ such that $$ L \subseteq \beta $$.

**Proof**  
Let $$ \beta $$ be a basis for $$ \mathsf{V} $$.  
(a) Let $$ G $$ be a finite generating set for $$ \mathsf{V} $$.
By Theorem 1.9 some subset $$ H $$ of $$ G $$ is a basis for $$ \mathsf{V} $$.
Corollary 1 implies that $$ H $$ contains exactly $$ n $$ vectors.
Since a subset of $$ G $$ contains $$ n $$ vectors, $$ G $$ must contain at least $$ n $$ vectors.
Moreover, if $$ G $$ contains exactly $$ n $$ vectors, then we must have $$ H = G $$, so that $$ G $$ is a basis for $$ \mathsf{V} $$.  
(b) Let $$ L $$ be a linearly independent subset of $$ \mathsf{V} $$ containing exactly $$ n $$ vectors.
It follows from the replacement theorem that there is a subset $$ H $$ of $$ \beta $$ containing $$ n - n = 0 $$ vectors such that $$ L \cup H $$ generates $$ \mathsf{V} $$.
Thus $$ H = \emptyset $$, and $$ L $$ generates $$ \mathsf{V} $$.
Since $$ L $$ is also linearly independent, $$ L $$ is a basis for $$ \mathsf{V} $$.  
(c) If $$ L $$ is a linearly independent subset of $$ \mathsf{V} $$ containing $$ m $$ vectors, then the replacement theorem asserts that there is a subset $$ H $$ of $$ \beta $$ containing exactly $$ n - m $$ vectors such that $$ L \cup H $$ generates $$ \mathsf{V} $$.
Now $$ L \cup H $$ contains at most $$ n $$ vectors; therefore (a) implies that $$ L \cup H $$ contains exactly $$ n $$ vectors and that $$ L \cup H $$ is a basis for $$ \mathsf{V} $$. $$ \blacksquare $$

### Theorem 1.11

Let $$ \mathsf{W} $$ be a subspace of a finite-dimensional vector space $$ \mathsf{V} $$.
Then $$ \mathsf{W} $$ is finite-dimensional and $$ \dim(\mathsf{W}) \le \dim(\mathsf{V}) $$.
Moreover, if $$ \dim(\mathsf{W}) = \dim(\mathsf{V}) $$, then $$ \mathsf{V} = \mathsf{W} $$.

**Proof**  
Let $$ \dim(\mathsf{V}) = n $$.
If $$ \mathsf{W} = \{ \mathit{0} \} $$, then $$ \mathsf{W} $$ is finite-dimensional and $$ \dim(\mathsf{W}) = 0 \le n $$.
Otherwise, $$ \mathsf{W} $$ contains a nonzero vector $$ x_1 $$; so $$ \{ x_1 \} $$ is a linearly independent set.
Continue choosing vectors, $$ x_1, \ x_2, \dots, x_k $$ in $$ \mathsf{W} $$ such that $$ \{ x_1, \ x_2, \dots, x_k \} $$ is linearly independent.
Since no linearly independent subset of $$ \mathsf{V} $$ can contain more than $$ n $$ vectors, this process must stop where $$ k \le n $$ and $$ \{ x_1, \ x_2, \dots, x_k \} $$ is linearly independent but adjoining any other vector space from $$ \mathsf{W} $$ produces a linearly dependent set.
Theorem 1.7 implies that $$ \{ x_1, \ x_2, \dots, x_k \} $$ generates $$ \mathsf{W} $$, and hence it is a basis for $$ \mathsf{W} $$.
Therefore $$ \dim(\mathsf{W}) = k \le n $$.  
If $$ \dim(\mathsf{W}) = n $$, then a basis for $$ \mathsf{W} $$ is a linearly independent subset of $$ \mathsf{V} $$ containing $$ n $$ vectors.
But Corollary 2 of the replacement theorem implies that this basis for $$ \mathsf{W} $$ is also a basis for $$ \mathsf{V} $$; so $$ \mathsf{W} = \mathsf{V} $$. $$ \blacksquare $$

### Corollary

If $$ \mathsf{W} $$ is a subspace of a finite-dimensional vector space $$ \mathsf{V} $$, then any basis for $$ \mathsf{W} $$ can be extended to a basis for $$ \mathsf{V} $$.

**Proof**  
Let $$ S $$ be a basis for $$ \mathsf{W} $$.
Because $$ S $$ is a linearly independent subset of $$ \mathsf{V} $$, Corollary 2 of the replacement theorem guarantees that $$ S $$ can be extended to a basis for $$ \mathsf{V} $$. $$ \blacksquare $$

## 1.6 Maximal linearly independent subsets

### Definition

Let $$ \mathcal{F} $$ be a family of sets.
A member $$ M $$ of $$ \mathcal{F} $$ is called maximal (with respect to set inclusion) if $$ M $$ is contained in no member of $$ \mathcal{F} $$ other than $$ M $$ itself.

### Definition

A collection of sets $$ \mathcal{C} $$ is called a chain (or nest or tower) if for each pair of sets $$ A $$ and $$ B $$ in $$ \mathcal{C} $$, either $$ A \subseteq B $$ or $$ B \subseteq A $$.

### Hausdorff Maximal Principle

Let $$ \mathcal{F} $$ be a family of sets.
If, for each chain $$ \mathcal{C} \subseteq \mathcal{F} $$, there exists a member of $$ \mathcal{F} $$ that contains all the members of $$ \mathcal{C} $$, then $$ \mathcal{F} $$ contains a maximal member.

### Definition

Let $$ S $$ be a subset of a vector space $$ \mathsf{V} $$.
A maximal linearly independent subset of $$ S $$ is a subset $$ B $$ of $$ S $$ satisfying both of the following conditions.  
(a) $$ B $$ is linearly independent.  
(b) The only independent subset of $$ S $$ that contains $$ B $$ is $$ B $$ itself.

A basis $$ \beta $$ for a vector space $$ \mathsf{V} $$ is a maximal linearly independent subset of $$ \mathsf{V} $$, because

1. $$ \beta $$ is linearly independent by definition.
2. If $$ v \in \mathsf{V} $$ and $$ v \notin \beta $$, then $$ \beta \cup \{ v \} $$ is linearly dependent by Theorem 1.7 because span($$ \beta $$) $$ = \mathsf{V} $$.

### Theorem 1.12

Let $$ \mathsf{V} $$ be a vector space and $$ S $$ a subset that generates $$ \mathsf{V} $$.
If $$ \beta $$ is a maximal linearly independent subset of $$ S $$, then $$ \beta $$ is a basis for $$ \mathsf{V} $$.

**Proof**  
Let $$ \beta $$ be a maximal linearly independent subset of $$ S $$.
Because $$ \beta $$ is linearly independent, it suffices to prove that $$ \beta $$ generates $$ \mathsf{V} $$.
We claim that $$ S \subseteq $$ span($$ \beta $$), for otherwise there exists a $$ v \in S $$ such that $$ v \notin $$ span($$ \beta $$).
Since Theorem 1.7 implies that $$ \beta \cup \{ v \} $$ is linearly independent, we have contradicted the maximality of $$ \beta $$.
Therefore $$ S \subseteq $$ span($$ \beta $$).
Because span($$ S $$) $$ = \mathsf{V} $$, it follows from Theorem 1.5 that span($$ \beta $$) $$ = \mathsf{V} $$. $$ \blacksquare $$

### Theorem 1.13

Let $$ S $$ be a linearly independent subset of a vector space $$ \mathsf{V} $$.
There exists a maximal linearly independent subset of $$ \mathsf{V} $$ that contains $$ S $$.

**Proof**  
Let $$ \mathcal{F} $$ denote the family of all linearly independent subsets of $$ \mathsf{V} $$ containing $$ S $$.
To show that $$ \mathcal{F} $$ contains a maximal element, we show that if $$ \mathcal{C} $$ is a chain in $$ \mathcal{F} $$, then there exists a member $$ U $$ of $$ \mathcal{F} $$ containing each member of $$ \mathcal{C} $$.
If $$ \mathcal{C} $$ is empty, take $$ U = S $$.
Otherwise take $$ U $$ equal to the union of the members of $$ \mathcal{C} $$.
Clearly $$ U $$ contains each member of $$ \mathcal{C} $$, and so it suffices to prove that $$ U \in \mathcal{F} $$ (i.e., that $$ U $$ is a linearly independent subset of $$ \mathsf{V} $$ that contains $$ S $$).
Because each member of $$ \mathcal{C} $$ is a subset of $$ \mathsf{V} $$ containing $$ S $$, we have $$ S \subseteq U \subseteq \mathsf{V} $$.
Thus we need only prove that $$ U $$ is linearly independent.
Let $$ u_1, u_2, \dots, u_n $$ be in $$ U $$ and $$ a_1, \ a_2, \dots, a_n $$ be scalars such that $$ a_1 u_1 + a_2 u_2 + \cdots + a_n u_n = \mathit{0} $$.
Because $$ u_i \in U $$ for $$ i = 1, \ 2, \dots, n $$, there exists a set $$ A_i $$ in $$ \mathcal{C} $$ such that $$ u_i \in A_i $$.
But since $$ \mathcal{C} $$ is a chain, one of these sets, say $$ A_k $$, contains all the others.
Thus $$ u_i \in A_k $$ for $$ i = 1, \ 2, \dots, n $$.
However, $$ A_k $$ is a linearly independent set; so $$ a_1 u_1 + a_2 u_2 + \cdots + a_n u_n = \mathit{0} $$ implies that $$ a_1 = a_2 = \cdots = a_n = 0 $$.
It follows that $$ U $$ is  linearly independent.  
The Hausdorff maximal principle implies that $$ \mathcal{F} $$ has a maximal element.
This element is easily seen to be a maximal linearly independent subset of $$ \mathsf{V} $$ that contains $$ S $$. $$ \blacksquare $$

### Corollary.

Every vector space has a basis.

---

Sources:
- [Friedberg, S. H., Insel, A. J., & Spence, L. E. (2020). Linear Algebra (5th ed.). Pearson Education.](https://a.co/d/e6ZmR0B)