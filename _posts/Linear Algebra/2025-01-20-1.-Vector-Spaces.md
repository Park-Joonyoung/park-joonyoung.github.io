---
title: 1. Vector Spaces
# description: Short summary of the post
date: 2025-01-01 10:11
categories: [Mathematics, Linear Algebra]
tags: []     # TAG names should always be lowercase
math: true
pin: false
---

## 1.1 Vector spaces

### Definition

A vector space (or linear space) $$ \mathsf{V} $$ over a field F consists of a set on which two operations (addition and scalar multiplication) are defined so that for each pair of elements $$ x $$, $$ y $$, in $$ \mathsf{V} $$ there is a unique element $$ x + y $$ in $$ \mathsf{V} $$, and for each element $$ a $$ in $$ F $$ and each element $$ x $$ in $$ \mathsf{V} $$ there is a unique element $$ ax $$ in $$ \mathsf{V} $$, such that the following conditions hold.

- (VS 1) For all $$ x $$, $$ y $$ in $$ \mathsf{V} $$, $$ x + y = y + x $$ (commutativity of addition).
- (VS 2) For all $$ x $$, $$ y $$, $$ z $$ in $$ \mathsf{V} $$, $$ (x + y) + z = x + (y + z) $$ (associativity of addition).
- (VS 3) There exists an element in $$ \mathsf{V} $$ denoted by $$ 0 $$ such that $$ x + 0 = x $$ for each $$ x $$ in $$ \mathsf{V} $$.
- (VS 4) For each element $$ x $$ in $$ \mathsf{V} $$ there exists an element $$ y $$ in $$ \mathsf{V} $$ such that $$ x + y = 0 $$.
- (VS 5) For each element $$ x $$ in $$ \mathsf{V} $$, $$ 1x = x $$.
- (VS 6) For each pair of elements $$ a $$, $$ b $$ in $$ F $$ and each element $$ x $$ in $$ \mathsf{V} $$, $$ (ab)x = a(bx) $$.
- (VS 7) For each element $$ a $$ in $$ F $$ and each pair of elements $$ x $$, $$ y $$ in $$ \mathsf{V} $$, $$ a(x + y) = ax + ay $$.
- (VS 8) For each pair of elements $$ a $$, $$ b $$ in $$ F $$ and each element $$ x $$ in $$ \mathsf{V} $$, $$ (a + b)x = ax + bx $$.

### Theorem 1.1 (Cancellation Law for Vector Addition)

If $$ x $$, $$ y $$, and $$ z $$ are vectors in a vector space $$ \mathsf{V} $$ such that $$ x + z = y + z $$, then $$ x = y $$.

**Proof**  
There exists a vector $$ v $$ in $$ \mathsf{V} $$ such that $$ z + v = 0 $$ (VS 4).
Thus

$$
\begin{align*}
    x &= x + 0 = x + (z + v) = (x + z) + v \\
      &= (y + z) + v = y + (z + v) = y + 0 = y
\end{align*}
$$

by (VS 2) and (VS 3). $$ \blacksquare $$

### Corollary 1.

The vector $$ 0 $$ described in (VS 3) is unique.

### Corollary 2.

The vector $$ y $$ described in (VS 4) is unique.

### Theorem 1.2

In any vector space $$ \mathsf{V} $$, the following statements are true:  
(a) $$ 0x = 0 $$ for each $$ x \in \mathsf{V} $$.  
(b) $$ (-a)x = -(ax) = a(-x) $$ for each $$ a \in F $$ and each $$ x \in \mathsf{V} $$.  
(c) $$ a0 = 0 $$ for each $$ a \in F $$.

**Proof**  
(a) By (VS 8), (VS 3), and (VS 1), it follows that

$$
\begin{align*}
    0x + 0x = (0 + 0)x = 0x = 0x + 0 = 0 + 0x
\end{align*}
$$

Hence $$ 0x = 0 $$ by Theorem 1.1.  
(b) The vector $$ -(ax) $$ is the unique element of $$ \mathsf{V} $$ such that $$ ax + [-(ax)] = 0 $$.
Thus if $$ ax + (-a)x = 0 $$, Corollary 2 to Theorem 1.1 implies that $$ (-a)x = -(ax) $$.
But by (VS 8),

$$
\begin{align*}
    ax + (-a)x = [a + (-a)]x = 0x = 0
\end{align*}
$$

by (a).
Consequently $$ (-a)x = -(ax) $$.  
The proof of (c) is similar to the proof of (a). $$ \blacksquare $$

## 1.2 Subspaces

### Definition

A subset $$ \mathsf{W} $$ of a vector space $$ \mathsf{V} $$ over a field $$ F $$ is called a subspace of $$ \mathsf{V} $$ if $$ \mathsf{W} $$ is a vector space over $$ F $$ with the operations of addition and scalar multiplication defined on $$ \mathsf{V} $$.

A subset $$ \mathsf{W} $$ of a vector space $$ \mathsf{V} $$ is a subspace of $$ \mathsf{V} $$ if and only if the following four properties hold.

1. $$ x + y \in \mathsf{W} $$ whenever $$ x \in \mathsf{W} $$ and $$ y \in \mathsf{W} $$. ($$ \mathsf{W} $$ is closed under addition.)
2. $$ cx \in \mathsf{W} $$ whenever $$ c \in F $$ and $$ x \in \mathsf{W} $$. ($$ \mathsf{W} $$ is closed under scalar multiplication.)
3. $$ \mathsf{W} $$ has a zero vector.
4. Each vector in $$ \mathsf{W} $$ has an additive inverse in $$ \mathsf{W} $$.

### Theorem 1.3

Let $$ \mathsf{V} $$ be a vector space and $$ \mathsf{W} $$ a subset of $$ \mathsf{V} $$.
Then $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$ if and only if the following three conditions hold for the operations defined in $$ \mathsf{V} $$.  
(a) $$ 0 \in \mathsf{W} $$.  
(b) $$ x + y \in \mathsf{W} $$ whenever $$ x \in \mathsf{W} $$ and $$ y \in \mathsf{W} $$.  
(c) $$ cx \in \mathsf{W} $$ whenever $$ c \in F $$ and $$ x \in \mathsf{W} $$.

**Proof**  
If $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$, then $$ \mathsf{W} $$ is a vector space with the operations of addition and scalar multiplication defined on $$ \mathsf{V} $$.
Hence conditions (b) and (c) hold, and there exists a vector $$ 0' \in \mathsf{W} $$ such that $$ x + 0' = x $$ for each $$ x \in \mathsf{W} $$.
But also $$ x + 0 = x $$, and thus $$ 0' = 0 $$ by Theorem 1.1.
So condition (a) holds.  
Conversely, if conditions (a), (b), and (c) hold, the discussion preceding this theorem shows that $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$ if the additive inverse of each vector in $$ \mathsf{W} $$ lies in $$ \mathsf{W} $$.
But if $$ x \in \mathsf{w} $$, then $$ (-1)x \in \mathsf{W} $$ by condition (c), and $$ -x = (-1)x $$ by Theorem 1.2.
Hence $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$. $$ \blacksquare $$

### Theorem 1.4

Any intersection of subspaces of a vector space $$ \mathsf{V} $$ is a subspace of $$ \mathsf{V} $$.

**Proof**  
Let $$ \mathcal{C} $$ be a collection of subspaces of $$ \mathsf{V} $$, and let $$ \mathsf{W} $$ denote the intersection of the subspaces in $$ \mathcal{C} $$.
Since every subspace contains the zero vector, $$ 0 \in \mathsf{W} $$.
Let $$ a \in F $$ and $$ x, \ y \in \mathsf{W} $$.
Then $$ x $$ and $$ y $$ are contained in each subspace in $$ \mathcal{C} $$.
Because each subspace in $$ \mathcal{C} $$ is closed under addition and scalar multiplication, it follows that $$ x + y $$ and and $$ ax $$ are contained in each subspace in $$ \mathcal{C} $$.
Hence $$ x + y $$ and $$ ax $$ are also contained in $$ \mathsf{W} $$, so that $$ \mathsf{W} $$ is a subspace of $$ \mathsf{V} $$ by Theorem 1.3. $$ \blacksquare $$

## 1.4 Linear combinations and systems of linear equations

### Definition

Let $$ \mathsf{V} $$ be a vector space and $$ S $$ a nonempty subset of $$ \mathsf{V} $$.
A vector $$ v \in \mathsf{V} $$ is called a linear combination of vectors of $$ S $$ if there exist a finite number of vectors $$ u_1, \ u_2, \dots, u_n $$ in $$ S $$ and scalars $$ a_1, \ a_2, \dots, a_n $$ in $$ F $$ such that $$ v = a_1 u_1 + a_2 u_2 + \cdots + a_n u_n $$.

Observe that in any vector space $$ \mathsf{V} $$, $$ 0v = 0 $$ for each $$ v \in \mathsf{V} $$.
Thus the zero vector is a linear combination of any nonempty subset of $$ \mathsf{V} $$.

### Definition

Let $$ S $$ be a nonempty subset of a vector space $$ \mathsf{V} $$.
The span of $$ S $$, denoted span($$ S $$), is the set consisting of all linear combinations of the vectors in $$ S $$.
We define span($$ \emptyset $$) $$ = \{ 0 \} $$.

### Theorem 1.5

The span of any subset $$ S $$ of a vector $$ \mathsf{V} $$ is a subspace of $$ \mathsf{V} $$ that contains $$ S $$.
Moreover, any subspace of $$ \mathsf{V} $$ that contains $$ S $$ must also contain the span of $$ S $$.

**Proof**  
This result is immediate if $$ S = \emptyset $$ because span($$ \emptyset $$) $$ = \{ 0 \} $$, which is a subspace that contains $$ S $$ and is contained in any subspace of $$ \mathsf{V} $$.
If $$ S \neq \emptyset $$, then $$ S $$ contains a vector $$ z $$.
So $$ 0z = 0 $$ is in span($$ S $$).
Let $$ x, \ y \in $$ span($$ S $$).
Then there exist vectors $$ u_1, \ u_2, \dots, u_m, \ v_1, \ v_2, \dots, v_n $$ in $$ S $$ and scalars $$ a_1, \ a_2, \dots, a_m, \ b_1, \ b_2, \dots, b_n $$ such that

$$
\begin{align*}
    x = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m
\end{align*}
$$

and

$$
\begin{align*}
    y = b_1 v_1 + b_2 v_2 + \cdots + b_n v_n
\end{align*}
$$

Then

$$
\begin{align*}
    x + y = a_1 u_1 + a_2 u_2 + \cdots + a_m u_m + b_1 v_1 + b_2 v_2 + \cdots + b_n v_n
\end{align*}
$$

and, for any scalar $$ c $$,

$$
\begin{align*}
    cx = (c a_1) u_1 + (c a_2) u_2 + \cdots + (c a_m) u_m
\end{align*}
$$

are clearly linear combinations of the vectors in $$ S $$; so $$ x + y $$ and $$ cx $$ are in span($$ S $$).
Thus span($$ S $$) is a subspace of $$ \mathsf{V} $$.
Furthermore, if $$ v \in S $$, then $$ v = 1 \cdot v \in $$ span($$ S $$); so the span of $$ S $$ contains $$ S $$.  
Now let $$ \mathsf{W} $$ denote any subspace of $$ \mathsf{V} $$ that contains $$ S $$.
If $$ w \in $$ span($$ S $$), then $$ w $$ has the form $$ w = c_1 w_1 + c_2 w_2 + \cdots + c_k w_k $$ for some vectors $$ w_1, \ w_2, \dots, w_k $$ in $$ S $$ and some scalars $$ c_1, \ c_2, \dots, c_k $$.
Since $$ S \subseteq \mathsf{W} $$, we have $$ w_1, \ w_2, \dots, w_k \in \mathsf{W} $$.
Therefore $$ w = c_1 w_1 + c_2 w_2 + \cdots + c_k w_k $$ is in $$ \mathsf{W} $$.
Because $$ w $$, an arbitrary vector in span($$ S $$), belongs to $$ \mathsf{W} $$, it follows that span($$ S $$) $$ \subseteq \mathsf{W} $$. $$ \blacksquare $$

### Definition

A subset $$ S $$ of a vector space $$ \mathsf{V} $$ generates (or spans) $$ \mathsf{V} $$ if span($$ S $$) = $$ \mathsf{V} $$.

## 1.5 Linear dependence and linear independence

### Definition

A subset $$ S $$ of a vector space $$ \mathsf{V} $$ is called linearly dependent if there exist a finite number of distinct vectors $$ u_1, \ u_2, \dots, u_n $$ in $$ S $$ and scalars $$ a_1, \ a_2, \dots, a_n $$ not all zero, such that

$$
\begin{align*}
    a_1 u_1 + a_2 u_2 + \cdots + a_n u_n = 0
\end{align*}
$$

In this case we also say that the vectors of $$ S $$ are linearly dependent.

### Definition

A subset $$ S $$ of a vector space that is not linearly dependent is called linearly independent.

The following facts about linearly independent sets are true in any vector space.

1. The empty set is linearly independent, for linearly dependent sets must be nonempty.
2. A set consisting of a single nonzero vector is linearly independent.
For if $$ \{ u \} $$ is linearly dependent, then $$ au = 0 $$ for some nonzero scalar $$ a $$.
Thus $$ u = a^{-1} (au) = a^{-1} 0 = 0 $$.
3. A set is linearly independent if and only if the only representations of $$ 0 $$ as linear combinations of its vectors are trivial representations.

### Theorem 1.6

Let $$ \mathsf{V} $$ be a vector space, and let $$ S_1 \subseteq S_2 \subseteq \mathsf{V} $$.
If $$ S_1 $$ is linearly dependent, then $$ S_2 $$ is linearly dependent.

### Corollary

Let $$ \mathsf{V} $$ be a vector space, and let $$ S_1 \subseteq S_2 \subseteq \mathsf{V} $$.
If $$ S_2 $$ is linearly independent, then $$ S_1 $$ is linearly independent.

### Theorem 1.7

Let $$ S $$ be a linearly independent subset of a vector space $$ \mathsf{V} $$, and let $$ v $$ be a vector in $$ \mathsf{V} $$ that is not in $$ S $$.
Then $$ S \cup \{ v \} $$ is linearly dependent if and only if $$ v \in $$ span($$ S $$).

**Proof**  
If $$ S \cup \{ v \} $$ is linearly dependent, then there are vectors $$ u_1, \ u_2, \dots, u_n $$ in $$ S \cup \{ v \} $$ such that $$ a_1 u_1 + a_2 u_2 + \cdots + a_n u_n = 0 $$ for some nonzero scalars $$ a_1, \ a_2, \dots, a_n $$.
Since $$ S $$ is linearly independent, one of the $$ u_i $$'s, say $$ u_1 $$, equals $$ v $$.
Thus $$ a_1 v + a_2 u_2 + \cdots + a_n u_n = 0 $$, and so

$$
\begin{align*}
    v = a^{-1} (-a_2 u_2 - \cdots - a_n u_n) = -({a_1}^{-1} a_2) u_2 - \cdots - ({a_1}^{-1} a_n) u_n
\end{align*}
$$

Because $$ v $$ is a linear combination of $$ u_2, \dots, u_n $$, which are in $$ S $$, we have $$ v \in $$ span($$ S $$).

Conversely, let $$ v \in $$ span($$ S $$).
Then there exist vectors $$ v_1, \ v_2, \dots, v_m $$ in $$ S $$ and scalars $$ b_1, \ b_2, \dots, b_m $$ such that $$ v = b_1 v_1 + b_2 v_2 + \cdots + b_m v_m $$.
Therefore

$$
\begin{align*}
    0 = b_1 v_1 + b_2 v_2 + \cdots + b_n v_m + (-1) v
\end{align*}
$$

Note that $$ v \neq v_i $$ for $$ i = 1, \ 2, \dots, m $$ because $$ v \notin S $$.
Hence the coefficient of $$ v $$ in this linear combination is nonzero, and so the set $$ \{ v_1, \ v_2, \dots, v_m, \ v \} $$ is linearly dependent.
Thus $$ S \cup \{ v \} $$ is linearly dependent by Theorem 1.6. $$ \blacksquare $$

## 1.6 Bases and dimension

### Definition

A basis $$ \beta $$ for a vector space $$ \mathsf{V} $$ is a linearly independent subset of $$ \mathsf{V} $$ that generates $$ \mathsf{V} $$.

### Theorem 1.8

Let $$ \mathsf{V} $$ be a vector space and $$ u_1, \ u_2, \dots, u_n $$ be distinct vectors in $$ \mathsf{V} $$.
Then $$ \beta = \{ u_1, \ u_2, \dots, u_n \} $$ is a basis for $$ \mathsf{V} $$ if and only if each $$ v \in \mathsf{V} $$ can be uniquely expressed as a linear combination of vectors of $$ \beta $$, that is, can be expressed in the form

$$
\begin{align*}
    v = a_1 u_1 + a_2 u_2 + \cdots + a_n u_n
\end{align*}
$$

for unique scalars $$ a_1, \ a_2, \dots, a_n $$.

**Proof**  
Let $$ \beta $$ be a basis for $$ \mathsf{V} $$.
If $$ v \in \mathsf{V} $$, then $$ v \in $$ span($$ \beta $$) because span($$ \beta $$) $$ = \mathsf{V} $$.
Thus $$ v $$ is a linear combination of the vectors of $$ \beta $$.
Suppose that

$$
\begin{align*}
    v = a_1 u_1 + a_2 u_2 + \cdots + a_n u_n
\end{align*}
$$

and

$$
\begin{align*}
    v = b_1 u_1 + b_2 u_2 + \cdots + b_n u_n
\end{align*}
$$

are two such representations of $$ v $$.
Subtracting the second equation from the first gives

$$
\begin{align*}
    0 = (a_1 - b_1) u_1 + (a_2 - b_2) u_2 + \cdots + (a_n - b_n) u_n
\end{align*}
$$

Since $$ \beta $$ is linearly independent, it follows that $$ a_1 - b_1 = a_2 - b_2 = \cdots = a_n - b_n = 0 $$.
Hence $$ a_1 = b_1, \ a_2 = b_2, \dots, a_n = b_n $$, and so $$ v $$ is uniquely expressible as a linear combination of the vectors of $$ \beta $$.  
The proof of the converse is an exercise. $$ \blacksquare $$

### Theorem 1.9

If a vector space $$ \mathsf{V} $$ is generated by a finite set $$ S $$, then some subset of $$ S $$ is a basis for $$ \mathsf{V} $$.
Hence $$ \mathsf{V} $$ has a finite basis.

**Proof**  
If $$ S = \emptyset $$ or $$ S = \{ 0 \} $$, then $$ \mathsf{V} = \{ 0 \} $$ and $$ \emptyset $$ is a subset of $$ S $$ that is a basis for $$ \mathsf{V} $$.
Otherwise $$ S $$ contains a nonzero vector $$ u_1 $$, and $$ \{ u_1 \} $$ is a linearly independent set.
Continue, if possible, choosing vectors $$ u_2, \dots, u_k $$ in $$ S $$ such that $$ \{ u_1, \ u_2, \dots, u_k \} $$ is a linearly independent set of $$ k $$ vectors.
Since $$ S $$ is a finite set, this process must end with a linearly independent set $$ \beta = \{ u_1, \ u_2, \dots, u_n \} $$.
There are two ways this could happen.  
(i) The set $$ \beta = S $$.
In this case, $$ S $$ is both a linearly independent set and a generating set for $$ \mathsf{V} $$.
That is, $$ S $$ is itself a basis for $$ \mathsf{V} $$.  
(ii) The set $$ \beta $$ is a proper linearly independent subset of $$ S $$ such that adjoining to $$ \beta $$ any vector in $$ S $$ not in $$ \beta $$ produces a linearly independent set.
In this case, we claim that $$ \beta $$ is the desired subset of $$ S $$ that is a basis for $$ \mathsf{V} $$.
Because $$ \beta $$ is linearly independent by construction, it suffices to show that $$ \beta $$ spans $$ \mathsf{V} $$.
By Theorem 1.5, we need to show that $$ S \subseteq $$ span($$ \beta $$).
Let $$ v \in S $$.
If $$ v \in \beta $$, then clearly $$ v \in $$ span($$ \beta $$).
Otherwise, if $$ v \notin \beta $$, then the preceding construction shows that $$ \beta \cup \{ v \} $$ is linearly dependent.
So $$ v \in $$ span($$ \beta $$) by Theorem 1.7.
Thus $$ S \subseteq $$ span($$ \beta $$), completing the proof. $$ \blacksquare $$

### Theorem 1.10 (Replacement Theorem)

Let $$ \mathsf{V} $$ be a vector space that is generated by a set $$ G $$ containing exactly $$ n $$ vectors, and let $$ L $$ be a linearly independent subset of $$ \mathsf{V} $$ containing exactly $$ m $$ vectors.
Then $$ m \le n $$ and there exists a subset $$ H $$ of $$ G $$ containing exactly $$ n - m $$ vectors such that $$ L \cup H $$ generates $$ \mathsf{V} $$.

**Proof**  
The proof is by mathematical induction on $$ m $$.
The induction begins with $$ m = 0 $$; for in this case $$ L = \emptyset $$, and so taking $$ H = G $$ gives the desired result.  
Now suppose that the theorem is true for some integer $$ m \ge 0 $$.
Let $$ L = \{ v_1, \ v_2, \dots, v_{m + 1} \} $$ be a linearly independent subset of $$ \mathsf{V} $$ consisting of $$ m + 1 $$ vectors.
By the corollary to Theorem 1.6, $$ \{ v_1, \ v_2, \dots, v_m \} $$ is linearly independent, and so we may apply the induction hypothesis to conclude that $$ m \le n $$ and that there is a subset $$ \{ u_1, \ u_2, \dots, u_{n - m} \} $$ of $$ G $$ such that $$ \{ v_1, \ v_2, \dots, v_m \} \cup \{ u_1, \ u_2, \dots, u_{n - m} \} $$ generates $$ \mathsf{V} $$.
Thus there exist scalars $$ a_1, \ a_2, \dots, a_m, \ b_1, \ b_2, \dots, b_{n - m} $$ such that

$$
\begin{align*}
    a_1 v_1 &+ a_2 v_2 + \cdots + a_m v_m \\
    &+ b_1 u_1 + b_2 u_2 + \cdots + b_{n - m} u_{n - m} = v_{m + 1}
\end{align*}
\label{eq:1}
\tag{1.1}
$$

Note that $$ n - m > 0 $$, lest $$ v_{m + 1} $$ be a linear combination of $$ v_1, \ v_2, \dots, v_m $$, which by Theorem 1.7 contradicts the assumption that $$ L $$ is linearly independent.
Hence $$ n > m $$; that is, $$ n \le m + 1 $$.
Moreover, some $$ b_i $$, say $$ b_1 $$, is nonzero, for otherwise we obtain the same contradiction.
Solving \eqref{eq:1} for $$ u_1 $$ gives

$$
\begin{align*}
    u_1 &= ({-b_1}^{-1} a_1) v_1 + ({-b_1}^{-1} a_2) v_2 + \cdots + ({-b_1}^{-1} a_m) v_m \\
    &+ ({b_1}^{-1}) v_{m + 1} + ({-b_1}^{-1} b_2) u_2 + \cdots + ({-b_1}^{-1} b_{n - m}) u_{n - m}
\end{align*}
$$