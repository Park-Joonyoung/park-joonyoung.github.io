---
title: 3. Elementary Matrix Operations and Systems of Linear Equations
# description: Short summary of the post
date: 2025-01-03 22:20
categories: [Mathematics, Linear Algebra]
tags: [linear-algebra]     # TAG names should always be lowercase
math: true
pin: false
---

## 3.1 Elementary matrix operations and elementary matrices

### Definition

Let $$ A $$ be an $$ m \times n $$ matrix.
Any one of following three operations on the rows [columns] of $$ A $$ is called an elementary row [column] operation:  
(1) Type 1: interchanging any two rows [columns] of $$ A $$  
(2) Type 2: multiplying any row [column] of $$ A $$ by a nonzero scalar  
(3) Type 3: adding any scalar multiple of a row [column] of $$ A $$ to another row [column].

### Definition

An $$ n \times n $$ elementary matrix is a matrix obtained by performing an elementary operation on $$ I_n $$.
The elementary matrix is said to be of type 1, 2, or 3 according to whether the elementary operation performed on $$ I_n $$ is a type 1, 2, or 3 operation, respectively.

### Theorem 3.1

Let $$ A \in \mathsf{M}_{m \times n}(F) $$, and suppose that $$ B $$ is obtained from $$ A $$ by performing an elementary row [column] operation.
Then there exists an $$ m \times m $$ $$[n \times n] $$ elementary matrix $$ E $$ such that $$ B = EA $$ $$ [B = AE] $$.
In fact, $$ E $$ is obtained from $$ I_m $$ $$ [I_n] $$ by performing the same elementary row [column] operation as that which was performed on $$ A $$ to obtain $$ B $$.
Conversely, if $$ E $$ is an elementary $$ m \times m $$ $$ [n \times n] $$ matrix, then $$ EA $$ $$ [AE] $$ is the matrix obtained from $$ A $$ by performing the same elementary row [column] operation as that which produces $$ E $$ from $$ I_m $$ $$ [I_n] $$.

### Theorem 3.2

Elementary matrices are invertible, and the inverse of an elementary matrix is an elementary matrix of the same type.

**Proof**  
Let $$ E $$ be an elementary $$ n \times n $$ matrix.
Then $$ E $$ can be obtained by an elementary row operation on $$ I_n $$.
By reversing the steps used to transform $$ I_n $$ into $$ E $$, we can transform $$ E $$ back into $$ I_n $$.
The result is that $$ I_n $$ can be obtained from $$ E $$ by an elementary row operation of the same type.
By Theorem 3.1, there is an elementary matrix $$ \overline{E} $$ such that $$ \overline{E} E = I_n $$.
Therefore, $$ E $$ is invertible and $$ E^{-1} = \overline{E} $$. $$ \blacksquare $$

## 3.2 The rank of a matrix and matrix inverses

### Definition

If $$ A \in \mathsf{M}_{m \times n}(F) $$, we define the rank of $$ A $$, denoted rank($$ A $$), to be the rank of the linear transformaion $$ \mathsf{L}_A : \mathsf{F}^n \rightarrow \mathsf{F}^m $$.

### Theorem 3.3

Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ be a linear transformation between finite-dimensional vector spaces, and let $$ \beta $$ and $$ \gamma $$ be ordered bases for $$ \mathsf{V} $$ and $$ \mathsf{W} $$, respectively.
Then rank($$ \mathsf{T} $$) $$ = $$ rank($$ {[\mathsf{T}]}_{\beta}^{\gamma} $$).

### Theorem 3.4

Let $$ A $$ be an $$ m \times n $$ matrix.
If $$ P $$ and $$ Q $$ are invertible $$ m \times m $$ and $$ n \times n $$ matrices, respectively, then  
(a) rank($$ AQ $$) $$ = $$ rank($$ A $$)  
(b) rank($$ PA $$) $$ = $$ rank($$ A $$)  
(c) rank($$ PAQ $$) $$ = $$ rank($$ A $$)

**Proof**  
First observe that

$$
\begin{align*}
    \mathsf{R}({\mathsf{L}}_{AQ}) = \mathsf{R}({\mathsf{L}}_A {\mathsf{L}}_Q) = {\mathsf{L}}_A {\mathsf{L}}_Q ({\mathsf{F}}^n) = {\mathsf{L}}_A ({\mathsf{L}}_Q({\mathsf{F}}^n)) = {\mathsf{L}}_A ({\mathsf{F}}^n) = \mathsf{R}({\mathsf{L}}_A)
\end{align*}
$$

since $$ {\mathsf{L}}_Q $$ is onto.
Therefore

$$
\begin{align*}
    \text{rank}(AQ) = \dim(\mathsf{R}({\mathsf{L}}_{AQ})) = \dim(\mathsf{R}({\mathsf{L}}_A)) = \text{rank}(A)
\end{align*}
$$

This establishes (a).
Establishing (b) is left for an exercise.
Finally, applying (a) and (b), we have

$$
\begin{align*}
    \text{rank}(PAQ) = \text{rank}(PA) = \text{rank}(A)
\end{align*}
$$

### Corollary

Elementary row and column operations on a matrix are rank-preserving.

**Proof**  
If $$ B $$ is obtained from a matrix $$ A $$ by an elementary row operation, then there exists an elementary matrix $$ E $$ such that $$ B = EA $$.
By Theorem 3.2, $$ E $$ is invertible, and hence rank($$ B $$) $$ = $$ rank($$ A $$) by Theorem 3.4.
The proof that elementary column operations are rank-preserving is left as an exercise. $$ \blacksquare $$

### Theorem 3.5

The rank of any matrix equals the maximum number of its linearly independent columns; that is, the rank of a matrix is the dimension of the subspace generated by its columns.

**Proof**  
For any $$ A \in {\mathsf{M}}_{m \times n}(F) $$,

$$
\begin{align*}
    \text{rank}(A) = \text{rank}({\mathsf{L}}_A) = \dim(\mathsf{R}({\mathsf{L}}_A))
\end{align*}
$$

Let $$ \beta $$ be the standard ordered basis for $$ \mathsf{F}^n $$.
Then $$ \beta $$ spans $$ \mathsf{F}^n $$ and hence, by Theorem 2.2,

$$
\begin{align*}
    \mathsf{R}({\mathsf{L}}_A) = \text{span}({\mathsf{L}}_A(\beta)) = \text{span}(\{ {\mathsf{L}}_A(e_1), \ {\mathsf{L}}_A(e_2), \dots, {\mathsf{L}}_A(e_n) \})
\end{align*}
$$

But, for any $$ j $$, we have seen in Theorem 2.13(b) that $$ {\mathsf{L}}_A(e_j) = Ae_j = a_j $$, where $$ a_j $$ is the $$ j $$th column of $$ A $$.
Hence

$$
\begin{align*}
    \mathsf{R}({\mathsf{L}}_A) = \text{span}(\{ a_1, \ a_2, \dots, a_n \})
\end{align*}
$$

Thus

$$
\begin{align*}
    \text{rank}(A) = \dim(\mathsf{R}({\mathsf{L}}_A)) = \dim(\text{span}(\{ a_1, \ a_2, \dots, a_n \}))
\end{align*}
$$

### Theorem 3.6

Let $$ A $$ be an $$ m \times n $$ matrix of rank $$ r $$.
Then $$ r \le m $$, $$ r \le n $$, and, by means of a finite number of elementary row and column operations, $$ A $$ can be transformed into the matrix

$$
\begin{align*}
    D = 
    \begin{pmatrix}
        I_r & O_1 \\
        O_2 & O_3
    \end{pmatrix}
\end{align*}
$$

where $$ O_1 $$, $$ O_2 $$, and $$ O_3 $$ are zero matrices.
Thus $$ D_{ii} = 1 $$ for $$ i \le r $$ and $$ D_{ij} = 0 $$ otherwise.

**Proof**  
If $$ A $$ is the zero matrix, $$ r = 0 $$.
In this case, the conclusion follows with $$ D = A $$.
Now suppose that $$ A \neq O $$ and $$ r = $$ rank($$ A $$); then $$ r > 0 $$.
The proof is by mathematical induction on $$ m $$, the number of rows of $$ A $$.  
Suppose that $$ m = 1 $$.
By means of at most one type 1 column operation and at most one type 2 column operation, $$ A $$ can be transformed into a matrix with a $$ 1 $$ in the $$ 1, \ 1 $$ position.
By means of at most $$ n - 1 $$ type 3 column operations, this matrix can in turn be transformed into the matrix

$$
\begin{align*}
    \begin{pmatrix}
        1 & 0 & \cdots & 0
    \end{pmatrix}
\end{align*}
$$

Note that there is one linearly independent column in $$ D $$.
So rank($$ D $$) $$ = $$ rank($$ A $$) $$ = 1 $$ by the corollary to Theorem 3.4 and by Theorem 3.5.
Thus the theorem is established for $$ m = 1 $$.  
Next assume that the theorem holds for any matrix with at most $$ m - 1 $$ rows (for some $$ m > 1 $$).
Suppose that $$ A $$ is any $$ m \times n $$ matrix.
If $$ n = 1 $$, Theorem 3.6 can be established in a manner analogous to that for $$ m = 1 $$.
We now suppose that $$ n > 1 $$.
Since $$ A \neq O $$, $$ A_{ij} \neq 0 $$ for some $$ i $$, $$ j $$.
By means of at most one elementary row and at most one elementary column operation (each of type 1), we can move the nonzero entry to the $$ 1, \ 1 $$ position.
By means of at most one additional type 2 operation, we can assure a $$ 1 $$ in the $$ 1, \ 1 $$ position.
By means of at most $$ m - 1 $$ type 3 row operations and at most $$ n - 1 $$ type 3 column operations, we can eliminate all nonzero entries in the first row and the first column with the exception of the $$ 1 $$ in the $$ 1, \ 1 $$ position.  
Thus, with a finite number of elementary operations, $$ A $$ can be transformed into a matrix

$$
\begin{align*}
    B =
    \begin{pmatrix}
        \begin{array}{c|ccc}
            1 & 0 & \cdots & 0 \\
            \hline
            0 & & & \\
            \vdots & & B' & \\
            0 & & &
        \end{array}
    \end{pmatrix}
\end{align*}
$$

where $$ B' $$ is an $$ (m - 1) \times (n - 1) $$ matrix.
$$ B' $$ has rank one less than $$ B $$.
Since rank($$ A $$) $$ = $$ rank($$ B $$) $$ = r $$, rank($$ B' $$) $$ = r - 1 $$.
Therefore $$ r - 1 \le m - 1 $$ and $$ r - 1 \le n - 1 $$ by the induction hypothesis.
Hence $$ r \le m $$ and $$ r \le n $$.  
Also by the induction hypothesis, $$ B' $$ can be transformed by a finite number of elementary row and column operations into the $$ (m - 1) \times (n - 1) $$ matrix $$ D' $$ such that

$$
\begin{align*}
    D' =
    \begin{pmatrix}
        I_{r - 1} & O_4 \\
        O_5 & O_6
    \end{pmatrix}
\end{align*}
$$

where $$ O_4 $$, $$ O_5 $$, and $$ O_6 $$ are zero matrices.
That is, $$ D' $$ consists of all zeros except for its first $$ r - 1 $$ diagonal entries, which are ones.
Let

$$
\begin{align*}
    D =
    \begin{pmatrix}
        \begin{array}{c|ccc}
            1 & 0 & \cdots & 0 \\
            \hline
            0 & & & \\
            \vdots & & D' & \\
            0 & & &
        \end{array}
    \end{pmatrix}
\end{align*}
$$

We see that the theorem now follows once we show that $$ D $$ can be obtained from $$ B $$ by means of a finite number of elementary row and column operations, and it is possible. Thus, since $$ A $$ can be transformed into $$ B $$ and $$ B $$ can be transformed into $$ D $$, each by a finite number of elementary operations, $$ A $$ can be transformed into $$ D $$ by a finite number of elementary operations.  
Finally, since $$ D' $$ contains ones as its first $$ r - 1 $$ diagonal entries, $$ D $$ contains ones as its first $$ r $$ diagonal entries and zeros elsewhere.
This establishes the theorem. $$ \blacksquare $$

### Corollary 1

Let $$ A $$ be an $$ m \times n $$ matrix of rank $$ r $$.
Then there exist invertible matrices $$ B $$ and $$ C $$ of sizes $$ m \times m $$ and $$ n \times n $$, respectively, such that $$ D = BAC $$, where

$$
\begin{align*}
    D =
    \begin{pmatrix}
        I_r & O_1 \\
        O_2 & O_3
    \end{pmatrix}
\end{align*}
$$

is the $$ m \times n $$ matrix in which $$ O_1 $$, $$ O_2 $$, and $$ O_3 $$ are zero matrices.

**Proof**  
By Theorem 3.6, $$ A $$ can be transformed by means of a finite number of elementary row and column operations into the matrix $$ D $$.
We can appeal to Theorem 3.1 each time we perform an elementary operation.
Thus there exist elementary $$ m \times m $$ matrices $$ E_1, \ E_2, \dots, E_p $$ and elementary $$ n \times n $$ matrices $$ G_1, \ G_2, \dots, G_q $$ such that

$$
\begin{align*}
    D = E_p E_{p - 1} \cdots E_2 E_1 A G_1 G_2 \cdots G_q
\end{align*}
$$

By Theorem 3.2, each $$ E_j $$ and $$ G_j $$ is invertible.
Let $$ B = E_p E_{p - 1} \cdots E_1 $$ and $$ C = G_1 G_2 \cdots G_q $$.
Then $$ B $$ and $$ C $$ are invertible, and $$ D = BAC $$. $$ \blacksquare $$

### Corollary 2

Let $$ A $$ be an $$ m \times n $$ matrix.
Then  
(a) rank($$ A^t $$) $$ = $$ rank($$ A $$)  
(b) The rank of any matrix equals the maximum number of its linearly independent rows; that is, the rank of a matrix is the dimension of the subspace generated by its rows.  
(c) The rows and columns of any matrix generate subspaces of the same dimension, numerically equal to the rank of the matrix.

**Proof**  
(a) By Corollary 1, there exist invertible matrices $$ B $$ and $$ C $$ such that $$ D = BAC $$, where $$ D $$ satisfies the stated conditions of the corollary.
Taking transposes, we have

$$
\begin{align*}
    D^t = (BAC)^t = C^t A^t B^t
\end{align*}
$$

Since $$ B $$ and $$ C $$ are invertible, so are $$ B^t $$ and $$ C^t $$.
Hence by Theorem 3.4,

$$
\begin{align*}
    \text{rank}(A^t) = \text{rank}(C^t A^t B^t) = \text{rank}(D^t)
\end{align*}
$$

Suppose that $$ r = \text{rank}(A) $$.
Then $$ D^t $$ is an $$ n \times m $$ matrix with the form of the matrix $$ D $$ in Corollary 1, and hence rank($$ D^t $$) $$ = r $$ by Theorem 3.5.
Thus

$$
\begin{align*}
    \text{rank}(A^t) = \text{rank}(D^t) = r = \text{rank}(A)
\end{align*}
$$

This establishes (a).  
The proofs of (b) and (c) are omitted. $$ \blacksquare $$

### Corollary 3

Every invertible matrix is a product of elementary matrices.

**Proof**  
If $$ A $$ is an invertible $$ n \times n $$ matrix, then rank($$ A $$) $$ = n $$.
Hence the matrix $$ D $$ in Corollary 1 equals $$ I_n $$, and there exist invertible matrices $$ B $$ and $$ C $$ such that $$ I_n = BAC $$.
As in the proof of Corollary 1, note that $$ B = E_p E_{p - 1} \cdots E_1 $$ and $$ C = G_1 G_2 \cdots G_q $$, where the $$ E_i $$'s and $$ G_i $$'s are elementary matrices.
Thus $$ A = B^{-1} I_n C^{-1} = B^{-1} C^{-1} $$, so that

$$
\begin{align*}
    A = {E_1}^{-1} {E_2}^{-1} \cdots {E_p}^{-1} {G_q}^{-1} {G_{q - 1}}^{-1} \cdots {G_1}^{-1}
\end{align*}
$$

The inverses of elementary matrices are elementary matrices, however, and hence $$ A $$ is the product of elementary matrices. $$ \blacksquare $$

### Theorem 3.7

Let $$ \mathsf{T} : \mathsf{V} \rightarrow \mathsf{W} $$ and $$ \mathsf{U} : \mathsf{W} \rightarrow \mathsf{Z} $$ be linear transformations on finite-dimensional vector spaces $$ \mathsf{V} $$, $$ \mathsf{W} $$, and $$ \mathsf{Z} $$, and let $$ A $$ and $$ B $$ be matrices such that the product $$ AB $$ is defined.
Then  
(a) rank($$ \mathsf{U} \mathsf{T} $$) $$ \le $$ rank($$ \mathsf{U} $$)  
(b) rank($$ \mathsf{U} \mathsf{T} $$) $$ \le $$ rank($$ \mathsf{T} $$)  
(c) rank($$ AB $$) $$ \le $$ rank($$ A $$)  
(d) rank($$ AB $$) $$ \le $$ rank($$ B $$)

**Proof**  
(a) Clearly, $$ \mathsf{R}(\mathsf{T}) \subseteq \mathsf{W} $$.
Hence

$$
\begin{align*}
    \mathsf{R}(\mathsf{U} \mathsf{T}) = \mathsf{U} \mathsf{T} (\mathsf{V}) = \mathsf{U}(\mathsf{T}(\mathsf{V})) = \mathsf{U}(\mathsf{R}(\mathsf{T})) \subseteq \mathsf{U}(\mathsf{W}) = \mathsf{R}(\mathsf{U})
\end{align*}
$$

Thus

$$
\begin{align*}
    \text{rank}(\mathsf{U} \mathsf{T}) = \dim(\mathsf{R}(\mathsf{U} \mathsf{T})) \le \dim(\mathsf{R}(\mathsf{U})) = \text{rank}(\mathsf{U})
\end{align*}
$$

(c) By (a),

$$
\begin{align*}
    \text{rank}(AB) = \text{rank}({\mathsf{L}}_{AB}) = \text{rank}(\mathsf{L}_A \mathsf{L}_B) \le \text{rank}(\mathsf{L}_A) = \text{rank}(A)
\end{align*}
$$

(d) By (c) and Corollary 2 to Theorem 3.6,

$$
\begin{align*}
    \text{rank}(AB) = \text{rank}({(AB)}^t) = \text{rank}(B^t A^t) \le \text{rank}(B^t) = \text{rank}(B)
\end{align*}
$$

(b) Let $$ \alpha $$, $$ \beta $$, and $$ \gamma $$ be ordered bases for $$ \mathsf{V} $$, $$ \mathsf{W} $$, and $$ \mathsf{Z} $$, respectively, and let $$ A' = {[\mathsf{U}]}_{\beta}^{\gamma} $$ and $$ B' = {[\mathsf{T}]}_{\alpha}^{\beta} $$.
Then $$ A'B' = {[\mathsf{U} \mathsf{T}]}_{\alpha}^{\gamma} $$ by Theorem 2.11.
Hence, by Theorem 3.3 and (d),

$$
\begin{align*}
    \text{rank}(\mathsf{U} \mathsf{T}) = \text{rank}(A'B') \le \text{rank}(B') = \text{rank}(\mathsf{T})
\end{align*}
$$

### The inverse of a matrix

### Definition

Let $$ A $$ and $$ B $$ be $$ m \times n $$ and $$ m \times p $$ matrices, respectively.
By the augmented matrix $$ (A|B) $$, we mean the $$ m \times (n + p) $$ matrix $$ (A \ B) $$, that is, the matrix whose first $$ n $$ columns are the columns of $$ A $$, and whose last $$ p $$ columns are the columns of $$ B $$.

## 3.3 Systems of linear equations—theoretical aspects

The system of equations

$$
\begin{align*}
    (S) &&
    \begin{matrix}
        a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n = b_1 \\
        a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n = b_2 \\
        \vdots \\
        a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n = b_m
    \end{matrix}
\end{align*}
$$

where $$ a_{ij} $$ and $$ b_i $$ ($$ 1 \le i \le m $$ and $$ 1 \le j \le n $$) are scalars in a field $$ F $$ and $$ x_1, \ x_2, \dots, x_n $$ are $$ n $$ variables taking values in $$ F $$, is called a system of $$ m $$ linear equations in $$ n $$ unknowns over the field $$ F $$.

### Definition

A system $$ Ax = b $$ of $$ m $$ linear equations in $$ n $$ unknowns is said to be homogeneous if $$ b = \mathit{0} $$.
Otherwise the system is said to be nonhomogeneous.

### Theorem 3.8

Let $$ Ax = \mathit{0} $$ be a homogeneous system of $$ m $$ linear equations in $$ n $$ unknowns over a field $$ F $$.
Let $$ \mathsf{K} $$ denote the set of all solutions to $$ Ax = \mathit{0} $$.
Then $$ \mathsf{K} = \mathsf{N}(\mathsf{L}_A) $$; hence $$ \mathsf{K} $$ is a subspace of $$ \mathsf{F}^n $$ of dimension $$ n - \text{rank}(\mathsf{L}_A) = n - \text{rank}(A) $$.

**Proof**  
Clearly, $$ \mathsf{K} = \{ s \in \mathsf{F}^n : As = \mathit{0} \} = \mathsf{N}(\mathsf{L}_A) $$.
The second part follows from the dimension theorem. $$ \blacksquare $$

### Corollary

If $$ m < n $$, the system $$ Ax = \mathit{0} $$ has a nonzero solution.

**Proof**  
Suppose that $$ m < n $$.
Then rank($$ A $$) $$ = $$ rank($$ \mathsf{L}_A $$) $$ \le m $$.
Hence

$$
\begin{align*}
    \dim(\mathsf{K}) = n - \text{rank}(\mathsf{L}_A) \ge n - m > 0
\end{align*}
$$

where $$ \mathsf{K} = \mathsf{N}(\mathsf{L}_A) $$.
Since $$ \dim(\mathsf{K}) > 0 $$, $$ \mathsf{K} \neq \{ \mathit{0} \} $$.
Thus there exists a nonzero vector $$ s \in \mathsf{K} $$; so $$ s $$ is a nonzero solution $$ Ax = \mathit{0} $$. $$ \blacksquare $$

### Theorem 3.9

Let $$ \mathsf{K} $$ be the solution set of a consistent system of linear equations $$ Ax = b $$, and let $$ \mathsf{K}_{\mathsf{H}} $$ be the solution set of the corresponding homogeneous system $$ Ax = \mathit{0} $$.
Then for any solution $$ s $$ to $$ Ax = b $$

$$
\begin{align*}
    K = \{ s \} + \mathsf{K}_{\mathsf{H}} = \{ s + k : k \in \mathsf{K}_{\mathsf{H}} \}
\end{align*}
$$

**Proof**  
Let $$ s $$ be any solution to $$ Ax = b $$.
We must show that $$ K = \{ s \} + \mathsf{K}_{\mathsf{H}} $$.
If $$ w \in K $$, then $$ Aw = b $$.
Hence

$$
\begin{align*}
    A(w - s) = Aw - As = b - b = \mathit{0}
\end{align*}
$$

So $$ w - s \in \mathsf{K}_{\mathsf{H}} $$.
Thus there exists $$ k \in \mathsf{K}_{\mathsf{H}} $$ such that $$ w - s = k $$.
It follows that $$ w = s + k \in \{ s \} + \mathsf{K}_{\mathsf{H}} $$, and therefore

$$
\begin{align*}
    K \subseteq \{ s \} + \mathsf{K}_{\mathsf{H}}
\end{align*}
$$

Conversely, suppose that $$ w \in \{ s \} + \mathsf{K}_{\mathsf{H}} $$; then $$ w = s + k $$ for some $$ k \in \mathsf{K}_{\mathsf{H}} $$.
But then $$ Aw = A(s + k) = As + Ak = b + \mathit{0} = b $$; so $$ w \in K $$.
Therefore $$ \{ s \} + \mathsf{K}_{\mathsf{H}} \subseteq K $$, and thus $$ K = \{ s \} + \mathsf{K}_{\mathsf{H}} $$. $$ \blacksquare $$

### Theorem 3.10

Let $$ Ax = b $$ be a system of $$ n $$ linear equations in $$ n $$ unknowns.
If $$ A $$ is invertible, then the system has exactly one solution, namely, $$ A^{-1} b $$.
Conversely, if the system has exactly one solution, then $$ A $$ is invertible.

**Proof**  
Suppose that $$ A $$ is invertible.
Substituting $$ A^{-1} b $$ into the system, we have $$ A(A^{-1} b) = (A A^{-1})b = b $$.
Thus $$ A^{-1} b $$ is a solution.
If $$ s $$ is an arbitrary solution, then $$ As = b $$.
Multiplying both sides by $$ A^{-1} $$ gives $$ s = A^{-1} b $$.
Thus the system has one and only one solution, namely, $$ A^{-1} b $$.  
Conversely, suppose that the system has exactly one solution $$ s $$.
Let $$ \mathsf{K}_{\mathsf{H}} $$ denote the solution set for the corresponding homogeneous system $$ Ax = \mathit{0} $$.
By Theorem 3.9, $$ \{ s \} = \{ s \} + \mathsf{K}_{\mathsf{H}} $$.
But this is so only if $$ \mathsf{K}_{\mathsf{H}} = \{ \mathit{0} \} $$.
Thus $$ \mathsf{N}(\mathsf{L}_A) = \{ \mathit{0} \} $$, and hence $$ A $$ is invertible. $$ \blacksquare $$

### Theorem 3.11

Let $$ Ax = b $$ be a system of linear equations.
Then the system is consistent if and only if rank($$ A $$) $$ = $$ rank($$ A | b $$).

**Proof**  
To say that $$ Ax = b $$ has a solution is equivalent to saying that $$ b \in \mathsf{R}(\mathsf{L}_A) $$.
In the proof of Theorem 3.5, we saw that

$$
\begin{align*}
    \mathsf{R}(\mathsf{L}_A) = \text{span}(\{ a_1, \ a_2, \dots, a_n \})
\end{align*}
$$

the span of the columns of $$ A $$.
Thus $$ Ax = b $$ has a solution if and only if $$ b \in \text{span}(\{ a_1, \ a_2, \dots, a_n \}) $$.
But $$ b \in \text{span}(\{ a_1, \ a_2, \dots, a_n \}) $$ if and only if $$ \text{span}(\{ a_1, \ a_2, \dots, a_n \}) = \text{span}(\{ a_1, \ a_2, \dots, a_n, \ b \}) $$.
This last statement is equivalent to

$$
\begin{align*}
    \dim(\text{span}(\{ a_1, \ a_2, \dots, a_n \})) = \dim(\text{span}(\{ a_1, \ a_2, \dots, a_n, \ b \}))
\end{align*}
$$

So by Theorem 3.5, the preceding equation reduces to

$$
\begin{align*}
    \text{rank}(A) = \text{rank}(A | b)
\end{align*}
$$

### Theorem 3.12

Let $$ A $$ be an $$ n \times n $$ input-output matrix having the form

$$
\begin{align*}
    A =
    \begin{pmatrix}
        B & C \\
        D & E
    \end{pmatrix}
\end{align*}
$$

where $$ D $$ is a $$ 1 \times (n - 1) $$ positive vector and $$ C $$ is an $$ (n - 1) \times 1 $$ positive vector.
Then $$ (I - A)x = \mathit{0} $$
has a one-dimensional solution set that is generated by a nonnegative vector.

## 3.4 Systems of linear equations—computational aspects

### Definition

Two systems of linear equations are called equivalent if they have the same solution set.

### Theorem 3.13

Let $$ Ax = b $$ be a system of $$ m $$ linear equations in $$ n $$ unknowns, and let $$ C $$ be an invertible $$ m \times m $$ matrix.
Then the system $$ (CA)x = Cb $$ is equivalent to $$ Ax = b $$.

**Proof**  
Let $$ K $$ be the solution set for $$ Ax = b $$ and $$ K' $$ the solution set for $$ (CA)x = Cb $$.
If $$ w \in K $$, then $$ Aw = b $$.
So $$ (CA)w = Cb $$, and hence $$ w \in K' $$.
Thus $$ K \subseteq K' $$.  
Conversely, if $$ w \in K' $$, then $$ (CA)w = Cb $$.
Hence

$$
\begin{align*}
    Aw = C^{-1} (CAw) = C^{-1} (Cb) = b
\end{align*}
$$

so $$ w \in K $$.
Thus $$ K' \subseteq K $$, and therefore, $$ K = K' $$. $$ \blacksquare $$

### Corollary

Let $$ Ax = b $$ be a system of $$ m $$ linear equations in $$ n $$ unknowns.
If $$(A'|b')$$ is obtained from $$ (A|b) $$ by a finite number of elementary row operations, then the system $$ A'x = b' $$ is  equivalent to the original system.

**Proof**  
Suppose that $$ (A'|b') $$ is obtained from $$ (A|b) $$ by elementary row operations.
These may be executed by multiplying $$ (A|b) $$ by elementary $$ m \times m $$ matrices $$ E_1, \ E_2, \dots, E_p $$.
Let $$ C = E_p \cdots E_2 E_1 $$; then

$$
\begin{align*}
    (A'|b') = C(A|b) = (CA|Cb)
\end{align*}
$$

Since each $$ E_i $$ is invertible, so is $$ C $$.
Now $$ A' = CA $$ and $$ b' = Cb $$.
Thus by Theorem 3.13, the system $$ A'x = b' $$ is equivalent to the system $$ Ax = b $$. $$ \blacksquare $$

### Definition

A matrix is said to be in reduced row echelon form if the following three conditions are satisfied.  
(a) Any row containing a nonzero entry precedes any row in which all the entries are zero (if any).  
(b) The first nonzero entry in each row is the only nonzero entry in its column.  
(c) The first nonzero entry in each row is $$ 1 $$ and it occurs in a column to the right of the first nonzero entry in the preceding row.

### Gaussian Elimination

Consider the system of linear equations

$$
\begin{align*}
    3x_1 + 2x_2 + 3x_3 - 2x_4 = 1 \\
    x_1 + x_2 + x_3 = 3 \\
    x_1 + 2x_2 + x_3 - x_4 = 2
\end{align*}
$$

First, we form the augmented matrix

$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        3 & 2 & 3 & -2 & 1 \\
        1 & 1 & 1 & 0 & 3 \\
        1 & 2 & 1 & -1 & 2
    \end{array}
    \end{pmatrix}
\end{align*}
$$

By using elementary row operations, we transform the augmented matrix into an upper triangular matrix in which the first nonzero entry of each row is $$ 1 $$, and it occurs in a column to the right of the first nonzero entry of each preceding row.

<p class="indented-paragraph">
1. In the leftmost nonzero column, create a $$ 1 $$ in the first row.
We can accomplish this step by interchanging the first and third rows.

<span class="mjx-display-style">
$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        1 & 2 & 1 & -1 & 2 \\
        1 & 1 & 1 & 0 & 3 \\
        3 & 2 & 3 & -2 & 1
    \end{array}
    \end{pmatrix}
\end{align*}
$$
</span>

2. By means of type 3 row operations, use the first row to obtain zeros in the remaining positions of the leftmost nonzero column.

<span class="mjx-display-style">
$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        1 & 2 & 1 & -1 & 2 \\
        0 & -1 & 0 & 1 & 1 \\
        0 & -4 & 0 & 1 & -5
    \end{array}
    \end{pmatrix}
\end{align*}
$$
</span>

3. Create a $$ 1 $$ in the next row in the leftmost possible column, without using previosu row(s).
We can obtain a $$ 1 $$ in the second row, second column by multiplying the second row by $$ -1 $$.

<span class="mjx-display-style">
$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        1 & 2 & 1 & -1 & 2 \\
        0 & 1 & 0 & -1 & -1 \\
        0 & -4 & 0 & 1 & -5
    \end{array}
    \end{pmatrix}
\end{align*}
$$
</span>

4. Now use type 3 elementary row operations to obtain zeros below the $$ 1 $$ created in the preceding step.

<span class="mjx-display-style">
$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        1 & 2 & 1 & -1 & 2 \\
        0 & 1 & 0 & -1 & -1 \\
        0 & 0 & 0 & -3 & -9
    \end{array}
    \end{pmatrix}
\end{align*}
$$
</span>

5. Repeat steps 3 and 4 on each succeeding row until no nonzero rows remain.

<span class="mjx-display-style">
$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        1 & 2 & 1 & -1 & 2 \\
        0 & 1 & 0 & -1 & -1 \\
        0 & 0 & 0 & 1 & 3
    \end{array}
    \end{pmatrix}
\end{align*}
$$
</span>
</p>

To complete the simplification of the augmented matrix, we must make the first nonzero entry in each row the only nonzero entry in its column.

<p class="indented-paragraph">
6. Work upward, beginning with the last nonzero row, and add multiples of each row to the rows above.

<span class="mjx-display-style">
$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        1 & 2 & 1 & 0 & 5 \\
        0 & 1 & 0 & 0 & 2 \\
        0 & 0 & 0 & 1 & 3
    \end{array}
    \end{pmatrix}
\end{align*}
$$
</span>

7. Repeat the process described  in step 6 for each preceding row until it is performed with the second row, at which time the reduction process is complete.

<span class="mjx-display-style">
$$
\begin{align*}
    \begin{pmatrix}
    \begin{array}{cccc|c}
        1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 0 & 0 & 2 \\
        0 & 0 & 0 & 1 & 3
    \end{array}
    \end{pmatrix}
\end{align*}
$$
</span>
</p>

We have now obtained the desired reduction of the augmented matrix.
This matrix corresponds to the system of linear equations

$$
\begin{align*}
    x_1 + x_3 = 1 \\
    x_2 = 2 \\
    x_4 = 3
\end{align*}
$$

$$ x_1 $$ and $$ x_3 $$ can have any values provided their sum is $$ 1 $$.
Letting $$ x_3 = t $$, we have $$ x_1 = 1 - t $$.
Thus an arbitrary solution to the original system has the form

$$
\begin{align*}
    \begin{pmatrix}
        1 - t \\
        2 \\
        t \\
        3
    \end{pmatrix}
    =
    \begin{pmatrix}
    1 \\
    2 \\
    0 \\
    3
    \end{pmatrix}
    + t
    \begin{pmatrix}
        -1 \\
        0 \\
        1 \\
        0
    \end{pmatrix}
\end{align*}
$$

Of all the methods for transforming a matrix into its reduced row echelon form, Gaussian elimination requires the fewest arithmetic operations.

### Theorem 3.14

Gaussian elimination transforms any matrix into its reduced row echelon form.

### Theorem 3.15

Let $$ Ax = b $$ be a system of $$ r $$ nonzero equations in $$ n $$ unknowns.
Suppose that rank$$ (A) = $$ rank$$(A|b)$$ and that $$ (A|b) $$ is in reduced row echelon form.
Then  
(a) rank$$(A) = r $$  
(b) If the general solution obtained is of the form

$$
\begin{align*}
    s = s_0 + t_1 u_1 + t_2 u_2 + \cdots + t_{n - r} u_{n - r}
\end{align*}
$$

then $$ \{ u_1, \ u_2, \dots, u_{n - r} \} $$ is a basis for the solution set of the corresponding homogeneous system, and $$ s_0 $$ is a solution to the original system.

**Proof**  
Since $$ (A|b) $$ is in reduced row echelon form, $$ (A|b) $$ must have $$ r $$ nonzero rows.
Clearly these rows are linearly independent by the definition of the reduced row echelon form, and so rank$$ (A|b) = r $$.
Thus rank$$ (A) = r $$.  
Let $$ K $$ be the solution set for $$ Ax = b $$, and let $$ \mathsf{K}_{\mathsf{H}} $$ be the solution set for $$ Ax = \mathit{0} $$.
Setting $$ t_1 = t_2 = \cdots = t_{n - r} = 0 $$, we see that $$ s = s_0 \in \mathsf{K} $$.
But by Theorem 3.9, $$ \mathsf{K} = \{ s_0 \} + \mathsf{K}_{\mathsf{H}} $$.
Hence

$$
\begin{align*}
    \mathsf{K}_{\mathsf{H}} = \{ -s_0 \} + K = \text{span}(\{ u_1, \ u_2, \dots, u_{n - r} \})
\end{align*}
$$

Because rank$$ (A) = r $$, we have $$ \dim(\mathsf{K}_{\mathsf{H}}) = n - r $$.
Thus since $$ \dim(\mathsf{K}_{\mathsf{H}}) = n - r $$ and $$ \mathsf{K}_{\mathsf{H}} $$ is generated by a set $$ \{ u_1, \ u_2, \dots, u_{n - r} \} $$ containing at most $$ n - r $$ vectors, we conclude that this set is a basis for $$ \mathsf{K}_{\mathsf{H}} $$. $$ \blacksquare $$

### Theorem 3.16

Let $$ A $$ be an $$ m \times n $$ matrix of rank $$ r $$, where $$ r > 0 $$, and let $$ B $$ be the reduced row echelon form of $$ A $$.
Then  
(a) The number of nonzero rows in $$ B $$ is $$ r $$.  
(b) For each $$ i = 1, \ 2, \dots, r $$, there is a column $$ b_j $$, of $$ B $$ such that $$ b_{j_i} = e_i $$.  
(c) The columns of $$ A $$ numbered $$ j_1, \ j_2, \dots, j_r $$ are linearly independent.  
(d) For each $$ k = 1, \ 2, \dots, n $$, if column $$ k $$ of $$ B $$ is $$ d_1 e_1 + d_2 e_2 + \cdots + d_r e_r $$, then column $$ k $$ of $$ A $$ is $$ d_1 a_{j_1} + d_2 a_{j_2} + \cdots + d_r a_{j_r} $$.

### Corollary

The reduced row echelon form of a matrix is unique.

---

Sources:
- [Friedberg, S. H., Insel, A. J., & Spence, L. E. (2020). Linear Algebra (5th ed.). Pearson Education.](https://a.co/d/e6ZmR0B)